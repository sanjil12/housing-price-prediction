{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "9N1Ow9LaSJA4",
    "outputId": "1b201392-b125-4b52-f196-750952204c6c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import models\n",
    "from keras.layers import Dense\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sflEwkWAS77h"
   },
   "outputs": [],
   "source": [
    "def readData(path):\n",
    "    data=pd.read_csv(path)\n",
    "    return data\n",
    "testingData=readData('/content/drive/My Drive/housingTest_Data.csv')\n",
    "trainingData=readData('/content/drive/My Drive/housingTrain_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PKiVBlA_TLft"
   },
   "outputs": [],
   "source": [
    "def dropColumns(df):\n",
    "    cols_to_drop=['Id','MiscFeature','PoolQC','Fence','FireplaceQu','Alley']\n",
    "    df.drop(columns=cols_to_drop,inplace=True)\n",
    "    return df\n",
    "\n",
    "## Handling Missing Data\n",
    "def handleMissingData(df):\n",
    "    cols=df.columns\n",
    "    for col in cols:\n",
    "        if df[col].isnull().any():\n",
    "            if df[col].dtype==np.dtype('object'):\n",
    "                df[col]=df[col].fillna(df[col].mode().iloc[[0]][0])\n",
    "            elif df[col].dtype==np.int64 or df[col].dtype==np.float64:\n",
    "                df[col]=df[col].fillna(df[col].mean())\n",
    "    return df\n",
    "\n",
    "trainingData=dropColumns(trainingData)\n",
    "trainingData=handleMissingData(trainingData)\n",
    "testingData=dropColumns(testingData)\n",
    "testingData=handleMissingData(testingData)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RFwJx6vQTOKy"
   },
   "outputs": [],
   "source": [
    "def oneHotEncode(df,colNames):\n",
    "    for col in colNames:\n",
    "        if( df[col].dtype == np.dtype('object')):\n",
    "            dummies = pd.get_dummies(df[col],prefix=col)\n",
    "            df = pd.concat([df,dummies],axis=1)\n",
    "\n",
    "            #drop the encoded column\n",
    "            df.drop([col],axis = 1 , inplace=True)\n",
    "    return df\n",
    "trainingData=oneHotEncode(trainingData,trainingData.columns)\n",
    "testingData=oneHotEncode(testingData,testingData.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m2Dun8pYTSGE"
   },
   "outputs": [],
   "source": [
    "target=trainingData['SalePrice']\n",
    "def getCommonColumns(df1,df2):\n",
    "    return list(set(df1.columns).intersection(set(df2.columns)))\n",
    "\n",
    "common_columns=getCommonColumns(trainingData,testingData)\n",
    " \n",
    "trainingData=trainingData[common_columns]\n",
    "testingData=testingData[common_columns]\n",
    "trainingData=pd.concat([trainingData,target],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m6LViXVmTYbp"
   },
   "outputs": [],
   "source": [
    "XT=trainingData.values[:,:-1]\n",
    "YT=trainingData.values[:,-1]\n",
    "\n",
    "Xt=testingData.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "C4y19lQNTbVZ",
    "outputId": "11839ee9-1e7c-4a8e-ade9-98167dfd3c8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               23400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 36,151\n",
      "Trainable params: 36,151\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=models.Sequential()\n",
    "model.add(Dense(100,kernel_initializer='normal',activation='relu',input_shape=(233,)))\n",
    "model.add(Dense(50,kernel_initializer='normal',activation='relu'))\n",
    "model.add(Dense(50,kernel_initializer='normal',activation='relu'))\n",
    "model.add(Dense(50,kernel_initializer='normal',activation='relu'))\n",
    "model.add(Dense(50,kernel_initializer='normal',activation='relu'))\n",
    "model.add(Dense(1,activation='linear'))\n",
    "\n",
    "model.compile(loss='mean_absolute_error',optimizer='adam',metrics=['mean_absolute_error'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z1a_sld7Td05"
   },
   "outputs": [],
   "source": [
    "checkpoint_name = 'weights.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "r6kgBkmdTgy3",
    "outputId": "5c9878bc-a1e7-4ae1-dbc7-ad1922c14e58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 880 samples, validate on 220 samples\n",
      "Epoch 1/1000\n",
      "880/880 [==============================] - 0s 260us/step - loss: 182275.5946 - mean_absolute_error: 182275.5781 - val_loss: 168606.9159 - val_mean_absolute_error: 168606.9062\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 168606.91591, saving model to weights.hdf5\n",
      "Epoch 2/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 124754.8685 - mean_absolute_error: 124754.8672 - val_loss: 60358.6933 - val_mean_absolute_error: 60358.6914\n",
      "\n",
      "Epoch 00002: val_loss improved from 168606.91591 to 60358.69332, saving model to weights.hdf5\n",
      "Epoch 3/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 54128.3625 - mean_absolute_error: 54128.3672 - val_loss: 41611.6381 - val_mean_absolute_error: 41611.6406\n",
      "\n",
      "Epoch 00003: val_loss improved from 60358.69332 to 41611.63810, saving model to weights.hdf5\n",
      "Epoch 4/1000\n",
      "880/880 [==============================] - 0s 69us/step - loss: 49914.9531 - mean_absolute_error: 49914.9492 - val_loss: 41000.7792 - val_mean_absolute_error: 41000.7812\n",
      "\n",
      "Epoch 00004: val_loss improved from 41611.63810 to 41000.77915, saving model to weights.hdf5\n",
      "Epoch 5/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 46456.1107 - mean_absolute_error: 46456.1055 - val_loss: 36427.6897 - val_mean_absolute_error: 36427.6875\n",
      "\n",
      "Epoch 00005: val_loss improved from 41000.77915 to 36427.68974, saving model to weights.hdf5\n",
      "Epoch 6/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 42557.1512 - mean_absolute_error: 42557.1484 - val_loss: 34540.9061 - val_mean_absolute_error: 34540.9062\n",
      "\n",
      "Epoch 00006: val_loss improved from 36427.68974 to 34540.90614, saving model to weights.hdf5\n",
      "Epoch 7/1000\n",
      "880/880 [==============================] - 0s 66us/step - loss: 38854.5781 - mean_absolute_error: 38854.5781 - val_loss: 32683.3236 - val_mean_absolute_error: 32683.3223\n",
      "\n",
      "Epoch 00007: val_loss improved from 34540.90614 to 32683.32358, saving model to weights.hdf5\n",
      "Epoch 8/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 35915.9180 - mean_absolute_error: 35915.9141 - val_loss: 31038.2054 - val_mean_absolute_error: 31038.2070\n",
      "\n",
      "Epoch 00008: val_loss improved from 32683.32358 to 31038.20543, saving model to weights.hdf5\n",
      "Epoch 9/1000\n",
      "880/880 [==============================] - 0s 66us/step - loss: 32862.1723 - mean_absolute_error: 32862.1719 - val_loss: 29720.4098 - val_mean_absolute_error: 29720.4121\n",
      "\n",
      "Epoch 00009: val_loss improved from 31038.20543 to 29720.40984, saving model to weights.hdf5\n",
      "Epoch 10/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 31014.0200 - mean_absolute_error: 31014.0234 - val_loss: 29465.4419 - val_mean_absolute_error: 29465.4414\n",
      "\n",
      "Epoch 00010: val_loss improved from 29720.40984 to 29465.44187, saving model to weights.hdf5\n",
      "Epoch 11/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 29580.2320 - mean_absolute_error: 29580.2305 - val_loss: 28155.2627 - val_mean_absolute_error: 28155.2617\n",
      "\n",
      "Epoch 00011: val_loss improved from 29465.44187 to 28155.26271, saving model to weights.hdf5\n",
      "Epoch 12/1000\n",
      "880/880 [==============================] - 0s 68us/step - loss: 28377.2883 - mean_absolute_error: 28377.2910 - val_loss: 28112.4060 - val_mean_absolute_error: 28112.4082\n",
      "\n",
      "Epoch 00012: val_loss improved from 28155.26271 to 28112.40597, saving model to weights.hdf5\n",
      "Epoch 13/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 28202.4688 - mean_absolute_error: 28202.4668 - val_loss: 28934.9486 - val_mean_absolute_error: 28934.9473\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 28112.40597\n",
      "Epoch 14/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 27163.8339 - mean_absolute_error: 27163.8320 - val_loss: 27697.5367 - val_mean_absolute_error: 27697.5371\n",
      "\n",
      "Epoch 00014: val_loss improved from 28112.40597 to 27697.53668, saving model to weights.hdf5\n",
      "Epoch 15/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 26603.3354 - mean_absolute_error: 26603.3320 - val_loss: 29113.6526 - val_mean_absolute_error: 29113.6523\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 27697.53668\n",
      "Epoch 16/1000\n",
      "880/880 [==============================] - 0s 70us/step - loss: 26495.7306 - mean_absolute_error: 26495.7324 - val_loss: 27534.3019 - val_mean_absolute_error: 27534.3027\n",
      "\n",
      "Epoch 00016: val_loss improved from 27697.53668 to 27534.30192, saving model to weights.hdf5\n",
      "Epoch 17/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 26409.2215 - mean_absolute_error: 26409.2227 - val_loss: 27285.8196 - val_mean_absolute_error: 27285.8203\n",
      "\n",
      "Epoch 00017: val_loss improved from 27534.30192 to 27285.81957, saving model to weights.hdf5\n",
      "Epoch 18/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 26067.9826 - mean_absolute_error: 26067.9844 - val_loss: 27827.2473 - val_mean_absolute_error: 27827.2500\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 27285.81957\n",
      "Epoch 19/1000\n",
      "880/880 [==============================] - 0s 71us/step - loss: 25725.3739 - mean_absolute_error: 25725.3730 - val_loss: 27257.0752 - val_mean_absolute_error: 27257.0781\n",
      "\n",
      "Epoch 00019: val_loss improved from 27285.81957 to 27257.07518, saving model to weights.hdf5\n",
      "Epoch 20/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 26009.3588 - mean_absolute_error: 26009.3555 - val_loss: 28060.9124 - val_mean_absolute_error: 28060.9141\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 27257.07518\n",
      "Epoch 21/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 26565.4864 - mean_absolute_error: 26565.4883 - val_loss: 26860.8394 - val_mean_absolute_error: 26860.8418\n",
      "\n",
      "Epoch 00021: val_loss improved from 27257.07518 to 26860.83938, saving model to weights.hdf5\n",
      "Epoch 22/1000\n",
      "880/880 [==============================] - 0s 60us/step - loss: 25365.3953 - mean_absolute_error: 25365.3945 - val_loss: 26993.1772 - val_mean_absolute_error: 26993.1758\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 26860.83938\n",
      "Epoch 23/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 25095.3760 - mean_absolute_error: 25095.3750 - val_loss: 26969.4021 - val_mean_absolute_error: 26969.4004\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 26860.83938\n",
      "Epoch 24/1000\n",
      "880/880 [==============================] - 0s 74us/step - loss: 25169.5436 - mean_absolute_error: 25169.5430 - val_loss: 26898.5034 - val_mean_absolute_error: 26898.5020\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 26860.83938\n",
      "Epoch 25/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 24782.0350 - mean_absolute_error: 24782.0332 - val_loss: 26743.8516 - val_mean_absolute_error: 26743.8496\n",
      "\n",
      "Epoch 00025: val_loss improved from 26860.83938 to 26743.85160, saving model to weights.hdf5\n",
      "Epoch 26/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 24797.6690 - mean_absolute_error: 24797.6680 - val_loss: 26666.9261 - val_mean_absolute_error: 26666.9277\n",
      "\n",
      "Epoch 00026: val_loss improved from 26743.85160 to 26666.92610, saving model to weights.hdf5\n",
      "Epoch 27/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 24981.1866 - mean_absolute_error: 24981.1855 - val_loss: 27370.7835 - val_mean_absolute_error: 27370.7812\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 26666.92610\n",
      "Epoch 28/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 24506.5993 - mean_absolute_error: 24506.5977 - val_loss: 29445.7239 - val_mean_absolute_error: 29445.7246\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 26666.92610\n",
      "Epoch 29/1000\n",
      "880/880 [==============================] - 0s 66us/step - loss: 24860.2057 - mean_absolute_error: 24860.2070 - val_loss: 26430.0355 - val_mean_absolute_error: 26430.0371\n",
      "\n",
      "Epoch 00029: val_loss improved from 26666.92610 to 26430.03548, saving model to weights.hdf5\n",
      "Epoch 30/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 24637.0115 - mean_absolute_error: 24637.0098 - val_loss: 26640.7292 - val_mean_absolute_error: 26640.7266\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 26430.03548\n",
      "Epoch 31/1000\n",
      "880/880 [==============================] - 0s 66us/step - loss: 24968.9297 - mean_absolute_error: 24968.9297 - val_loss: 26463.7567 - val_mean_absolute_error: 26463.7559\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 26430.03548\n",
      "Epoch 32/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 24871.8853 - mean_absolute_error: 24871.8848 - val_loss: 26429.1198 - val_mean_absolute_error: 26429.1191\n",
      "\n",
      "Epoch 00032: val_loss improved from 26430.03548 to 26429.11978, saving model to weights.hdf5\n",
      "Epoch 33/1000\n",
      "880/880 [==============================] - 0s 68us/step - loss: 24138.1042 - mean_absolute_error: 24138.1016 - val_loss: 26059.0892 - val_mean_absolute_error: 26059.0879\n",
      "\n",
      "Epoch 00033: val_loss improved from 26429.11978 to 26059.08920, saving model to weights.hdf5\n",
      "Epoch 34/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 24009.8208 - mean_absolute_error: 24009.8223 - val_loss: 26144.2931 - val_mean_absolute_error: 26144.2910\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 26059.08920\n",
      "Epoch 35/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 24711.8513 - mean_absolute_error: 24711.8496 - val_loss: 25997.1159 - val_mean_absolute_error: 25997.1133\n",
      "\n",
      "Epoch 00035: val_loss improved from 26059.08920 to 25997.11587, saving model to weights.hdf5\n",
      "Epoch 36/1000\n",
      "880/880 [==============================] - 0s 60us/step - loss: 24700.9558 - mean_absolute_error: 24700.9590 - val_loss: 25841.8994 - val_mean_absolute_error: 25841.9004\n",
      "\n",
      "Epoch 00036: val_loss improved from 25997.11587 to 25841.89940, saving model to weights.hdf5\n",
      "Epoch 37/1000\n",
      "880/880 [==============================] - 0s 60us/step - loss: 23827.3564 - mean_absolute_error: 23827.3574 - val_loss: 25868.4048 - val_mean_absolute_error: 25868.4062\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 25841.89940\n",
      "Epoch 38/1000\n",
      "880/880 [==============================] - 0s 69us/step - loss: 23898.3671 - mean_absolute_error: 23898.3652 - val_loss: 25811.7502 - val_mean_absolute_error: 25811.7500\n",
      "\n",
      "Epoch 00038: val_loss improved from 25841.89940 to 25811.75021, saving model to weights.hdf5\n",
      "Epoch 39/1000\n",
      "880/880 [==============================] - 0s 67us/step - loss: 23944.8886 - mean_absolute_error: 23944.8887 - val_loss: 30885.0485 - val_mean_absolute_error: 30885.0508\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 25811.75021\n",
      "Epoch 40/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 24331.2580 - mean_absolute_error: 24331.2598 - val_loss: 26856.0261 - val_mean_absolute_error: 26856.0273\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 25811.75021\n",
      "Epoch 41/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 23954.2615 - mean_absolute_error: 23954.2617 - val_loss: 25573.2728 - val_mean_absolute_error: 25573.2734\n",
      "\n",
      "Epoch 00041: val_loss improved from 25811.75021 to 25573.27280, saving model to weights.hdf5\n",
      "Epoch 42/1000\n",
      "880/880 [==============================] - 0s 60us/step - loss: 25177.3126 - mean_absolute_error: 25177.3105 - val_loss: 26259.7555 - val_mean_absolute_error: 26259.7559\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 25573.27280\n",
      "Epoch 43/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 23902.3287 - mean_absolute_error: 23902.3301 - val_loss: 26071.8887 - val_mean_absolute_error: 26071.8887\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 25573.27280\n",
      "Epoch 44/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 23569.1156 - mean_absolute_error: 23569.1152 - val_loss: 25407.9060 - val_mean_absolute_error: 25407.9043\n",
      "\n",
      "Epoch 00044: val_loss improved from 25573.27280 to 25407.90604, saving model to weights.hdf5\n",
      "Epoch 45/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 23835.1621 - mean_absolute_error: 23835.1621 - val_loss: 25304.2381 - val_mean_absolute_error: 25304.2363\n",
      "\n",
      "Epoch 00045: val_loss improved from 25407.90604 to 25304.23814, saving model to weights.hdf5\n",
      "Epoch 46/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 23395.6306 - mean_absolute_error: 23395.6289 - val_loss: 26010.2855 - val_mean_absolute_error: 26010.2832\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 25304.23814\n",
      "Epoch 47/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 23580.1048 - mean_absolute_error: 23580.1016 - val_loss: 26435.3462 - val_mean_absolute_error: 26435.3457\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 25304.23814\n",
      "Epoch 48/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 23701.8763 - mean_absolute_error: 23701.8750 - val_loss: 26314.8644 - val_mean_absolute_error: 26314.8652\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 25304.23814\n",
      "Epoch 49/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 23550.5229 - mean_absolute_error: 23550.5234 - val_loss: 26767.3012 - val_mean_absolute_error: 26767.3008\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 25304.23814\n",
      "Epoch 50/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 23316.8956 - mean_absolute_error: 23316.8984 - val_loss: 25591.2754 - val_mean_absolute_error: 25591.2754\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 25304.23814\n",
      "Epoch 51/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 23028.5596 - mean_absolute_error: 23028.5586 - val_loss: 27053.7909 - val_mean_absolute_error: 27053.7891\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 25304.23814\n",
      "Epoch 52/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 23985.5810 - mean_absolute_error: 23985.5840 - val_loss: 25371.9087 - val_mean_absolute_error: 25371.9082\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 25304.23814\n",
      "Epoch 53/1000\n",
      "880/880 [==============================] - 0s 68us/step - loss: 23283.1857 - mean_absolute_error: 23283.1836 - val_loss: 25614.7058 - val_mean_absolute_error: 25614.7051\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 25304.23814\n",
      "Epoch 54/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 23133.1727 - mean_absolute_error: 23133.1777 - val_loss: 25416.8172 - val_mean_absolute_error: 25416.8184\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 25304.23814\n",
      "Epoch 55/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 24969.8266 - mean_absolute_error: 24969.8203 - val_loss: 25777.6705 - val_mean_absolute_error: 25777.6719\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 25304.23814\n",
      "Epoch 56/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 23650.5577 - mean_absolute_error: 23650.5566 - val_loss: 24979.7065 - val_mean_absolute_error: 24979.7070\n",
      "\n",
      "Epoch 00056: val_loss improved from 25304.23814 to 24979.70653, saving model to weights.hdf5\n",
      "Epoch 57/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 23100.2925 - mean_absolute_error: 23100.2930 - val_loss: 24997.3136 - val_mean_absolute_error: 24997.3105\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 24979.70653\n",
      "Epoch 58/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 23397.8211 - mean_absolute_error: 23397.8203 - val_loss: 25112.5529 - val_mean_absolute_error: 25112.5527\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 24979.70653\n",
      "Epoch 59/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 23253.5347 - mean_absolute_error: 23253.5391 - val_loss: 25681.6071 - val_mean_absolute_error: 25681.6074\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 24979.70653\n",
      "Epoch 60/1000\n",
      "880/880 [==============================] - 0s 66us/step - loss: 22660.8678 - mean_absolute_error: 22660.8691 - val_loss: 24720.7666 - val_mean_absolute_error: 24720.7656\n",
      "\n",
      "Epoch 00060: val_loss improved from 24979.70653 to 24720.76662, saving model to weights.hdf5\n",
      "Epoch 61/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 23098.5898 - mean_absolute_error: 23098.5918 - val_loss: 25118.5173 - val_mean_absolute_error: 25118.5176\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 24720.76662\n",
      "Epoch 62/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 23987.5686 - mean_absolute_error: 23987.5684 - val_loss: 24611.3275 - val_mean_absolute_error: 24611.3281\n",
      "\n",
      "Epoch 00062: val_loss improved from 24720.76662 to 24611.32745, saving model to weights.hdf5\n",
      "Epoch 63/1000\n",
      "880/880 [==============================] - 0s 59us/step - loss: 23752.4585 - mean_absolute_error: 23752.4590 - val_loss: 25422.2278 - val_mean_absolute_error: 25422.2266\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 24611.32745\n",
      "Epoch 64/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 23429.1706 - mean_absolute_error: 23429.1699 - val_loss: 24915.8008 - val_mean_absolute_error: 24915.8008\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 24611.32745\n",
      "Epoch 65/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 22701.4305 - mean_absolute_error: 22701.4316 - val_loss: 24847.5436 - val_mean_absolute_error: 24847.5430\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 24611.32745\n",
      "Epoch 66/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 23363.7134 - mean_absolute_error: 23363.7129 - val_loss: 26212.9031 - val_mean_absolute_error: 26212.9023\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 24611.32745\n",
      "Epoch 67/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 22598.1980 - mean_absolute_error: 22598.1953 - val_loss: 25142.4279 - val_mean_absolute_error: 25142.4297\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 24611.32745\n",
      "Epoch 68/1000\n",
      "880/880 [==============================] - 0s 69us/step - loss: 22558.3192 - mean_absolute_error: 22558.3203 - val_loss: 27596.8866 - val_mean_absolute_error: 27596.8867\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 24611.32745\n",
      "Epoch 69/1000\n",
      "880/880 [==============================] - 0s 70us/step - loss: 23069.1567 - mean_absolute_error: 23069.1562 - val_loss: 26092.6231 - val_mean_absolute_error: 26092.6230\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 24611.32745\n",
      "Epoch 70/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 22801.4311 - mean_absolute_error: 22801.4316 - val_loss: 24743.0467 - val_mean_absolute_error: 24743.0469\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 24611.32745\n",
      "Epoch 71/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 22655.8613 - mean_absolute_error: 22655.8633 - val_loss: 27570.0288 - val_mean_absolute_error: 27570.0293\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 24611.32745\n",
      "Epoch 72/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 22830.5376 - mean_absolute_error: 22830.5332 - val_loss: 25020.8312 - val_mean_absolute_error: 25020.8320\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 24611.32745\n",
      "Epoch 73/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 22821.5786 - mean_absolute_error: 22821.5820 - val_loss: 24956.7983 - val_mean_absolute_error: 24956.8008\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 24611.32745\n",
      "Epoch 74/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 22550.8995 - mean_absolute_error: 22550.9004 - val_loss: 25526.7464 - val_mean_absolute_error: 25526.7480\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 24611.32745\n",
      "Epoch 75/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 22698.6864 - mean_absolute_error: 22698.6855 - val_loss: 24465.4786 - val_mean_absolute_error: 24465.4766\n",
      "\n",
      "Epoch 00075: val_loss improved from 24611.32745 to 24465.47862, saving model to weights.hdf5\n",
      "Epoch 76/1000\n",
      "880/880 [==============================] - 0s 67us/step - loss: 22334.9280 - mean_absolute_error: 22334.9277 - val_loss: 28586.6586 - val_mean_absolute_error: 28586.6582\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 24465.47862\n",
      "Epoch 77/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 24778.4247 - mean_absolute_error: 24778.4258 - val_loss: 27523.6702 - val_mean_absolute_error: 27523.6699\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 24465.47862\n",
      "Epoch 78/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 22489.7227 - mean_absolute_error: 22489.7227 - val_loss: 25335.5214 - val_mean_absolute_error: 25335.5195\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 24465.47862\n",
      "Epoch 79/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 22177.8824 - mean_absolute_error: 22177.8809 - val_loss: 24427.7355 - val_mean_absolute_error: 24427.7344\n",
      "\n",
      "Epoch 00079: val_loss improved from 24465.47862 to 24427.73555, saving model to weights.hdf5\n",
      "Epoch 80/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 22573.8531 - mean_absolute_error: 22573.8516 - val_loss: 24872.4398 - val_mean_absolute_error: 24872.4395\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 24427.73555\n",
      "Epoch 81/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 22782.5922 - mean_absolute_error: 22782.5918 - val_loss: 24147.4352 - val_mean_absolute_error: 24147.4336\n",
      "\n",
      "Epoch 00081: val_loss improved from 24427.73555 to 24147.43519, saving model to weights.hdf5\n",
      "Epoch 82/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 22309.5775 - mean_absolute_error: 22309.5801 - val_loss: 27646.3545 - val_mean_absolute_error: 27646.3555\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 24147.43519\n",
      "Epoch 83/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 23083.6437 - mean_absolute_error: 23083.6406 - val_loss: 25171.2573 - val_mean_absolute_error: 25171.2559\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 24147.43519\n",
      "Epoch 84/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 22431.0395 - mean_absolute_error: 22431.0430 - val_loss: 25820.4219 - val_mean_absolute_error: 25820.4180\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 24147.43519\n",
      "Epoch 85/1000\n",
      "880/880 [==============================] - 0s 69us/step - loss: 22532.2374 - mean_absolute_error: 22532.2344 - val_loss: 27002.4784 - val_mean_absolute_error: 27002.4805\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 24147.43519\n",
      "Epoch 86/1000\n",
      "880/880 [==============================] - 0s 68us/step - loss: 22737.2099 - mean_absolute_error: 22737.2090 - val_loss: 23977.1590 - val_mean_absolute_error: 23977.1582\n",
      "\n",
      "Epoch 00086: val_loss improved from 24147.43519 to 23977.15898, saving model to weights.hdf5\n",
      "Epoch 87/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 23010.5292 - mean_absolute_error: 23010.5254 - val_loss: 25540.8441 - val_mean_absolute_error: 25540.8438\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 23977.15898\n",
      "Epoch 88/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 22385.6798 - mean_absolute_error: 22385.6816 - val_loss: 24098.4052 - val_mean_absolute_error: 24098.4043\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 23977.15898\n",
      "Epoch 89/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 22369.0091 - mean_absolute_error: 22369.0059 - val_loss: 23885.7665 - val_mean_absolute_error: 23885.7656\n",
      "\n",
      "Epoch 00089: val_loss improved from 23977.15898 to 23885.76648, saving model to weights.hdf5\n",
      "Epoch 90/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 21885.8436 - mean_absolute_error: 21885.8438 - val_loss: 24110.2835 - val_mean_absolute_error: 24110.2832\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 23885.76648\n",
      "Epoch 91/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 22069.1108 - mean_absolute_error: 22069.1113 - val_loss: 24216.6370 - val_mean_absolute_error: 24216.6367\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 23885.76648\n",
      "Epoch 92/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 21734.1841 - mean_absolute_error: 21734.1816 - val_loss: 23886.6735 - val_mean_absolute_error: 23886.6719\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 23885.76648\n",
      "Epoch 93/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 21912.8496 - mean_absolute_error: 21912.8516 - val_loss: 23843.1887 - val_mean_absolute_error: 23843.1895\n",
      "\n",
      "Epoch 00093: val_loss improved from 23885.76648 to 23843.18871, saving model to weights.hdf5\n",
      "Epoch 94/1000\n",
      "880/880 [==============================] - 0s 59us/step - loss: 21875.7871 - mean_absolute_error: 21875.7871 - val_loss: 24491.1362 - val_mean_absolute_error: 24491.1367\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 23843.18871\n",
      "Epoch 95/1000\n",
      "880/880 [==============================] - 0s 60us/step - loss: 21851.8123 - mean_absolute_error: 21851.8105 - val_loss: 24579.6683 - val_mean_absolute_error: 24579.6680\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 23843.18871\n",
      "Epoch 96/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 21827.8824 - mean_absolute_error: 21827.8809 - val_loss: 23872.1557 - val_mean_absolute_error: 23872.1543\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 23843.18871\n",
      "Epoch 97/1000\n",
      "880/880 [==============================] - 0s 73us/step - loss: 21766.3659 - mean_absolute_error: 21766.3652 - val_loss: 24566.0988 - val_mean_absolute_error: 24566.0977\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 23843.18871\n",
      "Epoch 98/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 21988.2858 - mean_absolute_error: 21988.2871 - val_loss: 25599.8235 - val_mean_absolute_error: 25599.8223\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 23843.18871\n",
      "Epoch 99/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 22303.0438 - mean_absolute_error: 22303.0430 - val_loss: 23899.5364 - val_mean_absolute_error: 23899.5371\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 23843.18871\n",
      "Epoch 100/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 21794.5442 - mean_absolute_error: 21794.5430 - val_loss: 24583.7695 - val_mean_absolute_error: 24583.7695\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 23843.18871\n",
      "Epoch 101/1000\n",
      "880/880 [==============================] - 0s 68us/step - loss: 22939.0276 - mean_absolute_error: 22939.0293 - val_loss: 23680.9016 - val_mean_absolute_error: 23680.9023\n",
      "\n",
      "Epoch 00101: val_loss improved from 23843.18871 to 23680.90160, saving model to weights.hdf5\n",
      "Epoch 102/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 21815.1794 - mean_absolute_error: 21815.1816 - val_loss: 23785.9881 - val_mean_absolute_error: 23785.9883\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 23680.90160\n",
      "Epoch 103/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 21891.1232 - mean_absolute_error: 21891.1230 - val_loss: 24046.6143 - val_mean_absolute_error: 24046.6152\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 23680.90160\n",
      "Epoch 104/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 21459.9082 - mean_absolute_error: 21459.9062 - val_loss: 23561.6772 - val_mean_absolute_error: 23561.6777\n",
      "\n",
      "Epoch 00104: val_loss improved from 23680.90160 to 23561.67724, saving model to weights.hdf5\n",
      "Epoch 105/1000\n",
      "880/880 [==============================] - 0s 60us/step - loss: 22671.9402 - mean_absolute_error: 22671.9434 - val_loss: 24033.6565 - val_mean_absolute_error: 24033.6562\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 23561.67724\n",
      "Epoch 106/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 21881.3373 - mean_absolute_error: 21881.3340 - val_loss: 23711.3568 - val_mean_absolute_error: 23711.3574\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 23561.67724\n",
      "Epoch 107/1000\n",
      "880/880 [==============================] - 0s 68us/step - loss: 21447.7902 - mean_absolute_error: 21447.7910 - val_loss: 24121.7615 - val_mean_absolute_error: 24121.7598\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 23561.67724\n",
      "Epoch 108/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 21536.2355 - mean_absolute_error: 21536.2344 - val_loss: 23546.8576 - val_mean_absolute_error: 23546.8574\n",
      "\n",
      "Epoch 00108: val_loss improved from 23561.67724 to 23546.85756, saving model to weights.hdf5\n",
      "Epoch 109/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 22021.9878 - mean_absolute_error: 22021.9844 - val_loss: 23409.5067 - val_mean_absolute_error: 23409.5059\n",
      "\n",
      "Epoch 00109: val_loss improved from 23546.85756 to 23409.50675, saving model to weights.hdf5\n",
      "Epoch 110/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 21666.1879 - mean_absolute_error: 21666.1855 - val_loss: 23647.7495 - val_mean_absolute_error: 23647.7520\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 23409.50675\n",
      "Epoch 111/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 21495.0538 - mean_absolute_error: 21495.0547 - val_loss: 23455.7093 - val_mean_absolute_error: 23455.7090\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 23409.50675\n",
      "Epoch 112/1000\n",
      "880/880 [==============================] - 0s 67us/step - loss: 21531.9653 - mean_absolute_error: 21531.9609 - val_loss: 23601.8932 - val_mean_absolute_error: 23601.8906\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 23409.50675\n",
      "Epoch 113/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 21552.6926 - mean_absolute_error: 21552.6914 - val_loss: 25101.5904 - val_mean_absolute_error: 25101.5918\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 23409.50675\n",
      "Epoch 114/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 22162.3500 - mean_absolute_error: 22162.3496 - val_loss: 26154.4295 - val_mean_absolute_error: 26154.4316\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 23409.50675\n",
      "Epoch 115/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 21443.8533 - mean_absolute_error: 21443.8516 - val_loss: 23370.1082 - val_mean_absolute_error: 23370.1094\n",
      "\n",
      "Epoch 00115: val_loss improved from 23409.50675 to 23370.10817, saving model to weights.hdf5\n",
      "Epoch 116/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 21179.7615 - mean_absolute_error: 21179.7637 - val_loss: 25097.1875 - val_mean_absolute_error: 25097.1855\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 23370.10817\n",
      "Epoch 117/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 21200.3307 - mean_absolute_error: 21200.3281 - val_loss: 24392.1051 - val_mean_absolute_error: 24392.1074\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 23370.10817\n",
      "Epoch 118/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 22264.4518 - mean_absolute_error: 22264.4531 - val_loss: 25548.5126 - val_mean_absolute_error: 25548.5117\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 23370.10817\n",
      "Epoch 119/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 21689.3106 - mean_absolute_error: 21689.3086 - val_loss: 23180.1260 - val_mean_absolute_error: 23180.1250\n",
      "\n",
      "Epoch 00119: val_loss improved from 23370.10817 to 23180.12603, saving model to weights.hdf5\n",
      "Epoch 120/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 21269.1360 - mean_absolute_error: 21269.1367 - val_loss: 24464.8313 - val_mean_absolute_error: 24464.8301\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 23180.12603\n",
      "Epoch 121/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 21620.7670 - mean_absolute_error: 21620.7656 - val_loss: 26364.2812 - val_mean_absolute_error: 26364.2812\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 23180.12603\n",
      "Epoch 122/1000\n",
      "880/880 [==============================] - 0s 67us/step - loss: 22370.5857 - mean_absolute_error: 22370.5840 - val_loss: 25812.8622 - val_mean_absolute_error: 25812.8613\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 23180.12603\n",
      "Epoch 123/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 23258.4727 - mean_absolute_error: 23258.4727 - val_loss: 24641.9945 - val_mean_absolute_error: 24641.9941\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 23180.12603\n",
      "Epoch 124/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 20868.0515 - mean_absolute_error: 20868.0527 - val_loss: 23135.6986 - val_mean_absolute_error: 23135.6992\n",
      "\n",
      "Epoch 00124: val_loss improved from 23180.12603 to 23135.69862, saving model to weights.hdf5\n",
      "Epoch 125/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 21275.6818 - mean_absolute_error: 21275.6816 - val_loss: 23004.3052 - val_mean_absolute_error: 23004.3066\n",
      "\n",
      "Epoch 00125: val_loss improved from 23135.69862 to 23004.30518, saving model to weights.hdf5\n",
      "Epoch 126/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 21235.4181 - mean_absolute_error: 21235.4160 - val_loss: 23134.6148 - val_mean_absolute_error: 23134.6113\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 23004.30518\n",
      "Epoch 127/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 21471.7237 - mean_absolute_error: 21471.7246 - val_loss: 23624.7763 - val_mean_absolute_error: 23624.7773\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 23004.30518\n",
      "Epoch 128/1000\n",
      "880/880 [==============================] - 0s 72us/step - loss: 21556.1643 - mean_absolute_error: 21556.1621 - val_loss: 24969.5534 - val_mean_absolute_error: 24969.5508\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 23004.30518\n",
      "Epoch 129/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 21309.2507 - mean_absolute_error: 21309.2520 - val_loss: 23512.1832 - val_mean_absolute_error: 23512.1816\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 23004.30518\n",
      "Epoch 130/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 21998.8879 - mean_absolute_error: 21998.8887 - val_loss: 22855.5052 - val_mean_absolute_error: 22855.5039\n",
      "\n",
      "Epoch 00130: val_loss improved from 23004.30518 to 22855.50518, saving model to weights.hdf5\n",
      "Epoch 131/1000\n",
      "880/880 [==============================] - 0s 71us/step - loss: 21098.7544 - mean_absolute_error: 21098.7559 - val_loss: 23638.2093 - val_mean_absolute_error: 23638.2090\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 22855.50518\n",
      "Epoch 132/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 20982.3674 - mean_absolute_error: 20982.3691 - val_loss: 23951.5392 - val_mean_absolute_error: 23951.5391\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 22855.50518\n",
      "Epoch 133/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 20791.4200 - mean_absolute_error: 20791.4180 - val_loss: 22984.6475 - val_mean_absolute_error: 22984.6484\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 22855.50518\n",
      "Epoch 134/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 20886.3592 - mean_absolute_error: 20886.3594 - val_loss: 23216.9630 - val_mean_absolute_error: 23216.9629\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 22855.50518\n",
      "Epoch 135/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 20679.9461 - mean_absolute_error: 20679.9473 - val_loss: 22799.2527 - val_mean_absolute_error: 22799.2539\n",
      "\n",
      "Epoch 00135: val_loss improved from 22855.50518 to 22799.25266, saving model to weights.hdf5\n",
      "Epoch 136/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 20705.3510 - mean_absolute_error: 20705.3516 - val_loss: 25148.8594 - val_mean_absolute_error: 25148.8613\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 22799.25266\n",
      "Epoch 137/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 21311.1972 - mean_absolute_error: 21311.1973 - val_loss: 23172.5732 - val_mean_absolute_error: 23172.5742\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 22799.25266\n",
      "Epoch 138/1000\n",
      "880/880 [==============================] - 0s 69us/step - loss: 20907.6300 - mean_absolute_error: 20907.6309 - val_loss: 22750.7816 - val_mean_absolute_error: 22750.7812\n",
      "\n",
      "Epoch 00138: val_loss improved from 22799.25266 to 22750.78161, saving model to weights.hdf5\n",
      "Epoch 139/1000\n",
      "880/880 [==============================] - 0s 67us/step - loss: 20796.6629 - mean_absolute_error: 20796.6621 - val_loss: 22726.5169 - val_mean_absolute_error: 22726.5176\n",
      "\n",
      "Epoch 00139: val_loss improved from 22750.78161 to 22726.51694, saving model to weights.hdf5\n",
      "Epoch 140/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 20828.0409 - mean_absolute_error: 20828.0410 - val_loss: 23029.5247 - val_mean_absolute_error: 23029.5254\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 22726.51694\n",
      "Epoch 141/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 20941.5800 - mean_absolute_error: 20941.5801 - val_loss: 23261.2120 - val_mean_absolute_error: 23261.2109\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 22726.51694\n",
      "Epoch 142/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 21044.9604 - mean_absolute_error: 21044.9590 - val_loss: 22948.6378 - val_mean_absolute_error: 22948.6387\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 22726.51694\n",
      "Epoch 143/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 21140.2064 - mean_absolute_error: 21140.2051 - val_loss: 22760.6254 - val_mean_absolute_error: 22760.6250\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 22726.51694\n",
      "Epoch 144/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 20566.2164 - mean_absolute_error: 20566.2188 - val_loss: 23062.7932 - val_mean_absolute_error: 23062.7930\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 22726.51694\n",
      "Epoch 145/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 21195.8732 - mean_absolute_error: 21195.8750 - val_loss: 25444.0826 - val_mean_absolute_error: 25444.0820\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 22726.51694\n",
      "Epoch 146/1000\n",
      "880/880 [==============================] - 0s 69us/step - loss: 20618.5122 - mean_absolute_error: 20618.5137 - val_loss: 23011.1485 - val_mean_absolute_error: 23011.1484\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 22726.51694\n",
      "Epoch 147/1000\n",
      "880/880 [==============================] - 0s 67us/step - loss: 20273.0688 - mean_absolute_error: 20273.0703 - val_loss: 22695.0607 - val_mean_absolute_error: 22695.0605\n",
      "\n",
      "Epoch 00147: val_loss improved from 22726.51694 to 22695.06069, saving model to weights.hdf5\n",
      "Epoch 148/1000\n",
      "880/880 [==============================] - 0s 66us/step - loss: 20540.0346 - mean_absolute_error: 20540.0371 - val_loss: 22540.9667 - val_mean_absolute_error: 22540.9629\n",
      "\n",
      "Epoch 00148: val_loss improved from 22695.06069 to 22540.96673, saving model to weights.hdf5\n",
      "Epoch 149/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 20879.1493 - mean_absolute_error: 20879.1484 - val_loss: 24587.7549 - val_mean_absolute_error: 24587.7559\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 22540.96673\n",
      "Epoch 150/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 20827.2247 - mean_absolute_error: 20827.2227 - val_loss: 25647.5761 - val_mean_absolute_error: 25647.5781\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 22540.96673\n",
      "Epoch 151/1000\n",
      "880/880 [==============================] - 0s 88us/step - loss: 20930.5293 - mean_absolute_error: 20930.5254 - val_loss: 22443.7406 - val_mean_absolute_error: 22443.7402\n",
      "\n",
      "Epoch 00151: val_loss improved from 22540.96673 to 22443.74055, saving model to weights.hdf5\n",
      "Epoch 152/1000\n",
      "880/880 [==============================] - 0s 60us/step - loss: 20634.0952 - mean_absolute_error: 20634.0938 - val_loss: 23949.6552 - val_mean_absolute_error: 23949.6562\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 22443.74055\n",
      "Epoch 153/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 20546.9945 - mean_absolute_error: 20546.9961 - val_loss: 24145.4973 - val_mean_absolute_error: 24145.4961\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 22443.74055\n",
      "Epoch 154/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 20549.7467 - mean_absolute_error: 20549.7480 - val_loss: 25222.5247 - val_mean_absolute_error: 25222.5254\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 22443.74055\n",
      "Epoch 155/1000\n",
      "880/880 [==============================] - 0s 66us/step - loss: 21056.0212 - mean_absolute_error: 21056.0195 - val_loss: 23900.2110 - val_mean_absolute_error: 23900.2109\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 22443.74055\n",
      "Epoch 156/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 21028.6618 - mean_absolute_error: 21028.6621 - val_loss: 22026.4454 - val_mean_absolute_error: 22026.4453\n",
      "\n",
      "Epoch 00156: val_loss improved from 22443.74055 to 22026.44542, saving model to weights.hdf5\n",
      "Epoch 157/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 20016.6799 - mean_absolute_error: 20016.6797 - val_loss: 22038.9419 - val_mean_absolute_error: 22038.9434\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 22026.44542\n",
      "Epoch 158/1000\n",
      "880/880 [==============================] - 0s 67us/step - loss: 20628.1320 - mean_absolute_error: 20628.1348 - val_loss: 23395.0784 - val_mean_absolute_error: 23395.0801\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 22026.44542\n",
      "Epoch 159/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 20270.2235 - mean_absolute_error: 20270.2227 - val_loss: 25015.0676 - val_mean_absolute_error: 25015.0684\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 22026.44542\n",
      "Epoch 160/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 20915.2965 - mean_absolute_error: 20915.2949 - val_loss: 23144.6938 - val_mean_absolute_error: 23144.6934\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 22026.44542\n",
      "Epoch 161/1000\n",
      "880/880 [==============================] - 0s 69us/step - loss: 20052.4306 - mean_absolute_error: 20052.4297 - val_loss: 21954.7518 - val_mean_absolute_error: 21954.7520\n",
      "\n",
      "Epoch 00161: val_loss improved from 22026.44542 to 21954.75181, saving model to weights.hdf5\n",
      "Epoch 162/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 19787.6132 - mean_absolute_error: 19787.6113 - val_loss: 23100.5597 - val_mean_absolute_error: 23100.5586\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 21954.75181\n",
      "Epoch 163/1000\n",
      "880/880 [==============================] - 0s 67us/step - loss: 20395.6617 - mean_absolute_error: 20395.6562 - val_loss: 22003.1100 - val_mean_absolute_error: 22003.1074\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 21954.75181\n",
      "Epoch 164/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 20283.7237 - mean_absolute_error: 20283.7227 - val_loss: 23744.2747 - val_mean_absolute_error: 23744.2754\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 21954.75181\n",
      "Epoch 165/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 20651.0962 - mean_absolute_error: 20651.0938 - val_loss: 22258.2441 - val_mean_absolute_error: 22258.2461\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 21954.75181\n",
      "Epoch 166/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 19705.1194 - mean_absolute_error: 19705.1211 - val_loss: 21623.7105 - val_mean_absolute_error: 21623.7090\n",
      "\n",
      "Epoch 00166: val_loss improved from 21954.75181 to 21623.71051, saving model to weights.hdf5\n",
      "Epoch 167/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 20091.1500 - mean_absolute_error: 20091.1504 - val_loss: 22686.0226 - val_mean_absolute_error: 22686.0234\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 21623.71051\n",
      "Epoch 168/1000\n",
      "880/880 [==============================] - 0s 66us/step - loss: 19926.4266 - mean_absolute_error: 19926.4258 - val_loss: 21618.7347 - val_mean_absolute_error: 21618.7344\n",
      "\n",
      "Epoch 00168: val_loss improved from 21623.71051 to 21618.73473, saving model to weights.hdf5\n",
      "Epoch 169/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 20726.7936 - mean_absolute_error: 20726.7930 - val_loss: 23251.4362 - val_mean_absolute_error: 23251.4355\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 21618.73473\n",
      "Epoch 170/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 19921.3787 - mean_absolute_error: 19921.3809 - val_loss: 21758.6602 - val_mean_absolute_error: 21758.6582\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 21618.73473\n",
      "Epoch 171/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 19538.5541 - mean_absolute_error: 19538.5547 - val_loss: 22021.6472 - val_mean_absolute_error: 22021.6484\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 21618.73473\n",
      "Epoch 172/1000\n",
      "880/880 [==============================] - 0s 66us/step - loss: 19654.0370 - mean_absolute_error: 19654.0410 - val_loss: 21553.9414 - val_mean_absolute_error: 21553.9414\n",
      "\n",
      "Epoch 00172: val_loss improved from 21618.73473 to 21553.94137, saving model to weights.hdf5\n",
      "Epoch 173/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 19925.4199 - mean_absolute_error: 19925.4219 - val_loss: 22077.3225 - val_mean_absolute_error: 22077.3223\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 21553.94137\n",
      "Epoch 174/1000\n",
      "880/880 [==============================] - 0s 66us/step - loss: 19884.7809 - mean_absolute_error: 19884.7812 - val_loss: 21530.1169 - val_mean_absolute_error: 21530.1191\n",
      "\n",
      "Epoch 00174: val_loss improved from 21553.94137 to 21530.11687, saving model to weights.hdf5\n",
      "Epoch 175/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 20147.8511 - mean_absolute_error: 20147.8516 - val_loss: 21929.2268 - val_mean_absolute_error: 21929.2266\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 21530.11687\n",
      "Epoch 176/1000\n",
      "880/880 [==============================] - 0s 68us/step - loss: 20130.0126 - mean_absolute_error: 20130.0156 - val_loss: 21735.7468 - val_mean_absolute_error: 21735.7461\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 21530.11687\n",
      "Epoch 177/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 19666.5799 - mean_absolute_error: 19666.5820 - val_loss: 21402.2915 - val_mean_absolute_error: 21402.2910\n",
      "\n",
      "Epoch 00177: val_loss improved from 21530.11687 to 21402.29155, saving model to weights.hdf5\n",
      "Epoch 178/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 19198.7994 - mean_absolute_error: 19198.7969 - val_loss: 21917.3452 - val_mean_absolute_error: 21917.3438\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 21402.29155\n",
      "Epoch 179/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 19874.9553 - mean_absolute_error: 19874.9551 - val_loss: 23628.7051 - val_mean_absolute_error: 23628.7051\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 21402.29155\n",
      "Epoch 180/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 21989.7748 - mean_absolute_error: 21989.7734 - val_loss: 21446.2452 - val_mean_absolute_error: 21446.2461\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 21402.29155\n",
      "Epoch 181/1000\n",
      "880/880 [==============================] - 0s 66us/step - loss: 19525.1817 - mean_absolute_error: 19525.1797 - val_loss: 22670.6258 - val_mean_absolute_error: 22670.6250\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 21402.29155\n",
      "Epoch 182/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 19829.7001 - mean_absolute_error: 19829.6973 - val_loss: 22749.8580 - val_mean_absolute_error: 22749.8574\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 21402.29155\n",
      "Epoch 183/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 19466.4945 - mean_absolute_error: 19466.4961 - val_loss: 20841.3768 - val_mean_absolute_error: 20841.3770\n",
      "\n",
      "Epoch 00183: val_loss improved from 21402.29155 to 20841.37681, saving model to weights.hdf5\n",
      "Epoch 184/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 19607.8799 - mean_absolute_error: 19607.8789 - val_loss: 20845.1477 - val_mean_absolute_error: 20845.1484\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 20841.37681\n",
      "Epoch 185/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 20106.5400 - mean_absolute_error: 20106.5391 - val_loss: 21389.5056 - val_mean_absolute_error: 21389.5059\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 20841.37681\n",
      "Epoch 186/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 20301.4974 - mean_absolute_error: 20301.4980 - val_loss: 20888.3847 - val_mean_absolute_error: 20888.3848\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 20841.37681\n",
      "Epoch 187/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 19548.3305 - mean_absolute_error: 19548.3301 - val_loss: 21319.6673 - val_mean_absolute_error: 21319.6660\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 20841.37681\n",
      "Epoch 188/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 19260.8642 - mean_absolute_error: 19260.8652 - val_loss: 21461.3956 - val_mean_absolute_error: 21461.3945\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 20841.37681\n",
      "Epoch 189/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 18951.9488 - mean_absolute_error: 18951.9492 - val_loss: 20962.5673 - val_mean_absolute_error: 20962.5684\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 20841.37681\n",
      "Epoch 190/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 18812.1503 - mean_absolute_error: 18812.1523 - val_loss: 20862.9060 - val_mean_absolute_error: 20862.9062\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 20841.37681\n",
      "Epoch 191/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 18861.9449 - mean_absolute_error: 18861.9453 - val_loss: 20582.5381 - val_mean_absolute_error: 20582.5391\n",
      "\n",
      "Epoch 00191: val_loss improved from 20841.37681 to 20582.53807, saving model to weights.hdf5\n",
      "Epoch 192/1000\n",
      "880/880 [==============================] - 0s 79us/step - loss: 19973.4092 - mean_absolute_error: 19973.4082 - val_loss: 21098.0308 - val_mean_absolute_error: 21098.0312\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 20582.53807\n",
      "Epoch 193/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 18966.4445 - mean_absolute_error: 18966.4434 - val_loss: 20910.3298 - val_mean_absolute_error: 20910.3301\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 20582.53807\n",
      "Epoch 194/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 19109.0739 - mean_absolute_error: 19109.0723 - val_loss: 21054.2701 - val_mean_absolute_error: 21054.2676\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 20582.53807\n",
      "Epoch 195/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 20016.8458 - mean_absolute_error: 20016.8457 - val_loss: 22672.2091 - val_mean_absolute_error: 22672.2090\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 20582.53807\n",
      "Epoch 196/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 20400.8783 - mean_absolute_error: 20400.8770 - val_loss: 20765.4214 - val_mean_absolute_error: 20765.4219\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 20582.53807\n",
      "Epoch 197/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 19272.5319 - mean_absolute_error: 19272.5312 - val_loss: 21312.6044 - val_mean_absolute_error: 21312.6055\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 20582.53807\n",
      "Epoch 198/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 18642.9762 - mean_absolute_error: 18642.9766 - val_loss: 20080.6861 - val_mean_absolute_error: 20080.6855\n",
      "\n",
      "Epoch 00198: val_loss improved from 20582.53807 to 20080.68610, saving model to weights.hdf5\n",
      "Epoch 199/1000\n",
      "880/880 [==============================] - 0s 66us/step - loss: 18701.4032 - mean_absolute_error: 18701.4043 - val_loss: 21133.6707 - val_mean_absolute_error: 21133.6699\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 20080.68610\n",
      "Epoch 200/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 19127.8192 - mean_absolute_error: 19127.8184 - val_loss: 20225.4447 - val_mean_absolute_error: 20225.4453\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 20080.68610\n",
      "Epoch 201/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 19898.6522 - mean_absolute_error: 19898.6504 - val_loss: 20715.2912 - val_mean_absolute_error: 20715.2910\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 20080.68610\n",
      "Epoch 202/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 19302.3791 - mean_absolute_error: 19302.3809 - val_loss: 20621.2022 - val_mean_absolute_error: 20621.2031\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 20080.68610\n",
      "Epoch 203/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 18838.1675 - mean_absolute_error: 18838.1680 - val_loss: 20384.5235 - val_mean_absolute_error: 20384.5254\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 20080.68610\n",
      "Epoch 204/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 19523.6704 - mean_absolute_error: 19523.6699 - val_loss: 20171.6082 - val_mean_absolute_error: 20171.6074\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 20080.68610\n",
      "Epoch 205/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 18312.0434 - mean_absolute_error: 18312.0430 - val_loss: 20164.6715 - val_mean_absolute_error: 20164.6719\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 20080.68610\n",
      "Epoch 206/1000\n",
      "880/880 [==============================] - 0s 69us/step - loss: 18704.8989 - mean_absolute_error: 18704.9004 - val_loss: 19805.9239 - val_mean_absolute_error: 19805.9258\n",
      "\n",
      "Epoch 00206: val_loss improved from 20080.68610 to 19805.92388, saving model to weights.hdf5\n",
      "Epoch 207/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 18469.8157 - mean_absolute_error: 18469.8164 - val_loss: 21649.8344 - val_mean_absolute_error: 21649.8340\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 19805.92388\n",
      "Epoch 208/1000\n",
      "880/880 [==============================] - 0s 68us/step - loss: 19696.0924 - mean_absolute_error: 19696.0938 - val_loss: 20029.9086 - val_mean_absolute_error: 20029.9082\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 19805.92388\n",
      "Epoch 209/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 18737.1186 - mean_absolute_error: 18737.1191 - val_loss: 19617.2350 - val_mean_absolute_error: 19617.2344\n",
      "\n",
      "Epoch 00209: val_loss improved from 19805.92388 to 19617.23496, saving model to weights.hdf5\n",
      "Epoch 210/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 18164.6621 - mean_absolute_error: 18164.6602 - val_loss: 19761.3812 - val_mean_absolute_error: 19761.3809\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 19617.23496\n",
      "Epoch 211/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 18276.9221 - mean_absolute_error: 18276.9199 - val_loss: 19880.3401 - val_mean_absolute_error: 19880.3418\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 19617.23496\n",
      "Epoch 212/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 19047.3276 - mean_absolute_error: 19047.3262 - val_loss: 22882.4657 - val_mean_absolute_error: 22882.4668\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 19617.23496\n",
      "Epoch 213/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 18412.4736 - mean_absolute_error: 18412.4746 - val_loss: 20683.7170 - val_mean_absolute_error: 20683.7168\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 19617.23496\n",
      "Epoch 214/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 18502.3687 - mean_absolute_error: 18502.3691 - val_loss: 20018.4326 - val_mean_absolute_error: 20018.4316\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 19617.23496\n",
      "Epoch 215/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 18794.5464 - mean_absolute_error: 18794.5469 - val_loss: 19513.4993 - val_mean_absolute_error: 19513.5000\n",
      "\n",
      "Epoch 00215: val_loss improved from 19617.23496 to 19513.49933, saving model to weights.hdf5\n",
      "Epoch 216/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 17627.5229 - mean_absolute_error: 17627.5215 - val_loss: 19483.6221 - val_mean_absolute_error: 19483.6230\n",
      "\n",
      "Epoch 00216: val_loss improved from 19513.49933 to 19483.62212, saving model to weights.hdf5\n",
      "Epoch 217/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 18586.1800 - mean_absolute_error: 18586.1816 - val_loss: 19410.9722 - val_mean_absolute_error: 19410.9727\n",
      "\n",
      "Epoch 00217: val_loss improved from 19483.62212 to 19410.97221, saving model to weights.hdf5\n",
      "Epoch 218/1000\n",
      "880/880 [==============================] - 0s 67us/step - loss: 18807.2209 - mean_absolute_error: 18807.2207 - val_loss: 20437.6336 - val_mean_absolute_error: 20437.6348\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 19410.97221\n",
      "Epoch 219/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 17749.8175 - mean_absolute_error: 17749.8145 - val_loss: 19252.8611 - val_mean_absolute_error: 19252.8613\n",
      "\n",
      "Epoch 00219: val_loss improved from 19410.97221 to 19252.86106, saving model to weights.hdf5\n",
      "Epoch 220/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 17956.8909 - mean_absolute_error: 17956.8906 - val_loss: 19727.2260 - val_mean_absolute_error: 19727.2246\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 19252.86106\n",
      "Epoch 221/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 17956.0809 - mean_absolute_error: 17956.0820 - val_loss: 19398.7682 - val_mean_absolute_error: 19398.7676\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 19252.86106\n",
      "Epoch 222/1000\n",
      "880/880 [==============================] - 0s 67us/step - loss: 17874.2747 - mean_absolute_error: 17874.2754 - val_loss: 20639.4674 - val_mean_absolute_error: 20639.4688\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 19252.86106\n",
      "Epoch 223/1000\n",
      "880/880 [==============================] - 0s 70us/step - loss: 17775.9355 - mean_absolute_error: 17775.9336 - val_loss: 19237.6980 - val_mean_absolute_error: 19237.6973\n",
      "\n",
      "Epoch 00223: val_loss improved from 19252.86106 to 19237.69805, saving model to weights.hdf5\n",
      "Epoch 224/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 17804.4407 - mean_absolute_error: 17804.4395 - val_loss: 18925.1863 - val_mean_absolute_error: 18925.1855\n",
      "\n",
      "Epoch 00224: val_loss improved from 19237.69805 to 18925.18626, saving model to weights.hdf5\n",
      "Epoch 225/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 17974.4540 - mean_absolute_error: 17974.4531 - val_loss: 22945.0676 - val_mean_absolute_error: 22945.0664\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 18925.18626\n",
      "Epoch 226/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 18968.5757 - mean_absolute_error: 18968.5762 - val_loss: 18822.0315 - val_mean_absolute_error: 18822.0312\n",
      "\n",
      "Epoch 00226: val_loss improved from 18925.18626 to 18822.03150, saving model to weights.hdf5\n",
      "Epoch 227/1000\n",
      "880/880 [==============================] - 0s 60us/step - loss: 18588.2771 - mean_absolute_error: 18588.2793 - val_loss: 19570.5261 - val_mean_absolute_error: 19570.5273\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 18822.03150\n",
      "Epoch 228/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 18949.7699 - mean_absolute_error: 18949.7734 - val_loss: 19447.0034 - val_mean_absolute_error: 19447.0020\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 18822.03150\n",
      "Epoch 229/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 18854.7342 - mean_absolute_error: 18854.7344 - val_loss: 18850.5348 - val_mean_absolute_error: 18850.5352\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 18822.03150\n",
      "Epoch 230/1000\n",
      "880/880 [==============================] - 0s 66us/step - loss: 19306.3661 - mean_absolute_error: 19306.3691 - val_loss: 20283.4335 - val_mean_absolute_error: 20283.4336\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 18822.03150\n",
      "Epoch 231/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 18226.3201 - mean_absolute_error: 18226.3203 - val_loss: 18957.0956 - val_mean_absolute_error: 18957.0957\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 18822.03150\n",
      "Epoch 232/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 18650.1793 - mean_absolute_error: 18650.1777 - val_loss: 21943.8249 - val_mean_absolute_error: 21943.8242\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 18822.03150\n",
      "Epoch 233/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 18156.0277 - mean_absolute_error: 18156.0293 - val_loss: 19746.4280 - val_mean_absolute_error: 19746.4277\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 18822.03150\n",
      "Epoch 234/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 18080.4796 - mean_absolute_error: 18080.4805 - val_loss: 19028.8769 - val_mean_absolute_error: 19028.8789\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 18822.03150\n",
      "Epoch 235/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 18365.4553 - mean_absolute_error: 18365.4551 - val_loss: 22846.3129 - val_mean_absolute_error: 22846.3145\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 18822.03150\n",
      "Epoch 236/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 17650.5474 - mean_absolute_error: 17650.5469 - val_loss: 19140.3194 - val_mean_absolute_error: 19140.3184\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 18822.03150\n",
      "Epoch 237/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 17311.0295 - mean_absolute_error: 17311.0293 - val_loss: 21329.5736 - val_mean_absolute_error: 21329.5723\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 18822.03150\n",
      "Epoch 238/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 17898.7776 - mean_absolute_error: 17898.7793 - val_loss: 18992.1369 - val_mean_absolute_error: 18992.1367\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 18822.03150\n",
      "Epoch 239/1000\n",
      "880/880 [==============================] - 0s 71us/step - loss: 18358.6598 - mean_absolute_error: 18358.6602 - val_loss: 18466.6665 - val_mean_absolute_error: 18466.6680\n",
      "\n",
      "Epoch 00239: val_loss improved from 18822.03150 to 18466.66646, saving model to weights.hdf5\n",
      "Epoch 240/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 17709.3386 - mean_absolute_error: 17709.3398 - val_loss: 18464.0154 - val_mean_absolute_error: 18464.0137\n",
      "\n",
      "Epoch 00240: val_loss improved from 18466.66646 to 18464.01539, saving model to weights.hdf5\n",
      "Epoch 241/1000\n",
      "880/880 [==============================] - 0s 60us/step - loss: 17858.3856 - mean_absolute_error: 17858.3848 - val_loss: 23411.2631 - val_mean_absolute_error: 23411.2617\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 18464.01539\n",
      "Epoch 242/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 18259.2074 - mean_absolute_error: 18259.2070 - val_loss: 18865.9096 - val_mean_absolute_error: 18865.9082\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 18464.01539\n",
      "Epoch 243/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 17541.8838 - mean_absolute_error: 17541.8828 - val_loss: 18708.8543 - val_mean_absolute_error: 18708.8555\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 18464.01539\n",
      "Epoch 244/1000\n",
      "880/880 [==============================] - 0s 66us/step - loss: 18480.3583 - mean_absolute_error: 18480.3574 - val_loss: 18774.3939 - val_mean_absolute_error: 18774.3926\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 18464.01539\n",
      "Epoch 245/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 17499.3634 - mean_absolute_error: 17499.3652 - val_loss: 18836.7024 - val_mean_absolute_error: 18836.7031\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 18464.01539\n",
      "Epoch 246/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 17532.4753 - mean_absolute_error: 17532.4766 - val_loss: 18401.3812 - val_mean_absolute_error: 18401.3809\n",
      "\n",
      "Epoch 00246: val_loss improved from 18464.01539 to 18401.38121, saving model to weights.hdf5\n",
      "Epoch 247/1000\n",
      "880/880 [==============================] - 0s 71us/step - loss: 17775.9680 - mean_absolute_error: 17775.9668 - val_loss: 20328.5062 - val_mean_absolute_error: 20328.5059\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 18401.38121\n",
      "Epoch 248/1000\n",
      "880/880 [==============================] - 0s 67us/step - loss: 18316.5182 - mean_absolute_error: 18316.5176 - val_loss: 18623.3559 - val_mean_absolute_error: 18623.3555\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 18401.38121\n",
      "Epoch 249/1000\n",
      "880/880 [==============================] - 0s 67us/step - loss: 18572.8340 - mean_absolute_error: 18572.8340 - val_loss: 19521.0135 - val_mean_absolute_error: 19521.0137\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 18401.38121\n",
      "Epoch 250/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 18211.9198 - mean_absolute_error: 18211.9199 - val_loss: 20123.0696 - val_mean_absolute_error: 20123.0684\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 18401.38121\n",
      "Epoch 251/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 17486.1450 - mean_absolute_error: 17486.1445 - val_loss: 20126.8230 - val_mean_absolute_error: 20126.8223\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 18401.38121\n",
      "Epoch 252/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 17246.5286 - mean_absolute_error: 17246.5293 - val_loss: 18810.1039 - val_mean_absolute_error: 18810.1055\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 18401.38121\n",
      "Epoch 253/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 17173.6463 - mean_absolute_error: 17173.6445 - val_loss: 18175.6562 - val_mean_absolute_error: 18175.6562\n",
      "\n",
      "Epoch 00253: val_loss improved from 18401.38121 to 18175.65620, saving model to weights.hdf5\n",
      "Epoch 254/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 17104.4121 - mean_absolute_error: 17104.4121 - val_loss: 17819.7352 - val_mean_absolute_error: 17819.7363\n",
      "\n",
      "Epoch 00254: val_loss improved from 18175.65620 to 17819.73517, saving model to weights.hdf5\n",
      "Epoch 255/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 17587.6854 - mean_absolute_error: 17587.6836 - val_loss: 19166.3188 - val_mean_absolute_error: 19166.3184\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 17819.73517\n",
      "Epoch 256/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 17628.8512 - mean_absolute_error: 17628.8496 - val_loss: 21040.0730 - val_mean_absolute_error: 21040.0723\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 17819.73517\n",
      "Epoch 257/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 19270.5386 - mean_absolute_error: 19270.5391 - val_loss: 17740.2394 - val_mean_absolute_error: 17740.2402\n",
      "\n",
      "Epoch 00257: val_loss improved from 17819.73517 to 17740.23940, saving model to weights.hdf5\n",
      "Epoch 258/1000\n",
      "880/880 [==============================] - 0s 60us/step - loss: 17881.9381 - mean_absolute_error: 17881.9355 - val_loss: 18478.7487 - val_mean_absolute_error: 18478.7480\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 17740.23940\n",
      "Epoch 259/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 17984.2820 - mean_absolute_error: 17984.2812 - val_loss: 18575.4981 - val_mean_absolute_error: 18575.4980\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 17740.23940\n",
      "Epoch 260/1000\n",
      "880/880 [==============================] - 0s 67us/step - loss: 17382.4298 - mean_absolute_error: 17382.4297 - val_loss: 18188.6491 - val_mean_absolute_error: 18188.6484\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 17740.23940\n",
      "Epoch 261/1000\n",
      "880/880 [==============================] - 0s 67us/step - loss: 18015.8027 - mean_absolute_error: 18015.8027 - val_loss: 17967.1278 - val_mean_absolute_error: 17967.1289\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 17740.23940\n",
      "Epoch 262/1000\n",
      "880/880 [==============================] - 0s 60us/step - loss: 17945.4420 - mean_absolute_error: 17945.4434 - val_loss: 21157.2336 - val_mean_absolute_error: 21157.2344\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 17740.23940\n",
      "Epoch 263/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 17840.3901 - mean_absolute_error: 17840.3906 - val_loss: 19430.2903 - val_mean_absolute_error: 19430.2910\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 17740.23940\n",
      "Epoch 264/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 17808.8488 - mean_absolute_error: 17808.8457 - val_loss: 20231.7934 - val_mean_absolute_error: 20231.7930\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 17740.23940\n",
      "Epoch 265/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 16852.7153 - mean_absolute_error: 16852.7168 - val_loss: 18560.2944 - val_mean_absolute_error: 18560.2949\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 17740.23940\n",
      "Epoch 266/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 17207.0829 - mean_absolute_error: 17207.0820 - val_loss: 18028.0807 - val_mean_absolute_error: 18028.0820\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 17740.23940\n",
      "Epoch 267/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 17265.8134 - mean_absolute_error: 17265.8145 - val_loss: 18317.4107 - val_mean_absolute_error: 18317.4102\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 17740.23940\n",
      "Epoch 268/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 16617.7085 - mean_absolute_error: 16617.7070 - val_loss: 17545.8160 - val_mean_absolute_error: 17545.8164\n",
      "\n",
      "Epoch 00268: val_loss improved from 17740.23940 to 17545.81602, saving model to weights.hdf5\n",
      "Epoch 269/1000\n",
      "880/880 [==============================] - 0s 71us/step - loss: 17746.5407 - mean_absolute_error: 17746.5410 - val_loss: 18259.7463 - val_mean_absolute_error: 18259.7461\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 17545.81602\n",
      "Epoch 270/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 17549.2946 - mean_absolute_error: 17549.2949 - val_loss: 17544.9413 - val_mean_absolute_error: 17544.9414\n",
      "\n",
      "Epoch 00270: val_loss improved from 17545.81602 to 17544.94132, saving model to weights.hdf5\n",
      "Epoch 271/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 18251.8920 - mean_absolute_error: 18251.8945 - val_loss: 18676.0823 - val_mean_absolute_error: 18676.0820\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 17544.94132\n",
      "Epoch 272/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 16710.9820 - mean_absolute_error: 16710.9844 - val_loss: 17645.6735 - val_mean_absolute_error: 17645.6738\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 17544.94132\n",
      "Epoch 273/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 17520.6287 - mean_absolute_error: 17520.6289 - val_loss: 18194.5630 - val_mean_absolute_error: 18194.5645\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 17544.94132\n",
      "Epoch 274/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 18182.8472 - mean_absolute_error: 18182.8477 - val_loss: 17544.4588 - val_mean_absolute_error: 17544.4590\n",
      "\n",
      "Epoch 00274: val_loss improved from 17544.94132 to 17544.45884, saving model to weights.hdf5\n",
      "Epoch 275/1000\n",
      "880/880 [==============================] - 0s 68us/step - loss: 16874.3014 - mean_absolute_error: 16874.3008 - val_loss: 17541.8143 - val_mean_absolute_error: 17541.8145\n",
      "\n",
      "Epoch 00275: val_loss improved from 17544.45884 to 17541.81429, saving model to weights.hdf5\n",
      "Epoch 276/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 17675.9003 - mean_absolute_error: 17675.9004 - val_loss: 19501.3833 - val_mean_absolute_error: 19501.3848\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 17541.81429\n",
      "Epoch 277/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 17120.2994 - mean_absolute_error: 17120.3008 - val_loss: 18649.4468 - val_mean_absolute_error: 18649.4473\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 17541.81429\n",
      "Epoch 278/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 16638.6322 - mean_absolute_error: 16638.6309 - val_loss: 17410.3200 - val_mean_absolute_error: 17410.3184\n",
      "\n",
      "Epoch 00278: val_loss improved from 17541.81429 to 17410.32001, saving model to weights.hdf5\n",
      "Epoch 279/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 18222.4601 - mean_absolute_error: 18222.4629 - val_loss: 21435.0092 - val_mean_absolute_error: 21435.0098\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 17410.32001\n",
      "Epoch 280/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 18312.2573 - mean_absolute_error: 18312.2559 - val_loss: 17762.4311 - val_mean_absolute_error: 17762.4316\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 17410.32001\n",
      "Epoch 281/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 16959.4785 - mean_absolute_error: 16959.4785 - val_loss: 17506.9243 - val_mean_absolute_error: 17506.9238\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 17410.32001\n",
      "Epoch 282/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 16484.7828 - mean_absolute_error: 16484.7832 - val_loss: 18070.6837 - val_mean_absolute_error: 18070.6855\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 17410.32001\n",
      "Epoch 283/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 16369.8240 - mean_absolute_error: 16369.8223 - val_loss: 20218.1931 - val_mean_absolute_error: 20218.1934\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 17410.32001\n",
      "Epoch 284/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 16594.1500 - mean_absolute_error: 16594.1484 - val_loss: 17515.3175 - val_mean_absolute_error: 17515.3184\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 17410.32001\n",
      "Epoch 285/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 17114.1175 - mean_absolute_error: 17114.1152 - val_loss: 20602.7747 - val_mean_absolute_error: 20602.7754\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 17410.32001\n",
      "Epoch 286/1000\n",
      "880/880 [==============================] - 0s 70us/step - loss: 16874.3741 - mean_absolute_error: 16874.3750 - val_loss: 17235.7756 - val_mean_absolute_error: 17235.7754\n",
      "\n",
      "Epoch 00286: val_loss improved from 17410.32001 to 17235.77559, saving model to weights.hdf5\n",
      "Epoch 287/1000\n",
      "880/880 [==============================] - 0s 69us/step - loss: 16831.3960 - mean_absolute_error: 16831.3984 - val_loss: 18901.5351 - val_mean_absolute_error: 18901.5352\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 17235.77559\n",
      "Epoch 288/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 17211.1061 - mean_absolute_error: 17211.1055 - val_loss: 17713.1401 - val_mean_absolute_error: 17713.1406\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 17235.77559\n",
      "Epoch 289/1000\n",
      "880/880 [==============================] - 0s 60us/step - loss: 17747.6547 - mean_absolute_error: 17747.6543 - val_loss: 19008.6916 - val_mean_absolute_error: 19008.6914\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 17235.77559\n",
      "Epoch 290/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 16676.1045 - mean_absolute_error: 16676.1016 - val_loss: 18231.5194 - val_mean_absolute_error: 18231.5195\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 17235.77559\n",
      "Epoch 291/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 16663.3087 - mean_absolute_error: 16663.3086 - val_loss: 17984.3808 - val_mean_absolute_error: 17984.3809\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 17235.77559\n",
      "Epoch 292/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 16690.4415 - mean_absolute_error: 16690.4414 - val_loss: 18341.0571 - val_mean_absolute_error: 18341.0586\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 17235.77559\n",
      "Epoch 293/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 16937.2906 - mean_absolute_error: 16937.2910 - val_loss: 17331.5054 - val_mean_absolute_error: 17331.5059\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 17235.77559\n",
      "Epoch 294/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 17233.0993 - mean_absolute_error: 17233.0977 - val_loss: 21918.3718 - val_mean_absolute_error: 21918.3730\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 17235.77559\n",
      "Epoch 295/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 17097.4304 - mean_absolute_error: 17097.4316 - val_loss: 17151.9648 - val_mean_absolute_error: 17151.9668\n",
      "\n",
      "Epoch 00295: val_loss improved from 17235.77559 to 17151.96479, saving model to weights.hdf5\n",
      "Epoch 296/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 16384.7788 - mean_absolute_error: 16384.7793 - val_loss: 17354.8010 - val_mean_absolute_error: 17354.8008\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 17151.96479\n",
      "Epoch 297/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 16657.4104 - mean_absolute_error: 16657.4141 - val_loss: 20711.8357 - val_mean_absolute_error: 20711.8359\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 17151.96479\n",
      "Epoch 298/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 18152.9096 - mean_absolute_error: 18152.9121 - val_loss: 18122.5254 - val_mean_absolute_error: 18122.5254\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 17151.96479\n",
      "Epoch 299/1000\n",
      "880/880 [==============================] - 0s 66us/step - loss: 16813.7011 - mean_absolute_error: 16813.7031 - val_loss: 17048.6817 - val_mean_absolute_error: 17048.6816\n",
      "\n",
      "Epoch 00299: val_loss improved from 17151.96479 to 17048.68171, saving model to weights.hdf5\n",
      "Epoch 300/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 16794.8945 - mean_absolute_error: 16794.8926 - val_loss: 18225.6012 - val_mean_absolute_error: 18225.6016\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 17048.68171\n",
      "Epoch 301/1000\n",
      "880/880 [==============================] - 0s 66us/step - loss: 16486.7249 - mean_absolute_error: 16486.7246 - val_loss: 20587.5738 - val_mean_absolute_error: 20587.5723\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 17048.68171\n",
      "Epoch 302/1000\n",
      "880/880 [==============================] - 0s 70us/step - loss: 16535.0121 - mean_absolute_error: 16535.0117 - val_loss: 17266.7560 - val_mean_absolute_error: 17266.7539\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 17048.68171\n",
      "Epoch 303/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 17118.8879 - mean_absolute_error: 17118.8867 - val_loss: 22333.9150 - val_mean_absolute_error: 22333.9160\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 17048.68171\n",
      "Epoch 304/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 17976.4122 - mean_absolute_error: 17976.4121 - val_loss: 17398.0747 - val_mean_absolute_error: 17398.0742\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 17048.68171\n",
      "Epoch 305/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 16749.9027 - mean_absolute_error: 16749.9004 - val_loss: 17170.4939 - val_mean_absolute_error: 17170.4941\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 17048.68171\n",
      "Epoch 306/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 17482.9781 - mean_absolute_error: 17482.9805 - val_loss: 18547.4085 - val_mean_absolute_error: 18547.4082\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 17048.68171\n",
      "Epoch 307/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 16862.8570 - mean_absolute_error: 16862.8574 - val_loss: 17077.2328 - val_mean_absolute_error: 17077.2344\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 17048.68171\n",
      "Epoch 308/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 16415.9925 - mean_absolute_error: 16415.9941 - val_loss: 22608.9460 - val_mean_absolute_error: 22608.9453\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 17048.68171\n",
      "Epoch 309/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 16908.5951 - mean_absolute_error: 16908.5957 - val_loss: 17503.9762 - val_mean_absolute_error: 17503.9766\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 17048.68171\n",
      "Epoch 310/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 16841.1643 - mean_absolute_error: 16841.1641 - val_loss: 20708.2577 - val_mean_absolute_error: 20708.2559\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 17048.68171\n",
      "Epoch 311/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 17152.8712 - mean_absolute_error: 17152.8711 - val_loss: 17745.1262 - val_mean_absolute_error: 17745.1250\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 17048.68171\n",
      "Epoch 312/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 16378.0041 - mean_absolute_error: 16378.0049 - val_loss: 17107.2750 - val_mean_absolute_error: 17107.2754\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 17048.68171\n",
      "Epoch 313/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 17055.3125 - mean_absolute_error: 17055.3105 - val_loss: 17208.2764 - val_mean_absolute_error: 17208.2754\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 17048.68171\n",
      "Epoch 314/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 16123.9361 - mean_absolute_error: 16123.9355 - val_loss: 17771.7507 - val_mean_absolute_error: 17771.7500\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 17048.68171\n",
      "Epoch 315/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 16572.7626 - mean_absolute_error: 16572.7617 - val_loss: 17616.9922 - val_mean_absolute_error: 17616.9922\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 17048.68171\n",
      "Epoch 316/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 15951.4568 - mean_absolute_error: 15951.4561 - val_loss: 17116.0362 - val_mean_absolute_error: 17116.0371\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 17048.68171\n",
      "Epoch 317/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 16224.6877 - mean_absolute_error: 16224.6855 - val_loss: 16833.6528 - val_mean_absolute_error: 16833.6523\n",
      "\n",
      "Epoch 00317: val_loss improved from 17048.68171 to 16833.65275, saving model to weights.hdf5\n",
      "Epoch 318/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 16734.9640 - mean_absolute_error: 16734.9629 - val_loss: 17446.0386 - val_mean_absolute_error: 17446.0391\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 16833.65275\n",
      "Epoch 319/1000\n",
      "880/880 [==============================] - 0s 70us/step - loss: 16700.3651 - mean_absolute_error: 16700.3652 - val_loss: 19772.8931 - val_mean_absolute_error: 19772.8906\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 16833.65275\n",
      "Epoch 320/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 17068.9697 - mean_absolute_error: 17068.9688 - val_loss: 19868.6778 - val_mean_absolute_error: 19868.6777\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 16833.65275\n",
      "Epoch 321/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 17596.3870 - mean_absolute_error: 17596.3867 - val_loss: 16760.4001 - val_mean_absolute_error: 16760.4004\n",
      "\n",
      "Epoch 00321: val_loss improved from 16833.65275 to 16760.40009, saving model to weights.hdf5\n",
      "Epoch 322/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 16290.4998 - mean_absolute_error: 16290.4990 - val_loss: 17103.5721 - val_mean_absolute_error: 17103.5723\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 16760.40009\n",
      "Epoch 323/1000\n",
      "880/880 [==============================] - 0s 67us/step - loss: 16260.8220 - mean_absolute_error: 16260.8213 - val_loss: 16741.9522 - val_mean_absolute_error: 16741.9512\n",
      "\n",
      "Epoch 00323: val_loss improved from 16760.40009 to 16741.95217, saving model to weights.hdf5\n",
      "Epoch 324/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 15782.5977 - mean_absolute_error: 15782.5977 - val_loss: 16917.2468 - val_mean_absolute_error: 16917.2461\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 16741.95217\n",
      "Epoch 325/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 16047.1279 - mean_absolute_error: 16047.1279 - val_loss: 17314.9189 - val_mean_absolute_error: 17314.9199\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 16741.95217\n",
      "Epoch 326/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 16945.8159 - mean_absolute_error: 16945.8164 - val_loss: 17744.6354 - val_mean_absolute_error: 17744.6348\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 16741.95217\n",
      "Epoch 327/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 18591.6477 - mean_absolute_error: 18591.6484 - val_loss: 20616.6902 - val_mean_absolute_error: 20616.6895\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 16741.95217\n",
      "Epoch 328/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 17322.7530 - mean_absolute_error: 17322.7520 - val_loss: 16788.5206 - val_mean_absolute_error: 16788.5195\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 16741.95217\n",
      "Epoch 329/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 16221.3998 - mean_absolute_error: 16221.4004 - val_loss: 16788.5077 - val_mean_absolute_error: 16788.5078\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 16741.95217\n",
      "Epoch 330/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 15891.4665 - mean_absolute_error: 15891.4678 - val_loss: 16661.0379 - val_mean_absolute_error: 16661.0391\n",
      "\n",
      "Epoch 00330: val_loss improved from 16741.95217 to 16661.03794, saving model to weights.hdf5\n",
      "Epoch 331/1000\n",
      "880/880 [==============================] - 0s 66us/step - loss: 16726.0128 - mean_absolute_error: 16726.0117 - val_loss: 19083.9780 - val_mean_absolute_error: 19083.9805\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 16661.03794\n",
      "Epoch 332/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 16259.2703 - mean_absolute_error: 16259.2725 - val_loss: 18694.8353 - val_mean_absolute_error: 18694.8340\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 16661.03794\n",
      "Epoch 333/1000\n",
      "880/880 [==============================] - 0s 66us/step - loss: 17392.0430 - mean_absolute_error: 17392.0410 - val_loss: 16618.6470 - val_mean_absolute_error: 16618.6465\n",
      "\n",
      "Epoch 00333: val_loss improved from 16661.03794 to 16618.64700, saving model to weights.hdf5\n",
      "Epoch 334/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 16290.2148 - mean_absolute_error: 16290.2148 - val_loss: 16812.1211 - val_mean_absolute_error: 16812.1211\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 16618.64700\n",
      "Epoch 335/1000\n",
      "880/880 [==============================] - 0s 71us/step - loss: 15801.3389 - mean_absolute_error: 15801.3389 - val_loss: 17093.2859 - val_mean_absolute_error: 17093.2852\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 16618.64700\n",
      "Epoch 336/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 16232.7491 - mean_absolute_error: 16232.7490 - val_loss: 16776.8767 - val_mean_absolute_error: 16776.8789\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 16618.64700\n",
      "Epoch 337/1000\n",
      "880/880 [==============================] - 0s 59us/step - loss: 16276.8011 - mean_absolute_error: 16276.8008 - val_loss: 16558.6774 - val_mean_absolute_error: 16558.6777\n",
      "\n",
      "Epoch 00337: val_loss improved from 16618.64700 to 16558.67738, saving model to weights.hdf5\n",
      "Epoch 338/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 16490.2952 - mean_absolute_error: 16490.2949 - val_loss: 17169.1664 - val_mean_absolute_error: 17169.1660\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 16558.67738\n",
      "Epoch 339/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 16472.3706 - mean_absolute_error: 16472.3711 - val_loss: 17265.9027 - val_mean_absolute_error: 17265.9023\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 16558.67738\n",
      "Epoch 340/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 15794.6520 - mean_absolute_error: 15794.6543 - val_loss: 19476.9572 - val_mean_absolute_error: 19476.9570\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 16558.67738\n",
      "Epoch 341/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 17301.3877 - mean_absolute_error: 17301.3848 - val_loss: 18626.8705 - val_mean_absolute_error: 18626.8711\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 16558.67738\n",
      "Epoch 342/1000\n",
      "880/880 [==============================] - 0s 59us/step - loss: 16155.9358 - mean_absolute_error: 16155.9365 - val_loss: 18164.9077 - val_mean_absolute_error: 18164.9062\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 16558.67738\n",
      "Epoch 343/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 16054.3400 - mean_absolute_error: 16054.3398 - val_loss: 16515.3682 - val_mean_absolute_error: 16515.3691\n",
      "\n",
      "Epoch 00343: val_loss improved from 16558.67738 to 16515.36824, saving model to weights.hdf5\n",
      "Epoch 344/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 15489.2142 - mean_absolute_error: 15489.2129 - val_loss: 16839.3916 - val_mean_absolute_error: 16839.3906\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 16515.36824\n",
      "Epoch 345/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 16336.1938 - mean_absolute_error: 16336.1963 - val_loss: 19735.8753 - val_mean_absolute_error: 19735.8750\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 16515.36824\n",
      "Epoch 346/1000\n",
      "880/880 [==============================] - 0s 66us/step - loss: 16628.4440 - mean_absolute_error: 16628.4434 - val_loss: 17727.6868 - val_mean_absolute_error: 17727.6875\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 16515.36824\n",
      "Epoch 347/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 16162.1382 - mean_absolute_error: 16162.1396 - val_loss: 16952.6603 - val_mean_absolute_error: 16952.6602\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 16515.36824\n",
      "Epoch 348/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 15833.6406 - mean_absolute_error: 15833.6406 - val_loss: 16798.4031 - val_mean_absolute_error: 16798.4023\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 16515.36824\n",
      "Epoch 349/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 16698.3179 - mean_absolute_error: 16698.3164 - val_loss: 20216.3626 - val_mean_absolute_error: 20216.3633\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 16515.36824\n",
      "Epoch 350/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 16288.2763 - mean_absolute_error: 16288.2754 - val_loss: 17098.2434 - val_mean_absolute_error: 17098.2441\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 16515.36824\n",
      "Epoch 351/1000\n",
      "880/880 [==============================] - 0s 68us/step - loss: 15692.7833 - mean_absolute_error: 15692.7832 - val_loss: 16969.1407 - val_mean_absolute_error: 16969.1406\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 16515.36824\n",
      "Epoch 352/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 16388.2661 - mean_absolute_error: 16388.2656 - val_loss: 16854.6662 - val_mean_absolute_error: 16854.6680\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 16515.36824\n",
      "Epoch 353/1000\n",
      "880/880 [==============================] - 0s 59us/step - loss: 17319.9013 - mean_absolute_error: 17319.9043 - val_loss: 16554.0918 - val_mean_absolute_error: 16554.0918\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 16515.36824\n",
      "Epoch 354/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 15831.6211 - mean_absolute_error: 15831.6230 - val_loss: 18302.6399 - val_mean_absolute_error: 18302.6406\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 16515.36824\n",
      "Epoch 355/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 15815.2070 - mean_absolute_error: 15815.2080 - val_loss: 19240.9067 - val_mean_absolute_error: 19240.9062\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 16515.36824\n",
      "Epoch 356/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 17552.4729 - mean_absolute_error: 17552.4727 - val_loss: 17328.2091 - val_mean_absolute_error: 17328.2090\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 16515.36824\n",
      "Epoch 357/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 15806.1850 - mean_absolute_error: 15806.1875 - val_loss: 16356.0868 - val_mean_absolute_error: 16356.0879\n",
      "\n",
      "Epoch 00357: val_loss improved from 16515.36824 to 16356.08683, saving model to weights.hdf5\n",
      "Epoch 358/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 15489.1901 - mean_absolute_error: 15489.1904 - val_loss: 16506.9965 - val_mean_absolute_error: 16506.9961\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 16356.08683\n",
      "Epoch 359/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 16185.9984 - mean_absolute_error: 16185.9980 - val_loss: 17301.6953 - val_mean_absolute_error: 17301.6934\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 16356.08683\n",
      "Epoch 360/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 15675.2875 - mean_absolute_error: 15675.2871 - val_loss: 17204.8774 - val_mean_absolute_error: 17204.8770\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 16356.08683\n",
      "Epoch 361/1000\n",
      "880/880 [==============================] - 0s 68us/step - loss: 18108.3164 - mean_absolute_error: 18108.3184 - val_loss: 16395.5539 - val_mean_absolute_error: 16395.5527\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 16356.08683\n",
      "Epoch 362/1000\n",
      "880/880 [==============================] - 0s 77us/step - loss: 16154.8252 - mean_absolute_error: 16154.8252 - val_loss: 17029.1846 - val_mean_absolute_error: 17029.1855\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 16356.08683\n",
      "Epoch 363/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 15989.3599 - mean_absolute_error: 15989.3604 - val_loss: 18230.3793 - val_mean_absolute_error: 18230.3789\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 16356.08683\n",
      "Epoch 364/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 15954.0817 - mean_absolute_error: 15954.0830 - val_loss: 23463.1242 - val_mean_absolute_error: 23463.1230\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 16356.08683\n",
      "Epoch 365/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 16205.6621 - mean_absolute_error: 16205.6611 - val_loss: 16935.3418 - val_mean_absolute_error: 16935.3418\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 16356.08683\n",
      "Epoch 366/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 16369.2770 - mean_absolute_error: 16369.2764 - val_loss: 16566.8544 - val_mean_absolute_error: 16566.8535\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 16356.08683\n",
      "Epoch 367/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 16941.3001 - mean_absolute_error: 16941.2988 - val_loss: 16758.5937 - val_mean_absolute_error: 16758.5938\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 16356.08683\n",
      "Epoch 368/1000\n",
      "880/880 [==============================] - 0s 68us/step - loss: 17014.6554 - mean_absolute_error: 17014.6562 - val_loss: 16605.7784 - val_mean_absolute_error: 16605.7793\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 16356.08683\n",
      "Epoch 369/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 15716.9705 - mean_absolute_error: 15716.9707 - val_loss: 16540.5669 - val_mean_absolute_error: 16540.5664\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 16356.08683\n",
      "Epoch 370/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 15494.6935 - mean_absolute_error: 15494.6934 - val_loss: 16384.8454 - val_mean_absolute_error: 16384.8457\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 16356.08683\n",
      "Epoch 371/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 16182.0139 - mean_absolute_error: 16182.0156 - val_loss: 17418.1402 - val_mean_absolute_error: 17418.1406\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 16356.08683\n",
      "Epoch 372/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 16067.7812 - mean_absolute_error: 16067.7803 - val_loss: 19580.1807 - val_mean_absolute_error: 19580.1797\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 16356.08683\n",
      "Epoch 373/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 15946.3885 - mean_absolute_error: 15946.3896 - val_loss: 21613.0767 - val_mean_absolute_error: 21613.0742\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 16356.08683\n",
      "Epoch 374/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 16000.5995 - mean_absolute_error: 16000.5996 - val_loss: 16534.1890 - val_mean_absolute_error: 16534.1895\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 16356.08683\n",
      "Epoch 375/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 15825.3856 - mean_absolute_error: 15825.3838 - val_loss: 17549.2633 - val_mean_absolute_error: 17549.2637\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 16356.08683\n",
      "Epoch 376/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 15379.8188 - mean_absolute_error: 15379.8184 - val_loss: 16385.0176 - val_mean_absolute_error: 16385.0176\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 16356.08683\n",
      "Epoch 377/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 15791.8840 - mean_absolute_error: 15791.8838 - val_loss: 16017.6979 - val_mean_absolute_error: 16017.6992\n",
      "\n",
      "Epoch 00377: val_loss improved from 16356.08683 to 16017.69789, saving model to weights.hdf5\n",
      "Epoch 378/1000\n",
      "880/880 [==============================] - 0s 69us/step - loss: 15787.6225 - mean_absolute_error: 15787.6230 - val_loss: 16711.7830 - val_mean_absolute_error: 16711.7832\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 16017.69789\n",
      "Epoch 379/1000\n",
      "880/880 [==============================] - 0s 69us/step - loss: 15402.3662 - mean_absolute_error: 15402.3662 - val_loss: 16130.2152 - val_mean_absolute_error: 16130.2158\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 16017.69789\n",
      "Epoch 380/1000\n",
      "880/880 [==============================] - 0s 68us/step - loss: 15497.4399 - mean_absolute_error: 15497.4404 - val_loss: 15931.7281 - val_mean_absolute_error: 15931.7295\n",
      "\n",
      "Epoch 00380: val_loss improved from 16017.69789 to 15931.72812, saving model to weights.hdf5\n",
      "Epoch 381/1000\n",
      "880/880 [==============================] - 0s 68us/step - loss: 15100.7382 - mean_absolute_error: 15100.7373 - val_loss: 16699.0124 - val_mean_absolute_error: 16699.0117\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 15931.72812\n",
      "Epoch 382/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 15544.7540 - mean_absolute_error: 15544.7520 - val_loss: 16769.7262 - val_mean_absolute_error: 16769.7246\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 15931.72812\n",
      "Epoch 383/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 15044.9267 - mean_absolute_error: 15044.9258 - val_loss: 16123.4213 - val_mean_absolute_error: 16123.4219\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 15931.72812\n",
      "Epoch 384/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 15549.0337 - mean_absolute_error: 15549.0352 - val_loss: 18704.2583 - val_mean_absolute_error: 18704.2559\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 15931.72812\n",
      "Epoch 385/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 15888.3766 - mean_absolute_error: 15888.3770 - val_loss: 16403.9126 - val_mean_absolute_error: 16403.9121\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 15931.72812\n",
      "Epoch 386/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 15111.9035 - mean_absolute_error: 15111.9043 - val_loss: 16244.0798 - val_mean_absolute_error: 16244.0811\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 15931.72812\n",
      "Epoch 387/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 15214.8960 - mean_absolute_error: 15214.8955 - val_loss: 19406.1281 - val_mean_absolute_error: 19406.1289\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 15931.72812\n",
      "Epoch 388/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 15825.2894 - mean_absolute_error: 15825.2900 - val_loss: 17689.4266 - val_mean_absolute_error: 17689.4258\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 15931.72812\n",
      "Epoch 389/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 16027.5809 - mean_absolute_error: 16027.5791 - val_loss: 16365.8893 - val_mean_absolute_error: 16365.8896\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 15931.72812\n",
      "Epoch 390/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 15587.2236 - mean_absolute_error: 15587.2227 - val_loss: 16331.4838 - val_mean_absolute_error: 16331.4834\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 15931.72812\n",
      "Epoch 391/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 16273.3136 - mean_absolute_error: 16273.3125 - val_loss: 16311.5623 - val_mean_absolute_error: 16311.5615\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 15931.72812\n",
      "Epoch 392/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 15466.3874 - mean_absolute_error: 15466.3877 - val_loss: 16801.8543 - val_mean_absolute_error: 16801.8555\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 15931.72812\n",
      "Epoch 393/1000\n",
      "880/880 [==============================] - 0s 67us/step - loss: 15452.3665 - mean_absolute_error: 15452.3682 - val_loss: 16219.5655 - val_mean_absolute_error: 16219.5654\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 15931.72812\n",
      "Epoch 394/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 15509.3982 - mean_absolute_error: 15509.3975 - val_loss: 15667.7865 - val_mean_absolute_error: 15667.7861\n",
      "\n",
      "Epoch 00394: val_loss improved from 15931.72812 to 15667.78651, saving model to weights.hdf5\n",
      "Epoch 395/1000\n",
      "880/880 [==============================] - 0s 70us/step - loss: 15381.1675 - mean_absolute_error: 15381.1680 - val_loss: 16503.3898 - val_mean_absolute_error: 16503.3906\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 15667.78651\n",
      "Epoch 396/1000\n",
      "880/880 [==============================] - 0s 69us/step - loss: 15713.5373 - mean_absolute_error: 15713.5361 - val_loss: 17457.0544 - val_mean_absolute_error: 17457.0547\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 15667.78651\n",
      "Epoch 397/1000\n",
      "880/880 [==============================] - 0s 70us/step - loss: 15520.7844 - mean_absolute_error: 15520.7852 - val_loss: 16909.2293 - val_mean_absolute_error: 16909.2305\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 15667.78651\n",
      "Epoch 398/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 16114.1387 - mean_absolute_error: 16114.1396 - val_loss: 17586.1747 - val_mean_absolute_error: 17586.1758\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 15667.78651\n",
      "Epoch 399/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 16483.3621 - mean_absolute_error: 16483.3613 - val_loss: 16130.9070 - val_mean_absolute_error: 16130.9072\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 15667.78651\n",
      "Epoch 400/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 15727.8009 - mean_absolute_error: 15727.7998 - val_loss: 16508.4492 - val_mean_absolute_error: 16508.4492\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 15667.78651\n",
      "Epoch 401/1000\n",
      "880/880 [==============================] - 0s 68us/step - loss: 15904.7854 - mean_absolute_error: 15904.7871 - val_loss: 16334.6150 - val_mean_absolute_error: 16334.6152\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 15667.78651\n",
      "Epoch 402/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 15627.6680 - mean_absolute_error: 15627.6680 - val_loss: 18265.8755 - val_mean_absolute_error: 18265.8750\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 15667.78651\n",
      "Epoch 403/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 15211.1014 - mean_absolute_error: 15211.1016 - val_loss: 16256.3911 - val_mean_absolute_error: 16256.3906\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 15667.78651\n",
      "Epoch 404/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 16460.6995 - mean_absolute_error: 16460.6973 - val_loss: 16395.6052 - val_mean_absolute_error: 16395.6055\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 15667.78651\n",
      "Epoch 405/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 17119.2488 - mean_absolute_error: 17119.2480 - val_loss: 15925.9445 - val_mean_absolute_error: 15925.9453\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 15667.78651\n",
      "Epoch 406/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 15255.9130 - mean_absolute_error: 15255.9141 - val_loss: 17683.1332 - val_mean_absolute_error: 17683.1328\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 15667.78651\n",
      "Epoch 407/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 15090.9748 - mean_absolute_error: 15090.9727 - val_loss: 16181.5887 - val_mean_absolute_error: 16181.5889\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 15667.78651\n",
      "Epoch 408/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 16008.9309 - mean_absolute_error: 16008.9307 - val_loss: 16768.2332 - val_mean_absolute_error: 16768.2324\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 15667.78651\n",
      "Epoch 409/1000\n",
      "880/880 [==============================] - 0s 66us/step - loss: 15503.2668 - mean_absolute_error: 15503.2666 - val_loss: 15653.4206 - val_mean_absolute_error: 15653.4209\n",
      "\n",
      "Epoch 00409: val_loss improved from 15667.78651 to 15653.42061, saving model to weights.hdf5\n",
      "Epoch 410/1000\n",
      "880/880 [==============================] - 0s 69us/step - loss: 16350.5389 - mean_absolute_error: 16350.5361 - val_loss: 23570.3360 - val_mean_absolute_error: 23570.3359\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 15653.42061\n",
      "Epoch 411/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 17090.6526 - mean_absolute_error: 17090.6523 - val_loss: 15816.8146 - val_mean_absolute_error: 15816.8145\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 15653.42061\n",
      "Epoch 412/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 15331.0961 - mean_absolute_error: 15331.0967 - val_loss: 15947.5636 - val_mean_absolute_error: 15947.5635\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 15653.42061\n",
      "Epoch 413/1000\n",
      "880/880 [==============================] - 0s 67us/step - loss: 15855.8183 - mean_absolute_error: 15855.8193 - val_loss: 16033.4073 - val_mean_absolute_error: 16033.4072\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 15653.42061\n",
      "Epoch 414/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 15092.5462 - mean_absolute_error: 15092.5479 - val_loss: 17636.3914 - val_mean_absolute_error: 17636.3926\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 15653.42061\n",
      "Epoch 415/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 15193.1408 - mean_absolute_error: 15193.1396 - val_loss: 16323.7364 - val_mean_absolute_error: 16323.7363\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 15653.42061\n",
      "Epoch 416/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 15397.4860 - mean_absolute_error: 15397.4844 - val_loss: 16259.4033 - val_mean_absolute_error: 16259.4023\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 15653.42061\n",
      "Epoch 417/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 15199.0101 - mean_absolute_error: 15199.0127 - val_loss: 15929.1600 - val_mean_absolute_error: 15929.1602\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 15653.42061\n",
      "Epoch 418/1000\n",
      "880/880 [==============================] - 0s 67us/step - loss: 16725.0504 - mean_absolute_error: 16725.0527 - val_loss: 18442.5793 - val_mean_absolute_error: 18442.5801\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 15653.42061\n",
      "Epoch 419/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 15249.3528 - mean_absolute_error: 15249.3525 - val_loss: 15831.1550 - val_mean_absolute_error: 15831.1543\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 15653.42061\n",
      "Epoch 420/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 14817.7565 - mean_absolute_error: 14817.7529 - val_loss: 16214.4652 - val_mean_absolute_error: 16214.4658\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 15653.42061\n",
      "Epoch 421/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 15168.0052 - mean_absolute_error: 15168.0049 - val_loss: 15625.8159 - val_mean_absolute_error: 15625.8145\n",
      "\n",
      "Epoch 00421: val_loss improved from 15653.42061 to 15625.81589, saving model to weights.hdf5\n",
      "Epoch 422/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 16208.1666 - mean_absolute_error: 16208.1670 - val_loss: 15833.4882 - val_mean_absolute_error: 15833.4873\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 15625.81589\n",
      "Epoch 423/1000\n",
      "880/880 [==============================] - 0s 72us/step - loss: 16615.8511 - mean_absolute_error: 16615.8496 - val_loss: 15974.0178 - val_mean_absolute_error: 15974.0186\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 15625.81589\n",
      "Epoch 424/1000\n",
      "880/880 [==============================] - 0s 69us/step - loss: 15063.5131 - mean_absolute_error: 15063.5146 - val_loss: 20435.1821 - val_mean_absolute_error: 20435.1816\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 15625.81589\n",
      "Epoch 425/1000\n",
      "880/880 [==============================] - 0s 68us/step - loss: 15306.0680 - mean_absolute_error: 15306.0684 - val_loss: 18984.2770 - val_mean_absolute_error: 18984.2773\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 15625.81589\n",
      "Epoch 426/1000\n",
      "880/880 [==============================] - 0s 67us/step - loss: 15709.9494 - mean_absolute_error: 15709.9502 - val_loss: 15919.5730 - val_mean_absolute_error: 15919.5723\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 15625.81589\n",
      "Epoch 427/1000\n",
      "880/880 [==============================] - 0s 69us/step - loss: 15652.4771 - mean_absolute_error: 15652.4766 - val_loss: 17527.0480 - val_mean_absolute_error: 17527.0469\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 15625.81589\n",
      "Epoch 428/1000\n",
      "880/880 [==============================] - 0s 68us/step - loss: 15259.6179 - mean_absolute_error: 15259.6172 - val_loss: 15620.4661 - val_mean_absolute_error: 15620.4658\n",
      "\n",
      "Epoch 00428: val_loss improved from 15625.81589 to 15620.46605, saving model to weights.hdf5\n",
      "Epoch 429/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 15376.7879 - mean_absolute_error: 15376.7891 - val_loss: 18315.1400 - val_mean_absolute_error: 18315.1406\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 15620.46605\n",
      "Epoch 430/1000\n",
      "880/880 [==============================] - 0s 70us/step - loss: 14653.4221 - mean_absolute_error: 14653.4238 - val_loss: 15964.3159 - val_mean_absolute_error: 15964.3154\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 15620.46605\n",
      "Epoch 431/1000\n",
      "880/880 [==============================] - 0s 69us/step - loss: 15007.4412 - mean_absolute_error: 15007.4395 - val_loss: 15399.0939 - val_mean_absolute_error: 15399.0928\n",
      "\n",
      "Epoch 00431: val_loss improved from 15620.46605 to 15399.09395, saving model to weights.hdf5\n",
      "Epoch 432/1000\n",
      "880/880 [==============================] - 0s 69us/step - loss: 15256.8482 - mean_absolute_error: 15256.8496 - val_loss: 16779.5185 - val_mean_absolute_error: 16779.5176\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 15399.09395\n",
      "Epoch 433/1000\n",
      "880/880 [==============================] - 0s 78us/step - loss: 15198.5199 - mean_absolute_error: 15198.5195 - val_loss: 15877.7933 - val_mean_absolute_error: 15877.7930\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 15399.09395\n",
      "Epoch 434/1000\n",
      "880/880 [==============================] - 0s 67us/step - loss: 15235.7278 - mean_absolute_error: 15235.7285 - val_loss: 15761.7682 - val_mean_absolute_error: 15761.7686\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 15399.09395\n",
      "Epoch 435/1000\n",
      "880/880 [==============================] - 0s 66us/step - loss: 16462.1957 - mean_absolute_error: 16462.1953 - val_loss: 16638.3831 - val_mean_absolute_error: 16638.3809\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 15399.09395\n",
      "Epoch 436/1000\n",
      "880/880 [==============================] - 0s 67us/step - loss: 17960.8509 - mean_absolute_error: 17960.8496 - val_loss: 15856.5128 - val_mean_absolute_error: 15856.5137\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 15399.09395\n",
      "Epoch 437/1000\n",
      "880/880 [==============================] - 0s 68us/step - loss: 16395.3603 - mean_absolute_error: 16395.3574 - val_loss: 16193.7822 - val_mean_absolute_error: 16193.7832\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 15399.09395\n",
      "Epoch 438/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 15836.1029 - mean_absolute_error: 15836.1035 - val_loss: 15691.6731 - val_mean_absolute_error: 15691.6729\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 15399.09395\n",
      "Epoch 439/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 14704.8975 - mean_absolute_error: 14704.8975 - val_loss: 15738.8216 - val_mean_absolute_error: 15738.8223\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 15399.09395\n",
      "Epoch 440/1000\n",
      "880/880 [==============================] - 0s 59us/step - loss: 14691.4165 - mean_absolute_error: 14691.4160 - val_loss: 16147.6934 - val_mean_absolute_error: 16147.6934\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 15399.09395\n",
      "Epoch 441/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 15647.0639 - mean_absolute_error: 15647.0635 - val_loss: 16873.4026 - val_mean_absolute_error: 16873.4023\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 15399.09395\n",
      "Epoch 442/1000\n",
      "880/880 [==============================] - 0s 76us/step - loss: 16599.0346 - mean_absolute_error: 16599.0332 - val_loss: 15642.5783 - val_mean_absolute_error: 15642.5771\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 15399.09395\n",
      "Epoch 443/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 14880.5964 - mean_absolute_error: 14880.5967 - val_loss: 18476.2503 - val_mean_absolute_error: 18476.2520\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 15399.09395\n",
      "Epoch 444/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 14911.7753 - mean_absolute_error: 14911.7734 - val_loss: 17446.4653 - val_mean_absolute_error: 17446.4668\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 15399.09395\n",
      "Epoch 445/1000\n",
      "880/880 [==============================] - 0s 71us/step - loss: 15668.0668 - mean_absolute_error: 15668.0654 - val_loss: 15820.0243 - val_mean_absolute_error: 15820.0254\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 15399.09395\n",
      "Epoch 446/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 15842.4225 - mean_absolute_error: 15842.4229 - val_loss: 16070.2720 - val_mean_absolute_error: 16070.2715\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 15399.09395\n",
      "Epoch 447/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 14844.3521 - mean_absolute_error: 14844.3525 - val_loss: 17177.4637 - val_mean_absolute_error: 17177.4629\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 15399.09395\n",
      "Epoch 448/1000\n",
      "880/880 [==============================] - 0s 66us/step - loss: 14762.7357 - mean_absolute_error: 14762.7354 - val_loss: 15632.4288 - val_mean_absolute_error: 15632.4287\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 15399.09395\n",
      "Epoch 449/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 14599.1347 - mean_absolute_error: 14599.1348 - val_loss: 16171.6071 - val_mean_absolute_error: 16171.6064\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 15399.09395\n",
      "Epoch 450/1000\n",
      "880/880 [==============================] - 0s 70us/step - loss: 15687.1339 - mean_absolute_error: 15687.1338 - val_loss: 18353.4495 - val_mean_absolute_error: 18353.4492\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 15399.09395\n",
      "Epoch 451/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 17150.5305 - mean_absolute_error: 17150.5312 - val_loss: 16426.0502 - val_mean_absolute_error: 16426.0508\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 15399.09395\n",
      "Epoch 452/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 15835.3277 - mean_absolute_error: 15835.3262 - val_loss: 16734.7276 - val_mean_absolute_error: 16734.7285\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 15399.09395\n",
      "Epoch 453/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 15225.7364 - mean_absolute_error: 15225.7354 - val_loss: 16921.6816 - val_mean_absolute_error: 16921.6816\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 15399.09395\n",
      "Epoch 454/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 14811.4763 - mean_absolute_error: 14811.4785 - val_loss: 15709.2428 - val_mean_absolute_error: 15709.2422\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 15399.09395\n",
      "Epoch 455/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 14769.8589 - mean_absolute_error: 14769.8594 - val_loss: 15599.8628 - val_mean_absolute_error: 15599.8623\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 15399.09395\n",
      "Epoch 456/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 15396.0137 - mean_absolute_error: 15396.0127 - val_loss: 17811.0962 - val_mean_absolute_error: 17811.0957\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 15399.09395\n",
      "Epoch 457/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 15590.7843 - mean_absolute_error: 15590.7842 - val_loss: 18760.7615 - val_mean_absolute_error: 18760.7617\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 15399.09395\n",
      "Epoch 458/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 16577.5451 - mean_absolute_error: 16577.5449 - val_loss: 15643.5987 - val_mean_absolute_error: 15643.5986\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 15399.09395\n",
      "Epoch 459/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 16635.3581 - mean_absolute_error: 16635.3574 - val_loss: 15856.6253 - val_mean_absolute_error: 15856.6260\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 15399.09395\n",
      "Epoch 460/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 14883.3318 - mean_absolute_error: 14883.3320 - val_loss: 15307.0365 - val_mean_absolute_error: 15307.0361\n",
      "\n",
      "Epoch 00460: val_loss improved from 15399.09395 to 15307.03651, saving model to weights.hdf5\n",
      "Epoch 461/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 15152.4444 - mean_absolute_error: 15152.4453 - val_loss: 15619.9524 - val_mean_absolute_error: 15619.9521\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 15307.03651\n",
      "Epoch 462/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 14753.7935 - mean_absolute_error: 14753.7939 - val_loss: 15856.7759 - val_mean_absolute_error: 15856.7764\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 15307.03651\n",
      "Epoch 463/1000\n",
      "880/880 [==============================] - 0s 66us/step - loss: 16039.9353 - mean_absolute_error: 16039.9385 - val_loss: 15851.4735 - val_mean_absolute_error: 15851.4727\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 15307.03651\n",
      "Epoch 464/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 14929.7473 - mean_absolute_error: 14929.7490 - val_loss: 15186.0071 - val_mean_absolute_error: 15186.0068\n",
      "\n",
      "Epoch 00464: val_loss improved from 15307.03651 to 15186.00708, saving model to weights.hdf5\n",
      "Epoch 465/1000\n",
      "880/880 [==============================] - 0s 66us/step - loss: 14760.4158 - mean_absolute_error: 14760.4160 - val_loss: 15670.7560 - val_mean_absolute_error: 15670.7559\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 15186.00708\n",
      "Epoch 466/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 14672.1408 - mean_absolute_error: 14672.1396 - val_loss: 16360.9871 - val_mean_absolute_error: 16360.9863\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 15186.00708\n",
      "Epoch 467/1000\n",
      "880/880 [==============================] - 0s 70us/step - loss: 14500.0423 - mean_absolute_error: 14500.0439 - val_loss: 16796.2289 - val_mean_absolute_error: 16796.2285\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 15186.00708\n",
      "Epoch 468/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 14343.7270 - mean_absolute_error: 14343.7266 - val_loss: 15888.4111 - val_mean_absolute_error: 15888.4121\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 15186.00708\n",
      "Epoch 469/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 15059.1914 - mean_absolute_error: 15059.1904 - val_loss: 15461.0473 - val_mean_absolute_error: 15461.0479\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 15186.00708\n",
      "Epoch 470/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 15705.1118 - mean_absolute_error: 15705.1123 - val_loss: 15773.5085 - val_mean_absolute_error: 15773.5088\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 15186.00708\n",
      "Epoch 471/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 15161.8014 - mean_absolute_error: 15161.8027 - val_loss: 15097.2116 - val_mean_absolute_error: 15097.2109\n",
      "\n",
      "Epoch 00471: val_loss improved from 15186.00708 to 15097.21163, saving model to weights.hdf5\n",
      "Epoch 472/1000\n",
      "880/880 [==============================] - 0s 67us/step - loss: 14481.6704 - mean_absolute_error: 14481.6689 - val_loss: 15411.8287 - val_mean_absolute_error: 15411.8281\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 15097.21163\n",
      "Epoch 473/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 14435.2208 - mean_absolute_error: 14435.2207 - val_loss: 15263.8085 - val_mean_absolute_error: 15263.8076\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 15097.21163\n",
      "Epoch 474/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 15494.1358 - mean_absolute_error: 15494.1348 - val_loss: 16215.8471 - val_mean_absolute_error: 16215.8477\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 15097.21163\n",
      "Epoch 475/1000\n",
      "880/880 [==============================] - 0s 67us/step - loss: 15983.9305 - mean_absolute_error: 15983.9307 - val_loss: 15660.5592 - val_mean_absolute_error: 15660.5596\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 15097.21163\n",
      "Epoch 476/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 14537.5922 - mean_absolute_error: 14537.5918 - val_loss: 18271.6591 - val_mean_absolute_error: 18271.6582\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 15097.21163\n",
      "Epoch 477/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 14795.8959 - mean_absolute_error: 14795.8955 - val_loss: 15914.3365 - val_mean_absolute_error: 15914.3359\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 15097.21163\n",
      "Epoch 478/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 14501.1484 - mean_absolute_error: 14501.1475 - val_loss: 16398.4640 - val_mean_absolute_error: 16398.4629\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 15097.21163\n",
      "Epoch 479/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 14365.9868 - mean_absolute_error: 14365.9863 - val_loss: 15239.7662 - val_mean_absolute_error: 15239.7656\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 15097.21163\n",
      "Epoch 480/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 14718.0969 - mean_absolute_error: 14718.0977 - val_loss: 16209.4297 - val_mean_absolute_error: 16209.4297\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 15097.21163\n",
      "Epoch 481/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 15047.0483 - mean_absolute_error: 15047.0479 - val_loss: 16067.9049 - val_mean_absolute_error: 16067.9043\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 15097.21163\n",
      "Epoch 482/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 15472.0641 - mean_absolute_error: 15472.0635 - val_loss: 15611.5183 - val_mean_absolute_error: 15611.5186\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 15097.21163\n",
      "Epoch 483/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 15446.2548 - mean_absolute_error: 15446.2549 - val_loss: 16259.7509 - val_mean_absolute_error: 16259.7520\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 15097.21163\n",
      "Epoch 484/1000\n",
      "880/880 [==============================] - 0s 72us/step - loss: 14154.7660 - mean_absolute_error: 14154.7666 - val_loss: 15774.6925 - val_mean_absolute_error: 15774.6934\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 15097.21163\n",
      "Epoch 485/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 14461.6691 - mean_absolute_error: 14461.6680 - val_loss: 15169.3007 - val_mean_absolute_error: 15169.3008\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 15097.21163\n",
      "Epoch 486/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 14835.9115 - mean_absolute_error: 14835.9111 - val_loss: 21881.0877 - val_mean_absolute_error: 21881.0879\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 15097.21163\n",
      "Epoch 487/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 14857.8656 - mean_absolute_error: 14857.8662 - val_loss: 15339.9289 - val_mean_absolute_error: 15339.9297\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 15097.21163\n",
      "Epoch 488/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 14847.0196 - mean_absolute_error: 14847.0205 - val_loss: 16826.2110 - val_mean_absolute_error: 16826.2129\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 15097.21163\n",
      "Epoch 489/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 14978.9165 - mean_absolute_error: 14978.9160 - val_loss: 19580.6029 - val_mean_absolute_error: 19580.6016\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 15097.21163\n",
      "Epoch 490/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 15471.4560 - mean_absolute_error: 15471.4531 - val_loss: 16086.4461 - val_mean_absolute_error: 16086.4453\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 15097.21163\n",
      "Epoch 491/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 16538.9328 - mean_absolute_error: 16538.9336 - val_loss: 18349.1005 - val_mean_absolute_error: 18349.0996\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 15097.21163\n",
      "Epoch 492/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 15925.6724 - mean_absolute_error: 15925.6738 - val_loss: 18566.4251 - val_mean_absolute_error: 18566.4258\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 15097.21163\n",
      "Epoch 493/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 14412.8148 - mean_absolute_error: 14412.8145 - val_loss: 15213.0737 - val_mean_absolute_error: 15213.0752\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 15097.21163\n",
      "Epoch 494/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 14238.6399 - mean_absolute_error: 14238.6416 - val_loss: 15791.2919 - val_mean_absolute_error: 15791.2920\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 15097.21163\n",
      "Epoch 495/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 13979.9001 - mean_absolute_error: 13979.9004 - val_loss: 15102.8578 - val_mean_absolute_error: 15102.8594\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 15097.21163\n",
      "Epoch 496/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 13953.8923 - mean_absolute_error: 13953.8916 - val_loss: 15828.3568 - val_mean_absolute_error: 15828.3564\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 15097.21163\n",
      "Epoch 497/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 15876.1766 - mean_absolute_error: 15876.1777 - val_loss: 17770.4753 - val_mean_absolute_error: 17770.4746\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 15097.21163\n",
      "Epoch 498/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 15977.6681 - mean_absolute_error: 15977.6670 - val_loss: 17714.7414 - val_mean_absolute_error: 17714.7422\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 15097.21163\n",
      "Epoch 499/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 16764.5410 - mean_absolute_error: 16764.5391 - val_loss: 16244.0749 - val_mean_absolute_error: 16244.0752\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 15097.21163\n",
      "Epoch 500/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 14030.6191 - mean_absolute_error: 14030.6182 - val_loss: 15528.3936 - val_mean_absolute_error: 15528.3945\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 15097.21163\n",
      "Epoch 501/1000\n",
      "880/880 [==============================] - 0s 77us/step - loss: 15606.5830 - mean_absolute_error: 15606.5850 - val_loss: 15353.8093 - val_mean_absolute_error: 15353.8076\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 15097.21163\n",
      "Epoch 502/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 13911.2352 - mean_absolute_error: 13911.2344 - val_loss: 16145.3660 - val_mean_absolute_error: 16145.3662\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 15097.21163\n",
      "Epoch 503/1000\n",
      "880/880 [==============================] - 0s 77us/step - loss: 14351.5606 - mean_absolute_error: 14351.5596 - val_loss: 15350.3274 - val_mean_absolute_error: 15350.3271\n",
      "\n",
      "Epoch 00503: val_loss did not improve from 15097.21163\n",
      "Epoch 504/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 14086.4237 - mean_absolute_error: 14086.4248 - val_loss: 16029.7804 - val_mean_absolute_error: 16029.7822\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 15097.21163\n",
      "Epoch 505/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 14184.1512 - mean_absolute_error: 14184.1523 - val_loss: 16386.0402 - val_mean_absolute_error: 16386.0391\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 15097.21163\n",
      "Epoch 506/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 14349.1601 - mean_absolute_error: 14349.1592 - val_loss: 16505.4645 - val_mean_absolute_error: 16505.4648\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 15097.21163\n",
      "Epoch 507/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 14655.8450 - mean_absolute_error: 14655.8428 - val_loss: 15778.7493 - val_mean_absolute_error: 15778.7490\n",
      "\n",
      "Epoch 00507: val_loss did not improve from 15097.21163\n",
      "Epoch 508/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 14263.5471 - mean_absolute_error: 14263.5479 - val_loss: 17129.2515 - val_mean_absolute_error: 17129.2520\n",
      "\n",
      "Epoch 00508: val_loss did not improve from 15097.21163\n",
      "Epoch 509/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 14123.0614 - mean_absolute_error: 14123.0596 - val_loss: 14895.2637 - val_mean_absolute_error: 14895.2637\n",
      "\n",
      "Epoch 00509: val_loss improved from 15097.21163 to 14895.26369, saving model to weights.hdf5\n",
      "Epoch 510/1000\n",
      "880/880 [==============================] - 0s 59us/step - loss: 13850.8331 - mean_absolute_error: 13850.8320 - val_loss: 14917.0534 - val_mean_absolute_error: 14917.0537\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 14895.26369\n",
      "Epoch 511/1000\n",
      "880/880 [==============================] - 0s 66us/step - loss: 14007.3632 - mean_absolute_error: 14007.3652 - val_loss: 14874.1288 - val_mean_absolute_error: 14874.1299\n",
      "\n",
      "Epoch 00511: val_loss improved from 14895.26369 to 14874.12878, saving model to weights.hdf5\n",
      "Epoch 512/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 14882.5306 - mean_absolute_error: 14882.5293 - val_loss: 19339.0001 - val_mean_absolute_error: 19339.0000\n",
      "\n",
      "Epoch 00512: val_loss did not improve from 14874.12878\n",
      "Epoch 513/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 15509.6354 - mean_absolute_error: 15509.6367 - val_loss: 14981.9635 - val_mean_absolute_error: 14981.9639\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 14874.12878\n",
      "Epoch 514/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 15284.5387 - mean_absolute_error: 15284.5391 - val_loss: 16755.0051 - val_mean_absolute_error: 16755.0039\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 14874.12878\n",
      "Epoch 515/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 14356.3432 - mean_absolute_error: 14356.3428 - val_loss: 15003.6242 - val_mean_absolute_error: 15003.6240\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 14874.12878\n",
      "Epoch 516/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 14253.0215 - mean_absolute_error: 14253.0225 - val_loss: 15047.1640 - val_mean_absolute_error: 15047.1641\n",
      "\n",
      "Epoch 00516: val_loss did not improve from 14874.12878\n",
      "Epoch 517/1000\n",
      "880/880 [==============================] - 0s 66us/step - loss: 14486.6455 - mean_absolute_error: 14486.6455 - val_loss: 14724.1749 - val_mean_absolute_error: 14724.1748\n",
      "\n",
      "Epoch 00517: val_loss improved from 14874.12878 to 14724.17495, saving model to weights.hdf5\n",
      "Epoch 518/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 14250.5773 - mean_absolute_error: 14250.5771 - val_loss: 15132.9276 - val_mean_absolute_error: 15132.9277\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 14724.17495\n",
      "Epoch 519/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 15028.4938 - mean_absolute_error: 15028.4932 - val_loss: 15207.5273 - val_mean_absolute_error: 15207.5273\n",
      "\n",
      "Epoch 00519: val_loss did not improve from 14724.17495\n",
      "Epoch 520/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 15445.2142 - mean_absolute_error: 15445.2139 - val_loss: 15659.2521 - val_mean_absolute_error: 15659.2520\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 14724.17495\n",
      "Epoch 521/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 14361.8785 - mean_absolute_error: 14361.8779 - val_loss: 15095.7692 - val_mean_absolute_error: 15095.7695\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 14724.17495\n",
      "Epoch 522/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 13761.9610 - mean_absolute_error: 13761.9609 - val_loss: 14979.8460 - val_mean_absolute_error: 14979.8467\n",
      "\n",
      "Epoch 00522: val_loss did not improve from 14724.17495\n",
      "Epoch 523/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 13809.5297 - mean_absolute_error: 13809.5283 - val_loss: 15907.4712 - val_mean_absolute_error: 15907.4717\n",
      "\n",
      "Epoch 00523: val_loss did not improve from 14724.17495\n",
      "Epoch 524/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 14403.7091 - mean_absolute_error: 14403.7109 - val_loss: 14819.7249 - val_mean_absolute_error: 14819.7266\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 14724.17495\n",
      "Epoch 525/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 15509.9041 - mean_absolute_error: 15509.9053 - val_loss: 19064.9977 - val_mean_absolute_error: 19064.9980\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 14724.17495\n",
      "Epoch 526/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 15367.9989 - mean_absolute_error: 15367.9980 - val_loss: 15192.7656 - val_mean_absolute_error: 15192.7646\n",
      "\n",
      "Epoch 00526: val_loss did not improve from 14724.17495\n",
      "Epoch 527/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 13927.6207 - mean_absolute_error: 13927.6230 - val_loss: 15603.2226 - val_mean_absolute_error: 15603.2236\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 14724.17495\n",
      "Epoch 528/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 14094.2351 - mean_absolute_error: 14094.2363 - val_loss: 17051.3094 - val_mean_absolute_error: 17051.3086\n",
      "\n",
      "Epoch 00528: val_loss did not improve from 14724.17495\n",
      "Epoch 529/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 14068.5091 - mean_absolute_error: 14068.5078 - val_loss: 15252.9444 - val_mean_absolute_error: 15252.9443\n",
      "\n",
      "Epoch 00529: val_loss did not improve from 14724.17495\n",
      "Epoch 530/1000\n",
      "880/880 [==============================] - 0s 78us/step - loss: 13883.3431 - mean_absolute_error: 13883.3428 - val_loss: 14848.5501 - val_mean_absolute_error: 14848.5508\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 14724.17495\n",
      "Epoch 531/1000\n",
      "880/880 [==============================] - 0s 69us/step - loss: 14644.0291 - mean_absolute_error: 14644.0283 - val_loss: 16677.9841 - val_mean_absolute_error: 16677.9844\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 14724.17495\n",
      "Epoch 532/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 15098.2249 - mean_absolute_error: 15098.2246 - val_loss: 16150.8883 - val_mean_absolute_error: 16150.8877\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 14724.17495\n",
      "Epoch 533/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 14917.8168 - mean_absolute_error: 14917.8174 - val_loss: 14789.9018 - val_mean_absolute_error: 14789.9023\n",
      "\n",
      "Epoch 00533: val_loss did not improve from 14724.17495\n",
      "Epoch 534/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 15422.7563 - mean_absolute_error: 15422.7549 - val_loss: 15134.0552 - val_mean_absolute_error: 15134.0547\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 14724.17495\n",
      "Epoch 535/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 13950.6629 - mean_absolute_error: 13950.6611 - val_loss: 15175.7903 - val_mean_absolute_error: 15175.7910\n",
      "\n",
      "Epoch 00535: val_loss did not improve from 14724.17495\n",
      "Epoch 536/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 13913.6423 - mean_absolute_error: 13913.6436 - val_loss: 15696.8542 - val_mean_absolute_error: 15696.8545\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 14724.17495\n",
      "Epoch 537/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 13669.7177 - mean_absolute_error: 13669.7178 - val_loss: 15644.7695 - val_mean_absolute_error: 15644.7705\n",
      "\n",
      "Epoch 00537: val_loss did not improve from 14724.17495\n",
      "Epoch 538/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 14474.3834 - mean_absolute_error: 14474.3809 - val_loss: 15159.9785 - val_mean_absolute_error: 15159.9795\n",
      "\n",
      "Epoch 00538: val_loss did not improve from 14724.17495\n",
      "Epoch 539/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 14402.2363 - mean_absolute_error: 14402.2363 - val_loss: 16190.5843 - val_mean_absolute_error: 16190.5840\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 14724.17495\n",
      "Epoch 540/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 13710.4477 - mean_absolute_error: 13710.4473 - val_loss: 14593.1855 - val_mean_absolute_error: 14593.1846\n",
      "\n",
      "Epoch 00540: val_loss improved from 14724.17495 to 14593.18551, saving model to weights.hdf5\n",
      "Epoch 541/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 14327.4789 - mean_absolute_error: 14327.4785 - val_loss: 15873.5710 - val_mean_absolute_error: 15873.5703\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 14593.18551\n",
      "Epoch 542/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 13928.9193 - mean_absolute_error: 13928.9209 - val_loss: 14911.1795 - val_mean_absolute_error: 14911.1797\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 14593.18551\n",
      "Epoch 543/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 14302.8173 - mean_absolute_error: 14302.8154 - val_loss: 15475.3561 - val_mean_absolute_error: 15475.3564\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 14593.18551\n",
      "Epoch 544/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 14539.8189 - mean_absolute_error: 14539.8154 - val_loss: 14896.9409 - val_mean_absolute_error: 14896.9395\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 14593.18551\n",
      "Epoch 545/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 13906.4387 - mean_absolute_error: 13906.4385 - val_loss: 15126.7148 - val_mean_absolute_error: 15126.7158\n",
      "\n",
      "Epoch 00545: val_loss did not improve from 14593.18551\n",
      "Epoch 546/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 13970.3919 - mean_absolute_error: 13970.3936 - val_loss: 14894.5517 - val_mean_absolute_error: 14894.5527\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 14593.18551\n",
      "Epoch 547/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 15104.2340 - mean_absolute_error: 15104.2354 - val_loss: 15146.8345 - val_mean_absolute_error: 15146.8340\n",
      "\n",
      "Epoch 00547: val_loss did not improve from 14593.18551\n",
      "Epoch 548/1000\n",
      "880/880 [==============================] - 0s 60us/step - loss: 13934.8208 - mean_absolute_error: 13934.8213 - val_loss: 16296.0308 - val_mean_absolute_error: 16296.0303\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 14593.18551\n",
      "Epoch 549/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 14935.0292 - mean_absolute_error: 14935.0273 - val_loss: 14966.9821 - val_mean_absolute_error: 14966.9814\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 14593.18551\n",
      "Epoch 550/1000\n",
      "880/880 [==============================] - 0s 66us/step - loss: 13993.1007 - mean_absolute_error: 13993.0996 - val_loss: 15797.2518 - val_mean_absolute_error: 15797.2510\n",
      "\n",
      "Epoch 00550: val_loss did not improve from 14593.18551\n",
      "Epoch 551/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 14292.1698 - mean_absolute_error: 14292.1719 - val_loss: 15182.5745 - val_mean_absolute_error: 15182.5742\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 14593.18551\n",
      "Epoch 552/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 13854.5365 - mean_absolute_error: 13854.5352 - val_loss: 14928.1296 - val_mean_absolute_error: 14928.1299\n",
      "\n",
      "Epoch 00552: val_loss did not improve from 14593.18551\n",
      "Epoch 553/1000\n",
      "880/880 [==============================] - 0s 69us/step - loss: 13605.3736 - mean_absolute_error: 13605.3750 - val_loss: 15779.5267 - val_mean_absolute_error: 15779.5264\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 14593.18551\n",
      "Epoch 554/1000\n",
      "880/880 [==============================] - 0s 78us/step - loss: 14133.7059 - mean_absolute_error: 14133.7070 - val_loss: 14997.1100 - val_mean_absolute_error: 14997.1094\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 14593.18551\n",
      "Epoch 555/1000\n",
      "880/880 [==============================] - 0s 66us/step - loss: 13857.2589 - mean_absolute_error: 13857.2578 - val_loss: 14902.1728 - val_mean_absolute_error: 14902.1729\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 14593.18551\n",
      "Epoch 556/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 14167.5897 - mean_absolute_error: 14167.5898 - val_loss: 15405.3400 - val_mean_absolute_error: 15405.3398\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 14593.18551\n",
      "Epoch 557/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 13659.1963 - mean_absolute_error: 13659.1953 - val_loss: 15093.0452 - val_mean_absolute_error: 15093.0439\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 14593.18551\n",
      "Epoch 558/1000\n",
      "880/880 [==============================] - 0s 69us/step - loss: 14719.0698 - mean_absolute_error: 14719.0703 - val_loss: 14727.7501 - val_mean_absolute_error: 14727.7500\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 14593.18551\n",
      "Epoch 559/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 13562.4738 - mean_absolute_error: 13562.4736 - val_loss: 16449.0493 - val_mean_absolute_error: 16449.0488\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 14593.18551\n",
      "Epoch 560/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 13661.8619 - mean_absolute_error: 13661.8604 - val_loss: 14726.3036 - val_mean_absolute_error: 14726.3037\n",
      "\n",
      "Epoch 00560: val_loss did not improve from 14593.18551\n",
      "Epoch 561/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 13921.9121 - mean_absolute_error: 13921.9141 - val_loss: 15167.0432 - val_mean_absolute_error: 15167.0430\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 14593.18551\n",
      "Epoch 562/1000\n",
      "880/880 [==============================] - 0s 67us/step - loss: 14113.7459 - mean_absolute_error: 14113.7441 - val_loss: 14677.2464 - val_mean_absolute_error: 14677.2471\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 14593.18551\n",
      "Epoch 563/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 13633.7875 - mean_absolute_error: 13633.7871 - val_loss: 15073.0375 - val_mean_absolute_error: 15073.0371\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 14593.18551\n",
      "Epoch 564/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 13492.6852 - mean_absolute_error: 13492.6865 - val_loss: 15026.9064 - val_mean_absolute_error: 15026.9043\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 14593.18551\n",
      "Epoch 565/1000\n",
      "880/880 [==============================] - 0s 60us/step - loss: 14019.9016 - mean_absolute_error: 14019.9014 - val_loss: 14810.5086 - val_mean_absolute_error: 14810.5088\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 14593.18551\n",
      "Epoch 566/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 13568.5197 - mean_absolute_error: 13568.5195 - val_loss: 16999.6067 - val_mean_absolute_error: 16999.6074\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 14593.18551\n",
      "Epoch 567/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 14087.1262 - mean_absolute_error: 14087.1260 - val_loss: 17502.6117 - val_mean_absolute_error: 17502.6094\n",
      "\n",
      "Epoch 00567: val_loss did not improve from 14593.18551\n",
      "Epoch 568/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 15227.8172 - mean_absolute_error: 15227.8174 - val_loss: 15521.5829 - val_mean_absolute_error: 15521.5830\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 14593.18551\n",
      "Epoch 569/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 13937.4254 - mean_absolute_error: 13937.4248 - val_loss: 15212.8722 - val_mean_absolute_error: 15212.8721\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 14593.18551\n",
      "Epoch 570/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 13895.3115 - mean_absolute_error: 13895.3105 - val_loss: 14642.7321 - val_mean_absolute_error: 14642.7314\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 14593.18551\n",
      "Epoch 571/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 13302.8942 - mean_absolute_error: 13302.8945 - val_loss: 14978.8887 - val_mean_absolute_error: 14978.8887\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 14593.18551\n",
      "Epoch 572/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 13746.0784 - mean_absolute_error: 13746.0781 - val_loss: 14658.8158 - val_mean_absolute_error: 14658.8145\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 14593.18551\n",
      "Epoch 573/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 13656.6296 - mean_absolute_error: 13656.6279 - val_loss: 14757.7848 - val_mean_absolute_error: 14757.7852\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 14593.18551\n",
      "Epoch 574/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 13571.3022 - mean_absolute_error: 13571.3027 - val_loss: 17482.0534 - val_mean_absolute_error: 17482.0527\n",
      "\n",
      "Epoch 00574: val_loss did not improve from 14593.18551\n",
      "Epoch 575/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 14640.7225 - mean_absolute_error: 14640.7227 - val_loss: 14954.2498 - val_mean_absolute_error: 14954.2500\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 14593.18551\n",
      "Epoch 576/1000\n",
      "880/880 [==============================] - 0s 67us/step - loss: 13354.9740 - mean_absolute_error: 13354.9766 - val_loss: 14539.9910 - val_mean_absolute_error: 14539.9922\n",
      "\n",
      "Epoch 00576: val_loss improved from 14593.18551 to 14539.99102, saving model to weights.hdf5\n",
      "Epoch 577/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 13504.8846 - mean_absolute_error: 13504.8838 - val_loss: 14556.1972 - val_mean_absolute_error: 14556.1973\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 14539.99102\n",
      "Epoch 578/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 13326.4041 - mean_absolute_error: 13326.4033 - val_loss: 14277.1906 - val_mean_absolute_error: 14277.1904\n",
      "\n",
      "Epoch 00578: val_loss improved from 14539.99102 to 14277.19059, saving model to weights.hdf5\n",
      "Epoch 579/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 14331.6165 - mean_absolute_error: 14331.6172 - val_loss: 14791.3812 - val_mean_absolute_error: 14791.3799\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 14277.19059\n",
      "Epoch 580/1000\n",
      "880/880 [==============================] - 0s 66us/step - loss: 14186.1834 - mean_absolute_error: 14186.1807 - val_loss: 15012.6981 - val_mean_absolute_error: 15012.6992\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 14277.19059\n",
      "Epoch 581/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 15322.2553 - mean_absolute_error: 15322.2549 - val_loss: 17776.2983 - val_mean_absolute_error: 17776.2988\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 14277.19059\n",
      "Epoch 582/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 14986.9788 - mean_absolute_error: 14986.9785 - val_loss: 18722.1327 - val_mean_absolute_error: 18722.1309\n",
      "\n",
      "Epoch 00582: val_loss did not improve from 14277.19059\n",
      "Epoch 583/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 15186.4625 - mean_absolute_error: 15186.4600 - val_loss: 14945.8750 - val_mean_absolute_error: 14945.8750\n",
      "\n",
      "Epoch 00583: val_loss did not improve from 14277.19059\n",
      "Epoch 584/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 14107.1007 - mean_absolute_error: 14107.0986 - val_loss: 15152.6040 - val_mean_absolute_error: 15152.6035\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 14277.19059\n",
      "Epoch 585/1000\n",
      "880/880 [==============================] - 0s 68us/step - loss: 13971.5131 - mean_absolute_error: 13971.5137 - val_loss: 15387.8345 - val_mean_absolute_error: 15387.8350\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 14277.19059\n",
      "Epoch 586/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 13835.2591 - mean_absolute_error: 13835.2578 - val_loss: 14769.0309 - val_mean_absolute_error: 14769.0303\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 14277.19059\n",
      "Epoch 587/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 13319.8484 - mean_absolute_error: 13319.8486 - val_loss: 14665.8130 - val_mean_absolute_error: 14665.8135\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 14277.19059\n",
      "Epoch 588/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 13182.0909 - mean_absolute_error: 13182.0889 - val_loss: 14481.4042 - val_mean_absolute_error: 14481.4043\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 14277.19059\n",
      "Epoch 589/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 14097.0510 - mean_absolute_error: 14097.0527 - val_loss: 14813.6177 - val_mean_absolute_error: 14813.6182\n",
      "\n",
      "Epoch 00589: val_loss did not improve from 14277.19059\n",
      "Epoch 590/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 13000.7352 - mean_absolute_error: 13000.7344 - val_loss: 14418.0246 - val_mean_absolute_error: 14418.0254\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 14277.19059\n",
      "Epoch 591/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 14161.2581 - mean_absolute_error: 14161.2568 - val_loss: 15561.3506 - val_mean_absolute_error: 15561.3496\n",
      "\n",
      "Epoch 00591: val_loss did not improve from 14277.19059\n",
      "Epoch 592/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 14056.6420 - mean_absolute_error: 14056.6406 - val_loss: 14132.0705 - val_mean_absolute_error: 14132.0703\n",
      "\n",
      "Epoch 00592: val_loss improved from 14277.19059 to 14132.07047, saving model to weights.hdf5\n",
      "Epoch 593/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 15117.3534 - mean_absolute_error: 15117.3525 - val_loss: 18433.3334 - val_mean_absolute_error: 18433.3320\n",
      "\n",
      "Epoch 00593: val_loss did not improve from 14132.07047\n",
      "Epoch 594/1000\n",
      "880/880 [==============================] - 0s 70us/step - loss: 13999.5872 - mean_absolute_error: 13999.5889 - val_loss: 15223.1395 - val_mean_absolute_error: 15223.1387\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 14132.07047\n",
      "Epoch 595/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 14963.4223 - mean_absolute_error: 14963.4229 - val_loss: 15237.2231 - val_mean_absolute_error: 15237.2227\n",
      "\n",
      "Epoch 00595: val_loss did not improve from 14132.07047\n",
      "Epoch 596/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 15905.1167 - mean_absolute_error: 15905.1162 - val_loss: 17868.0913 - val_mean_absolute_error: 17868.0918\n",
      "\n",
      "Epoch 00596: val_loss did not improve from 14132.07047\n",
      "Epoch 597/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 14021.7069 - mean_absolute_error: 14021.7061 - val_loss: 14438.4928 - val_mean_absolute_error: 14438.4922\n",
      "\n",
      "Epoch 00597: val_loss did not improve from 14132.07047\n",
      "Epoch 598/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 13415.2697 - mean_absolute_error: 13415.2695 - val_loss: 14615.4495 - val_mean_absolute_error: 14615.4502\n",
      "\n",
      "Epoch 00598: val_loss did not improve from 14132.07047\n",
      "Epoch 599/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 13603.0803 - mean_absolute_error: 13603.0830 - val_loss: 14534.0006 - val_mean_absolute_error: 14534.0010\n",
      "\n",
      "Epoch 00599: val_loss did not improve from 14132.07047\n",
      "Epoch 600/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 13294.9729 - mean_absolute_error: 13294.9727 - val_loss: 14500.7524 - val_mean_absolute_error: 14500.7510\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 14132.07047\n",
      "Epoch 601/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 13612.2632 - mean_absolute_error: 13612.2646 - val_loss: 17974.8686 - val_mean_absolute_error: 17974.8691\n",
      "\n",
      "Epoch 00601: val_loss did not improve from 14132.07047\n",
      "Epoch 602/1000\n",
      "880/880 [==============================] - 0s 75us/step - loss: 13384.5000 - mean_absolute_error: 13384.5010 - val_loss: 14868.1339 - val_mean_absolute_error: 14868.1338\n",
      "\n",
      "Epoch 00602: val_loss did not improve from 14132.07047\n",
      "Epoch 603/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 13411.2374 - mean_absolute_error: 13411.2383 - val_loss: 15464.5890 - val_mean_absolute_error: 15464.5889\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 14132.07047\n",
      "Epoch 604/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 13321.6384 - mean_absolute_error: 13321.6367 - val_loss: 14347.9784 - val_mean_absolute_error: 14347.9795\n",
      "\n",
      "Epoch 00604: val_loss did not improve from 14132.07047\n",
      "Epoch 605/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 13050.7825 - mean_absolute_error: 13050.7822 - val_loss: 14597.7480 - val_mean_absolute_error: 14597.7490\n",
      "\n",
      "Epoch 00605: val_loss did not improve from 14132.07047\n",
      "Epoch 606/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 13621.3545 - mean_absolute_error: 13621.3564 - val_loss: 14506.0511 - val_mean_absolute_error: 14506.0508\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 14132.07047\n",
      "Epoch 607/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 13205.2023 - mean_absolute_error: 13205.2021 - val_loss: 15076.9570 - val_mean_absolute_error: 15076.9570\n",
      "\n",
      "Epoch 00607: val_loss did not improve from 14132.07047\n",
      "Epoch 608/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 13578.5423 - mean_absolute_error: 13578.5430 - val_loss: 15545.9603 - val_mean_absolute_error: 15545.9609\n",
      "\n",
      "Epoch 00608: val_loss did not improve from 14132.07047\n",
      "Epoch 609/1000\n",
      "880/880 [==============================] - 0s 72us/step - loss: 13210.1998 - mean_absolute_error: 13210.2012 - val_loss: 15275.5542 - val_mean_absolute_error: 15275.5547\n",
      "\n",
      "Epoch 00609: val_loss did not improve from 14132.07047\n",
      "Epoch 610/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 13622.2468 - mean_absolute_error: 13622.2471 - val_loss: 15069.6764 - val_mean_absolute_error: 15069.6758\n",
      "\n",
      "Epoch 00610: val_loss did not improve from 14132.07047\n",
      "Epoch 611/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 14127.8722 - mean_absolute_error: 14127.8721 - val_loss: 15506.1965 - val_mean_absolute_error: 15506.1973\n",
      "\n",
      "Epoch 00611: val_loss did not improve from 14132.07047\n",
      "Epoch 612/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 13169.8589 - mean_absolute_error: 13169.8604 - val_loss: 14392.4325 - val_mean_absolute_error: 14392.4316\n",
      "\n",
      "Epoch 00612: val_loss did not improve from 14132.07047\n",
      "Epoch 613/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 13581.0369 - mean_absolute_error: 13581.0371 - val_loss: 14965.7361 - val_mean_absolute_error: 14965.7363\n",
      "\n",
      "Epoch 00613: val_loss did not improve from 14132.07047\n",
      "Epoch 614/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 13469.5975 - mean_absolute_error: 13469.5977 - val_loss: 14289.3622 - val_mean_absolute_error: 14289.3613\n",
      "\n",
      "Epoch 00614: val_loss did not improve from 14132.07047\n",
      "Epoch 615/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 13469.0884 - mean_absolute_error: 13469.0879 - val_loss: 16265.4082 - val_mean_absolute_error: 16265.4092\n",
      "\n",
      "Epoch 00615: val_loss did not improve from 14132.07047\n",
      "Epoch 616/1000\n",
      "880/880 [==============================] - 0s 79us/step - loss: 13056.4483 - mean_absolute_error: 13056.4502 - val_loss: 14784.7071 - val_mean_absolute_error: 14784.7070\n",
      "\n",
      "Epoch 00616: val_loss did not improve from 14132.07047\n",
      "Epoch 617/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 13110.1304 - mean_absolute_error: 13110.1299 - val_loss: 14268.3952 - val_mean_absolute_error: 14268.3945\n",
      "\n",
      "Epoch 00617: val_loss did not improve from 14132.07047\n",
      "Epoch 618/1000\n",
      "880/880 [==============================] - 0s 74us/step - loss: 14460.9764 - mean_absolute_error: 14460.9775 - val_loss: 14454.9855 - val_mean_absolute_error: 14454.9863\n",
      "\n",
      "Epoch 00618: val_loss did not improve from 14132.07047\n",
      "Epoch 619/1000\n",
      "880/880 [==============================] - 0s 76us/step - loss: 14676.5233 - mean_absolute_error: 14676.5225 - val_loss: 18349.5483 - val_mean_absolute_error: 18349.5469\n",
      "\n",
      "Epoch 00619: val_loss did not improve from 14132.07047\n",
      "Epoch 620/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 14205.1383 - mean_absolute_error: 14205.1387 - val_loss: 15392.0241 - val_mean_absolute_error: 15392.0254\n",
      "\n",
      "Epoch 00620: val_loss did not improve from 14132.07047\n",
      "Epoch 621/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 12809.5637 - mean_absolute_error: 12809.5635 - val_loss: 14672.3730 - val_mean_absolute_error: 14672.3730\n",
      "\n",
      "Epoch 00621: val_loss did not improve from 14132.07047\n",
      "Epoch 622/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 13029.9177 - mean_absolute_error: 13029.9170 - val_loss: 14525.7270 - val_mean_absolute_error: 14525.7266\n",
      "\n",
      "Epoch 00622: val_loss did not improve from 14132.07047\n",
      "Epoch 623/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 12900.5376 - mean_absolute_error: 12900.5391 - val_loss: 15913.4618 - val_mean_absolute_error: 15913.4609\n",
      "\n",
      "Epoch 00623: val_loss did not improve from 14132.07047\n",
      "Epoch 624/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 13210.9000 - mean_absolute_error: 13210.9004 - val_loss: 14412.5070 - val_mean_absolute_error: 14412.5068\n",
      "\n",
      "Epoch 00624: val_loss did not improve from 14132.07047\n",
      "Epoch 625/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 12708.9184 - mean_absolute_error: 12708.9170 - val_loss: 14899.7736 - val_mean_absolute_error: 14899.7725\n",
      "\n",
      "Epoch 00625: val_loss did not improve from 14132.07047\n",
      "Epoch 626/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 13427.7785 - mean_absolute_error: 13427.7783 - val_loss: 15384.2148 - val_mean_absolute_error: 15384.2158\n",
      "\n",
      "Epoch 00626: val_loss did not improve from 14132.07047\n",
      "Epoch 627/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 13089.4367 - mean_absolute_error: 13089.4385 - val_loss: 14155.6579 - val_mean_absolute_error: 14155.6582\n",
      "\n",
      "Epoch 00627: val_loss did not improve from 14132.07047\n",
      "Epoch 628/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 13651.5085 - mean_absolute_error: 13651.5088 - val_loss: 14866.6848 - val_mean_absolute_error: 14866.6846\n",
      "\n",
      "Epoch 00628: val_loss did not improve from 14132.07047\n",
      "Epoch 629/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 13623.3893 - mean_absolute_error: 13623.3896 - val_loss: 18431.2468 - val_mean_absolute_error: 18431.2480\n",
      "\n",
      "Epoch 00629: val_loss did not improve from 14132.07047\n",
      "Epoch 630/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 13281.8188 - mean_absolute_error: 13281.8203 - val_loss: 14225.7038 - val_mean_absolute_error: 14225.7031\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 14132.07047\n",
      "Epoch 631/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 13381.7861 - mean_absolute_error: 13381.7852 - val_loss: 15127.4580 - val_mean_absolute_error: 15127.4580\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 14132.07047\n",
      "Epoch 632/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 13654.4614 - mean_absolute_error: 13654.4590 - val_loss: 15304.4426 - val_mean_absolute_error: 15304.4424\n",
      "\n",
      "Epoch 00632: val_loss did not improve from 14132.07047\n",
      "Epoch 633/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 13290.7121 - mean_absolute_error: 13290.7109 - val_loss: 14347.1032 - val_mean_absolute_error: 14347.1035\n",
      "\n",
      "Epoch 00633: val_loss did not improve from 14132.07047\n",
      "Epoch 634/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 12860.0274 - mean_absolute_error: 12860.0273 - val_loss: 14133.7236 - val_mean_absolute_error: 14133.7236\n",
      "\n",
      "Epoch 00634: val_loss did not improve from 14132.07047\n",
      "Epoch 635/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 14026.1349 - mean_absolute_error: 14026.1377 - val_loss: 14880.7520 - val_mean_absolute_error: 14880.7520\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 14132.07047\n",
      "Epoch 636/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 13209.4020 - mean_absolute_error: 13209.4033 - val_loss: 16096.5241 - val_mean_absolute_error: 16096.5234\n",
      "\n",
      "Epoch 00636: val_loss did not improve from 14132.07047\n",
      "Epoch 637/1000\n",
      "880/880 [==============================] - 0s 69us/step - loss: 13775.5410 - mean_absolute_error: 13775.5420 - val_loss: 15712.6497 - val_mean_absolute_error: 15712.6504\n",
      "\n",
      "Epoch 00637: val_loss did not improve from 14132.07047\n",
      "Epoch 638/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 13686.5502 - mean_absolute_error: 13686.5488 - val_loss: 14760.5564 - val_mean_absolute_error: 14760.5557\n",
      "\n",
      "Epoch 00638: val_loss did not improve from 14132.07047\n",
      "Epoch 639/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12879.1838 - mean_absolute_error: 12879.1846 - val_loss: 14541.4409 - val_mean_absolute_error: 14541.4424\n",
      "\n",
      "Epoch 00639: val_loss did not improve from 14132.07047\n",
      "Epoch 640/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 13043.6096 - mean_absolute_error: 13043.6094 - val_loss: 14576.8652 - val_mean_absolute_error: 14576.8662\n",
      "\n",
      "Epoch 00640: val_loss did not improve from 14132.07047\n",
      "Epoch 641/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 12583.0821 - mean_absolute_error: 12583.0840 - val_loss: 17225.6136 - val_mean_absolute_error: 17225.6133\n",
      "\n",
      "Epoch 00641: val_loss did not improve from 14132.07047\n",
      "Epoch 642/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 13124.2139 - mean_absolute_error: 13124.2148 - val_loss: 14145.5517 - val_mean_absolute_error: 14145.5508\n",
      "\n",
      "Epoch 00642: val_loss did not improve from 14132.07047\n",
      "Epoch 643/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 12943.0427 - mean_absolute_error: 12943.0420 - val_loss: 15480.1213 - val_mean_absolute_error: 15480.1221\n",
      "\n",
      "Epoch 00643: val_loss did not improve from 14132.07047\n",
      "Epoch 644/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 13469.0075 - mean_absolute_error: 13469.0078 - val_loss: 14322.4757 - val_mean_absolute_error: 14322.4766\n",
      "\n",
      "Epoch 00644: val_loss did not improve from 14132.07047\n",
      "Epoch 645/1000\n",
      "880/880 [==============================] - 0s 67us/step - loss: 12836.4320 - mean_absolute_error: 12836.4316 - val_loss: 14961.2152 - val_mean_absolute_error: 14961.2158\n",
      "\n",
      "Epoch 00645: val_loss did not improve from 14132.07047\n",
      "Epoch 646/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 12721.1049 - mean_absolute_error: 12721.1045 - val_loss: 14169.6816 - val_mean_absolute_error: 14169.6807\n",
      "\n",
      "Epoch 00646: val_loss did not improve from 14132.07047\n",
      "Epoch 647/1000\n",
      "880/880 [==============================] - 0s 67us/step - loss: 13470.3630 - mean_absolute_error: 13470.3623 - val_loss: 15232.8564 - val_mean_absolute_error: 15232.8564\n",
      "\n",
      "Epoch 00647: val_loss did not improve from 14132.07047\n",
      "Epoch 648/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 13730.6696 - mean_absolute_error: 13730.6660 - val_loss: 14127.1956 - val_mean_absolute_error: 14127.1963\n",
      "\n",
      "Epoch 00648: val_loss improved from 14132.07047 to 14127.19565, saving model to weights.hdf5\n",
      "Epoch 649/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 13263.9635 - mean_absolute_error: 13263.9639 - val_loss: 15300.0400 - val_mean_absolute_error: 15300.0391\n",
      "\n",
      "Epoch 00649: val_loss did not improve from 14127.19565\n",
      "Epoch 650/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 13085.6380 - mean_absolute_error: 13085.6367 - val_loss: 14142.6628 - val_mean_absolute_error: 14142.6621\n",
      "\n",
      "Epoch 00650: val_loss did not improve from 14127.19565\n",
      "Epoch 651/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 13578.0498 - mean_absolute_error: 13578.0498 - val_loss: 15210.4686 - val_mean_absolute_error: 15210.4678\n",
      "\n",
      "Epoch 00651: val_loss did not improve from 14127.19565\n",
      "Epoch 652/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 13286.6841 - mean_absolute_error: 13286.6846 - val_loss: 15697.6705 - val_mean_absolute_error: 15697.6729\n",
      "\n",
      "Epoch 00652: val_loss did not improve from 14127.19565\n",
      "Epoch 653/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 13635.8021 - mean_absolute_error: 13635.8008 - val_loss: 14101.7815 - val_mean_absolute_error: 14101.7822\n",
      "\n",
      "Epoch 00653: val_loss improved from 14127.19565 to 14101.78152, saving model to weights.hdf5\n",
      "Epoch 654/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 13149.7986 - mean_absolute_error: 13149.7998 - val_loss: 14439.2975 - val_mean_absolute_error: 14439.2969\n",
      "\n",
      "Epoch 00654: val_loss did not improve from 14101.78152\n",
      "Epoch 655/1000\n",
      "880/880 [==============================] - 0s 66us/step - loss: 12259.1230 - mean_absolute_error: 12259.1250 - val_loss: 14994.0097 - val_mean_absolute_error: 14994.0088\n",
      "\n",
      "Epoch 00655: val_loss did not improve from 14101.78152\n",
      "Epoch 656/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12578.9156 - mean_absolute_error: 12578.9150 - val_loss: 17569.9557 - val_mean_absolute_error: 17569.9551\n",
      "\n",
      "Epoch 00656: val_loss did not improve from 14101.78152\n",
      "Epoch 657/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 13158.0081 - mean_absolute_error: 13158.0068 - val_loss: 14102.9153 - val_mean_absolute_error: 14102.9160\n",
      "\n",
      "Epoch 00657: val_loss did not improve from 14101.78152\n",
      "Epoch 658/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 13028.1912 - mean_absolute_error: 13028.1924 - val_loss: 15328.5249 - val_mean_absolute_error: 15328.5264\n",
      "\n",
      "Epoch 00658: val_loss did not improve from 14101.78152\n",
      "Epoch 659/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 12918.5693 - mean_absolute_error: 12918.5693 - val_loss: 14869.4179 - val_mean_absolute_error: 14869.4170\n",
      "\n",
      "Epoch 00659: val_loss did not improve from 14101.78152\n",
      "Epoch 660/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 14392.5124 - mean_absolute_error: 14392.5117 - val_loss: 14529.6875 - val_mean_absolute_error: 14529.6865\n",
      "\n",
      "Epoch 00660: val_loss did not improve from 14101.78152\n",
      "Epoch 661/1000\n",
      "880/880 [==============================] - 0s 60us/step - loss: 13644.1591 - mean_absolute_error: 13644.1582 - val_loss: 14802.1923 - val_mean_absolute_error: 14802.1924\n",
      "\n",
      "Epoch 00661: val_loss did not improve from 14101.78152\n",
      "Epoch 662/1000\n",
      "880/880 [==============================] - 0s 60us/step - loss: 16800.2464 - mean_absolute_error: 16800.2441 - val_loss: 14465.7556 - val_mean_absolute_error: 14465.7559\n",
      "\n",
      "Epoch 00662: val_loss did not improve from 14101.78152\n",
      "Epoch 663/1000\n",
      "880/880 [==============================] - 0s 60us/step - loss: 13860.9520 - mean_absolute_error: 13860.9521 - val_loss: 17390.6448 - val_mean_absolute_error: 17390.6445\n",
      "\n",
      "Epoch 00663: val_loss did not improve from 14101.78152\n",
      "Epoch 664/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 12940.0801 - mean_absolute_error: 12940.0791 - val_loss: 14212.3228 - val_mean_absolute_error: 14212.3242\n",
      "\n",
      "Epoch 00664: val_loss did not improve from 14101.78152\n",
      "Epoch 665/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 13330.2279 - mean_absolute_error: 13330.2295 - val_loss: 14910.6165 - val_mean_absolute_error: 14910.6162\n",
      "\n",
      "Epoch 00665: val_loss did not improve from 14101.78152\n",
      "Epoch 666/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 13458.5706 - mean_absolute_error: 13458.5723 - val_loss: 14451.5330 - val_mean_absolute_error: 14451.5332\n",
      "\n",
      "Epoch 00666: val_loss did not improve from 14101.78152\n",
      "Epoch 667/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 12612.2634 - mean_absolute_error: 12612.2637 - val_loss: 14646.9939 - val_mean_absolute_error: 14646.9941\n",
      "\n",
      "Epoch 00667: val_loss did not improve from 14101.78152\n",
      "Epoch 668/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 13100.2337 - mean_absolute_error: 13100.2344 - val_loss: 15889.0710 - val_mean_absolute_error: 15889.0713\n",
      "\n",
      "Epoch 00668: val_loss did not improve from 14101.78152\n",
      "Epoch 669/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 12992.3332 - mean_absolute_error: 12992.3340 - val_loss: 15026.0105 - val_mean_absolute_error: 15026.0117\n",
      "\n",
      "Epoch 00669: val_loss did not improve from 14101.78152\n",
      "Epoch 670/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 12816.5044 - mean_absolute_error: 12816.5049 - val_loss: 15209.6331 - val_mean_absolute_error: 15209.6328\n",
      "\n",
      "Epoch 00670: val_loss did not improve from 14101.78152\n",
      "Epoch 671/1000\n",
      "880/880 [==============================] - 0s 70us/step - loss: 12447.1804 - mean_absolute_error: 12447.1797 - val_loss: 15428.0386 - val_mean_absolute_error: 15428.0391\n",
      "\n",
      "Epoch 00671: val_loss did not improve from 14101.78152\n",
      "Epoch 672/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12829.5215 - mean_absolute_error: 12829.5215 - val_loss: 14021.2977 - val_mean_absolute_error: 14021.2979\n",
      "\n",
      "Epoch 00672: val_loss improved from 14101.78152 to 14021.29767, saving model to weights.hdf5\n",
      "Epoch 673/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 13019.2556 - mean_absolute_error: 13019.2559 - val_loss: 14530.4705 - val_mean_absolute_error: 14530.4717\n",
      "\n",
      "Epoch 00673: val_loss did not improve from 14021.29767\n",
      "Epoch 674/1000\n",
      "880/880 [==============================] - 0s 60us/step - loss: 12646.3353 - mean_absolute_error: 12646.3359 - val_loss: 15110.1786 - val_mean_absolute_error: 15110.1787\n",
      "\n",
      "Epoch 00674: val_loss did not improve from 14021.29767\n",
      "Epoch 675/1000\n",
      "880/880 [==============================] - 0s 60us/step - loss: 13114.3892 - mean_absolute_error: 13114.3906 - val_loss: 15388.1764 - val_mean_absolute_error: 15388.1758\n",
      "\n",
      "Epoch 00675: val_loss did not improve from 14021.29767\n",
      "Epoch 676/1000\n",
      "880/880 [==============================] - 0s 66us/step - loss: 12427.4047 - mean_absolute_error: 12427.4033 - val_loss: 15317.4847 - val_mean_absolute_error: 15317.4844\n",
      "\n",
      "Epoch 00676: val_loss did not improve from 14021.29767\n",
      "Epoch 677/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 13126.1005 - mean_absolute_error: 13126.0986 - val_loss: 17532.0035 - val_mean_absolute_error: 17532.0039\n",
      "\n",
      "Epoch 00677: val_loss did not improve from 14021.29767\n",
      "Epoch 678/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 12937.0831 - mean_absolute_error: 12937.0830 - val_loss: 14214.1623 - val_mean_absolute_error: 14214.1621\n",
      "\n",
      "Epoch 00678: val_loss did not improve from 14021.29767\n",
      "Epoch 679/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 12890.5377 - mean_absolute_error: 12890.5361 - val_loss: 14292.7005 - val_mean_absolute_error: 14292.7002\n",
      "\n",
      "Epoch 00679: val_loss did not improve from 14021.29767\n",
      "Epoch 680/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 13220.6053 - mean_absolute_error: 13220.6045 - val_loss: 15276.4114 - val_mean_absolute_error: 15276.4102\n",
      "\n",
      "Epoch 00680: val_loss did not improve from 14021.29767\n",
      "Epoch 681/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 13296.4928 - mean_absolute_error: 13296.4971 - val_loss: 15413.1757 - val_mean_absolute_error: 15413.1758\n",
      "\n",
      "Epoch 00681: val_loss did not improve from 14021.29767\n",
      "Epoch 682/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 13176.6198 - mean_absolute_error: 13176.6201 - val_loss: 14796.1230 - val_mean_absolute_error: 14796.1230\n",
      "\n",
      "Epoch 00682: val_loss did not improve from 14021.29767\n",
      "Epoch 683/1000\n",
      "880/880 [==============================] - 0s 68us/step - loss: 12862.5542 - mean_absolute_error: 12862.5547 - val_loss: 13961.7153 - val_mean_absolute_error: 13961.7158\n",
      "\n",
      "Epoch 00683: val_loss improved from 14021.29767 to 13961.71532, saving model to weights.hdf5\n",
      "Epoch 684/1000\n",
      "880/880 [==============================] - 0s 67us/step - loss: 13623.4279 - mean_absolute_error: 13623.4277 - val_loss: 16955.8211 - val_mean_absolute_error: 16955.8223\n",
      "\n",
      "Epoch 00684: val_loss did not improve from 13961.71532\n",
      "Epoch 685/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 13622.8979 - mean_absolute_error: 13622.8975 - val_loss: 14441.7123 - val_mean_absolute_error: 14441.7109\n",
      "\n",
      "Epoch 00685: val_loss did not improve from 13961.71532\n",
      "Epoch 686/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 13188.0249 - mean_absolute_error: 13188.0264 - val_loss: 15510.1527 - val_mean_absolute_error: 15510.1523\n",
      "\n",
      "Epoch 00686: val_loss did not improve from 13961.71532\n",
      "Epoch 687/1000\n",
      "880/880 [==============================] - 0s 70us/step - loss: 13598.9575 - mean_absolute_error: 13598.9570 - val_loss: 15795.1080 - val_mean_absolute_error: 15795.1094\n",
      "\n",
      "Epoch 00687: val_loss did not improve from 13961.71532\n",
      "Epoch 688/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 14228.9902 - mean_absolute_error: 14228.9902 - val_loss: 15003.5257 - val_mean_absolute_error: 15003.5264\n",
      "\n",
      "Epoch 00688: val_loss did not improve from 13961.71532\n",
      "Epoch 689/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 13155.0053 - mean_absolute_error: 13155.0068 - val_loss: 14455.0201 - val_mean_absolute_error: 14455.0205\n",
      "\n",
      "Epoch 00689: val_loss did not improve from 13961.71532\n",
      "Epoch 690/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 12911.2425 - mean_absolute_error: 12911.2422 - val_loss: 14118.0013 - val_mean_absolute_error: 14118.0010\n",
      "\n",
      "Epoch 00690: val_loss did not improve from 13961.71532\n",
      "Epoch 691/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 14043.8104 - mean_absolute_error: 14043.8105 - val_loss: 21330.4831 - val_mean_absolute_error: 21330.4844\n",
      "\n",
      "Epoch 00691: val_loss did not improve from 13961.71532\n",
      "Epoch 692/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 14391.3898 - mean_absolute_error: 14391.3906 - val_loss: 14516.7436 - val_mean_absolute_error: 14516.7441\n",
      "\n",
      "Epoch 00692: val_loss did not improve from 13961.71532\n",
      "Epoch 693/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12727.5941 - mean_absolute_error: 12727.5947 - val_loss: 16829.2441 - val_mean_absolute_error: 16829.2441\n",
      "\n",
      "Epoch 00693: val_loss did not improve from 13961.71532\n",
      "Epoch 694/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 12886.2876 - mean_absolute_error: 12886.2861 - val_loss: 15286.7531 - val_mean_absolute_error: 15286.7529\n",
      "\n",
      "Epoch 00694: val_loss did not improve from 13961.71532\n",
      "Epoch 695/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 14278.7073 - mean_absolute_error: 14278.7080 - val_loss: 14474.8348 - val_mean_absolute_error: 14474.8350\n",
      "\n",
      "Epoch 00695: val_loss did not improve from 13961.71532\n",
      "Epoch 696/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 13968.9318 - mean_absolute_error: 13968.9326 - val_loss: 13944.2047 - val_mean_absolute_error: 13944.2041\n",
      "\n",
      "Epoch 00696: val_loss improved from 13961.71532 to 13944.20465, saving model to weights.hdf5\n",
      "Epoch 697/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 12150.1170 - mean_absolute_error: 12150.1162 - val_loss: 14345.1187 - val_mean_absolute_error: 14345.1191\n",
      "\n",
      "Epoch 00697: val_loss did not improve from 13944.20465\n",
      "Epoch 698/1000\n",
      "880/880 [==============================] - 0s 66us/step - loss: 13084.3352 - mean_absolute_error: 13084.3359 - val_loss: 14092.8803 - val_mean_absolute_error: 14092.8809\n",
      "\n",
      "Epoch 00698: val_loss did not improve from 13944.20465\n",
      "Epoch 699/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 12651.5373 - mean_absolute_error: 12651.5391 - val_loss: 14457.6943 - val_mean_absolute_error: 14457.6943\n",
      "\n",
      "Epoch 00699: val_loss did not improve from 13944.20465\n",
      "Epoch 700/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 13327.7357 - mean_absolute_error: 13327.7363 - val_loss: 15109.7274 - val_mean_absolute_error: 15109.7275\n",
      "\n",
      "Epoch 00700: val_loss did not improve from 13944.20465\n",
      "Epoch 701/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 13529.7184 - mean_absolute_error: 13529.7178 - val_loss: 13873.7907 - val_mean_absolute_error: 13873.7910\n",
      "\n",
      "Epoch 00701: val_loss improved from 13944.20465 to 13873.79073, saving model to weights.hdf5\n",
      "Epoch 702/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 13774.5669 - mean_absolute_error: 13774.5674 - val_loss: 14483.7367 - val_mean_absolute_error: 14483.7373\n",
      "\n",
      "Epoch 00702: val_loss did not improve from 13873.79073\n",
      "Epoch 703/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 14166.7661 - mean_absolute_error: 14166.7646 - val_loss: 14344.9088 - val_mean_absolute_error: 14344.9092\n",
      "\n",
      "Epoch 00703: val_loss did not improve from 13873.79073\n",
      "Epoch 704/1000\n",
      "880/880 [==============================] - 0s 68us/step - loss: 13111.9778 - mean_absolute_error: 13111.9766 - val_loss: 16956.3874 - val_mean_absolute_error: 16956.3867\n",
      "\n",
      "Epoch 00704: val_loss did not improve from 13873.79073\n",
      "Epoch 705/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 12479.9310 - mean_absolute_error: 12479.9316 - val_loss: 14022.8937 - val_mean_absolute_error: 14022.8945\n",
      "\n",
      "Epoch 00705: val_loss did not improve from 13873.79073\n",
      "Epoch 706/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 13581.6835 - mean_absolute_error: 13581.6826 - val_loss: 14578.3923 - val_mean_absolute_error: 14578.3936\n",
      "\n",
      "Epoch 00706: val_loss did not improve from 13873.79073\n",
      "Epoch 707/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 14955.7000 - mean_absolute_error: 14955.7002 - val_loss: 14530.9793 - val_mean_absolute_error: 14530.9795\n",
      "\n",
      "Epoch 00707: val_loss did not improve from 13873.79073\n",
      "Epoch 708/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 14167.5874 - mean_absolute_error: 14167.5879 - val_loss: 15216.0458 - val_mean_absolute_error: 15216.0459\n",
      "\n",
      "Epoch 00708: val_loss did not improve from 13873.79073\n",
      "Epoch 709/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 13170.1872 - mean_absolute_error: 13170.1895 - val_loss: 14826.1416 - val_mean_absolute_error: 14826.1406\n",
      "\n",
      "Epoch 00709: val_loss did not improve from 13873.79073\n",
      "Epoch 710/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12912.1977 - mean_absolute_error: 12912.1973 - val_loss: 14257.6041 - val_mean_absolute_error: 14257.6045\n",
      "\n",
      "Epoch 00710: val_loss did not improve from 13873.79073\n",
      "Epoch 711/1000\n",
      "880/880 [==============================] - 0s 67us/step - loss: 12771.2512 - mean_absolute_error: 12771.2510 - val_loss: 15284.5494 - val_mean_absolute_error: 15284.5488\n",
      "\n",
      "Epoch 00711: val_loss did not improve from 13873.79073\n",
      "Epoch 712/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12945.9028 - mean_absolute_error: 12945.9023 - val_loss: 14570.3215 - val_mean_absolute_error: 14570.3213\n",
      "\n",
      "Epoch 00712: val_loss did not improve from 13873.79073\n",
      "Epoch 713/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 12307.1055 - mean_absolute_error: 12307.1064 - val_loss: 14301.7995 - val_mean_absolute_error: 14301.7998\n",
      "\n",
      "Epoch 00713: val_loss did not improve from 13873.79073\n",
      "Epoch 714/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 12386.9509 - mean_absolute_error: 12386.9502 - val_loss: 14170.9262 - val_mean_absolute_error: 14170.9258\n",
      "\n",
      "Epoch 00714: val_loss did not improve from 13873.79073\n",
      "Epoch 715/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12980.9689 - mean_absolute_error: 12980.9697 - val_loss: 15321.4395 - val_mean_absolute_error: 15321.4385\n",
      "\n",
      "Epoch 00715: val_loss did not improve from 13873.79073\n",
      "Epoch 716/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 14191.6144 - mean_absolute_error: 14191.6162 - val_loss: 14251.2893 - val_mean_absolute_error: 14251.2891\n",
      "\n",
      "Epoch 00716: val_loss did not improve from 13873.79073\n",
      "Epoch 717/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12288.5382 - mean_absolute_error: 12288.5400 - val_loss: 13866.0725 - val_mean_absolute_error: 13866.0723\n",
      "\n",
      "Epoch 00717: val_loss improved from 13873.79073 to 13866.07253, saving model to weights.hdf5\n",
      "Epoch 718/1000\n",
      "880/880 [==============================] - 0s 66us/step - loss: 12564.2395 - mean_absolute_error: 12564.2383 - val_loss: 15074.9153 - val_mean_absolute_error: 15074.9160\n",
      "\n",
      "Epoch 00718: val_loss did not improve from 13866.07253\n",
      "Epoch 719/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 12874.7108 - mean_absolute_error: 12874.7109 - val_loss: 13748.9241 - val_mean_absolute_error: 13748.9248\n",
      "\n",
      "Epoch 00719: val_loss improved from 13866.07253 to 13748.92413, saving model to weights.hdf5\n",
      "Epoch 720/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 12786.3349 - mean_absolute_error: 12786.3340 - val_loss: 14192.7009 - val_mean_absolute_error: 14192.7002\n",
      "\n",
      "Epoch 00720: val_loss did not improve from 13748.92413\n",
      "Epoch 721/1000\n",
      "880/880 [==============================] - 0s 71us/step - loss: 12462.4872 - mean_absolute_error: 12462.4863 - val_loss: 14088.9412 - val_mean_absolute_error: 14088.9404\n",
      "\n",
      "Epoch 00721: val_loss did not improve from 13748.92413\n",
      "Epoch 722/1000\n",
      "880/880 [==============================] - 0s 66us/step - loss: 13173.4020 - mean_absolute_error: 13173.4014 - val_loss: 14271.1296 - val_mean_absolute_error: 14271.1299\n",
      "\n",
      "Epoch 00722: val_loss did not improve from 13748.92413\n",
      "Epoch 723/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12501.8269 - mean_absolute_error: 12501.8262 - val_loss: 13876.6177 - val_mean_absolute_error: 13876.6182\n",
      "\n",
      "Epoch 00723: val_loss did not improve from 13748.92413\n",
      "Epoch 724/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 13593.5090 - mean_absolute_error: 13593.5078 - val_loss: 16667.3901 - val_mean_absolute_error: 16667.3906\n",
      "\n",
      "Epoch 00724: val_loss did not improve from 13748.92413\n",
      "Epoch 725/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12682.5930 - mean_absolute_error: 12682.5928 - val_loss: 17477.1322 - val_mean_absolute_error: 17477.1328\n",
      "\n",
      "Epoch 00725: val_loss did not improve from 13748.92413\n",
      "Epoch 726/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 13277.0207 - mean_absolute_error: 13277.0205 - val_loss: 13954.6172 - val_mean_absolute_error: 13954.6172\n",
      "\n",
      "Epoch 00726: val_loss did not improve from 13748.92413\n",
      "Epoch 727/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12789.5664 - mean_absolute_error: 12789.5674 - val_loss: 13871.9669 - val_mean_absolute_error: 13871.9668\n",
      "\n",
      "Epoch 00727: val_loss did not improve from 13748.92413\n",
      "Epoch 728/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12536.8216 - mean_absolute_error: 12536.8213 - val_loss: 14325.2888 - val_mean_absolute_error: 14325.2891\n",
      "\n",
      "Epoch 00728: val_loss did not improve from 13748.92413\n",
      "Epoch 729/1000\n",
      "880/880 [==============================] - 0s 67us/step - loss: 12278.8449 - mean_absolute_error: 12278.8467 - val_loss: 14053.4749 - val_mean_absolute_error: 14053.4766\n",
      "\n",
      "Epoch 00729: val_loss did not improve from 13748.92413\n",
      "Epoch 730/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 13835.6246 - mean_absolute_error: 13835.6240 - val_loss: 18571.9203 - val_mean_absolute_error: 18571.9199\n",
      "\n",
      "Epoch 00730: val_loss did not improve from 13748.92413\n",
      "Epoch 731/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 14365.6895 - mean_absolute_error: 14365.6895 - val_loss: 14615.4805 - val_mean_absolute_error: 14615.4805\n",
      "\n",
      "Epoch 00731: val_loss did not improve from 13748.92413\n",
      "Epoch 732/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12125.8523 - mean_absolute_error: 12125.8525 - val_loss: 14724.9527 - val_mean_absolute_error: 14724.9521\n",
      "\n",
      "Epoch 00732: val_loss did not improve from 13748.92413\n",
      "Epoch 733/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12177.5612 - mean_absolute_error: 12177.5605 - val_loss: 14503.0259 - val_mean_absolute_error: 14503.0264\n",
      "\n",
      "Epoch 00733: val_loss did not improve from 13748.92413\n",
      "Epoch 734/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 12723.6942 - mean_absolute_error: 12723.6943 - val_loss: 15426.2208 - val_mean_absolute_error: 15426.2197\n",
      "\n",
      "Epoch 00734: val_loss did not improve from 13748.92413\n",
      "Epoch 735/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 12235.8121 - mean_absolute_error: 12235.8135 - val_loss: 14681.4811 - val_mean_absolute_error: 14681.4805\n",
      "\n",
      "Epoch 00735: val_loss did not improve from 13748.92413\n",
      "Epoch 736/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 13129.0984 - mean_absolute_error: 13129.0986 - val_loss: 13762.1345 - val_mean_absolute_error: 13762.1348\n",
      "\n",
      "Epoch 00736: val_loss did not improve from 13748.92413\n",
      "Epoch 737/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12092.1890 - mean_absolute_error: 12092.1875 - val_loss: 14099.8812 - val_mean_absolute_error: 14099.8818\n",
      "\n",
      "Epoch 00737: val_loss did not improve from 13748.92413\n",
      "Epoch 738/1000\n",
      "880/880 [==============================] - 0s 101us/step - loss: 12065.2301 - mean_absolute_error: 12065.2295 - val_loss: 14516.0679 - val_mean_absolute_error: 14516.0684\n",
      "\n",
      "Epoch 00738: val_loss did not improve from 13748.92413\n",
      "Epoch 739/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 12476.5575 - mean_absolute_error: 12476.5566 - val_loss: 15528.4811 - val_mean_absolute_error: 15528.4814\n",
      "\n",
      "Epoch 00739: val_loss did not improve from 13748.92413\n",
      "Epoch 740/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 12162.4160 - mean_absolute_error: 12162.4170 - val_loss: 14827.0566 - val_mean_absolute_error: 14827.0566\n",
      "\n",
      "Epoch 00740: val_loss did not improve from 13748.92413\n",
      "Epoch 741/1000\n",
      "880/880 [==============================] - 0s 77us/step - loss: 12743.5345 - mean_absolute_error: 12743.5352 - val_loss: 14038.6786 - val_mean_absolute_error: 14038.6787\n",
      "\n",
      "Epoch 00741: val_loss did not improve from 13748.92413\n",
      "Epoch 742/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 11832.5589 - mean_absolute_error: 11832.5576 - val_loss: 13893.2963 - val_mean_absolute_error: 13893.2969\n",
      "\n",
      "Epoch 00742: val_loss did not improve from 13748.92413\n",
      "Epoch 743/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 13147.0088 - mean_absolute_error: 13147.0088 - val_loss: 14431.7297 - val_mean_absolute_error: 14431.7295\n",
      "\n",
      "Epoch 00743: val_loss did not improve from 13748.92413\n",
      "Epoch 744/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 14396.2845 - mean_absolute_error: 14396.2842 - val_loss: 17703.3594 - val_mean_absolute_error: 17703.3594\n",
      "\n",
      "Epoch 00744: val_loss did not improve from 13748.92413\n",
      "Epoch 745/1000\n",
      "880/880 [==============================] - 0s 66us/step - loss: 12931.6298 - mean_absolute_error: 12931.6299 - val_loss: 13875.1531 - val_mean_absolute_error: 13875.1543\n",
      "\n",
      "Epoch 00745: val_loss did not improve from 13748.92413\n",
      "Epoch 746/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 13136.1469 - mean_absolute_error: 13136.1465 - val_loss: 15363.3389 - val_mean_absolute_error: 15363.3408\n",
      "\n",
      "Epoch 00746: val_loss did not improve from 13748.92413\n",
      "Epoch 747/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 13023.7962 - mean_absolute_error: 13023.7969 - val_loss: 14834.3161 - val_mean_absolute_error: 14834.3174\n",
      "\n",
      "Epoch 00747: val_loss did not improve from 13748.92413\n",
      "Epoch 748/1000\n",
      "880/880 [==============================] - 0s 66us/step - loss: 11831.4562 - mean_absolute_error: 11831.4561 - val_loss: 14144.1479 - val_mean_absolute_error: 14144.1475\n",
      "\n",
      "Epoch 00748: val_loss did not improve from 13748.92413\n",
      "Epoch 749/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 12024.5873 - mean_absolute_error: 12024.5879 - val_loss: 14150.0218 - val_mean_absolute_error: 14150.0215\n",
      "\n",
      "Epoch 00749: val_loss did not improve from 13748.92413\n",
      "Epoch 750/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 12475.0414 - mean_absolute_error: 12475.0420 - val_loss: 17995.1053 - val_mean_absolute_error: 17995.1055\n",
      "\n",
      "Epoch 00750: val_loss did not improve from 13748.92413\n",
      "Epoch 751/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 13061.8269 - mean_absolute_error: 13061.8271 - val_loss: 13982.3166 - val_mean_absolute_error: 13982.3174\n",
      "\n",
      "Epoch 00751: val_loss did not improve from 13748.92413\n",
      "Epoch 752/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12920.7400 - mean_absolute_error: 12920.7412 - val_loss: 18925.4591 - val_mean_absolute_error: 18925.4590\n",
      "\n",
      "Epoch 00752: val_loss did not improve from 13748.92413\n",
      "Epoch 753/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 13555.9682 - mean_absolute_error: 13555.9668 - val_loss: 17058.2350 - val_mean_absolute_error: 17058.2344\n",
      "\n",
      "Epoch 00753: val_loss did not improve from 13748.92413\n",
      "Epoch 754/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12778.1267 - mean_absolute_error: 12778.1260 - val_loss: 13976.3852 - val_mean_absolute_error: 13976.3867\n",
      "\n",
      "Epoch 00754: val_loss did not improve from 13748.92413\n",
      "Epoch 755/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 12499.7861 - mean_absolute_error: 12499.7871 - val_loss: 13848.4042 - val_mean_absolute_error: 13848.4043\n",
      "\n",
      "Epoch 00755: val_loss did not improve from 13748.92413\n",
      "Epoch 756/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 12919.0042 - mean_absolute_error: 12919.0029 - val_loss: 14359.9365 - val_mean_absolute_error: 14359.9365\n",
      "\n",
      "Epoch 00756: val_loss did not improve from 13748.92413\n",
      "Epoch 757/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 11740.3154 - mean_absolute_error: 11740.3154 - val_loss: 14230.8188 - val_mean_absolute_error: 14230.8184\n",
      "\n",
      "Epoch 00757: val_loss did not improve from 13748.92413\n",
      "Epoch 758/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 12402.5377 - mean_absolute_error: 12402.5361 - val_loss: 15781.2444 - val_mean_absolute_error: 15781.2432\n",
      "\n",
      "Epoch 00758: val_loss did not improve from 13748.92413\n",
      "Epoch 759/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 12805.3054 - mean_absolute_error: 12805.3057 - val_loss: 14427.5770 - val_mean_absolute_error: 14427.5771\n",
      "\n",
      "Epoch 00759: val_loss did not improve from 13748.92413\n",
      "Epoch 760/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 12558.2397 - mean_absolute_error: 12558.2402 - val_loss: 14758.9709 - val_mean_absolute_error: 14758.9707\n",
      "\n",
      "Epoch 00760: val_loss did not improve from 13748.92413\n",
      "Epoch 761/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12281.0072 - mean_absolute_error: 12281.0078 - val_loss: 14123.9094 - val_mean_absolute_error: 14123.9092\n",
      "\n",
      "Epoch 00761: val_loss did not improve from 13748.92413\n",
      "Epoch 762/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 11537.2690 - mean_absolute_error: 11537.2695 - val_loss: 14110.6612 - val_mean_absolute_error: 14110.6611\n",
      "\n",
      "Epoch 00762: val_loss did not improve from 13748.92413\n",
      "Epoch 763/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 12955.8726 - mean_absolute_error: 12955.8721 - val_loss: 14373.1686 - val_mean_absolute_error: 14373.1689\n",
      "\n",
      "Epoch 00763: val_loss did not improve from 13748.92413\n",
      "Epoch 764/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12683.2447 - mean_absolute_error: 12683.2432 - val_loss: 13992.9125 - val_mean_absolute_error: 13992.9141\n",
      "\n",
      "Epoch 00764: val_loss did not improve from 13748.92413\n",
      "Epoch 765/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 12510.2373 - mean_absolute_error: 12510.2363 - val_loss: 14422.1958 - val_mean_absolute_error: 14422.1953\n",
      "\n",
      "Epoch 00765: val_loss did not improve from 13748.92413\n",
      "Epoch 766/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 11980.3347 - mean_absolute_error: 11980.3340 - val_loss: 15102.5461 - val_mean_absolute_error: 15102.5459\n",
      "\n",
      "Epoch 00766: val_loss did not improve from 13748.92413\n",
      "Epoch 767/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12735.7261 - mean_absolute_error: 12735.7266 - val_loss: 15661.0703 - val_mean_absolute_error: 15661.0703\n",
      "\n",
      "Epoch 00767: val_loss did not improve from 13748.92413\n",
      "Epoch 768/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 13334.2893 - mean_absolute_error: 13334.2891 - val_loss: 14952.7334 - val_mean_absolute_error: 14952.7344\n",
      "\n",
      "Epoch 00768: val_loss did not improve from 13748.92413\n",
      "Epoch 769/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12267.2423 - mean_absolute_error: 12267.2432 - val_loss: 14092.2176 - val_mean_absolute_error: 14092.2178\n",
      "\n",
      "Epoch 00769: val_loss did not improve from 13748.92413\n",
      "Epoch 770/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12903.8239 - mean_absolute_error: 12903.8242 - val_loss: 13947.6774 - val_mean_absolute_error: 13947.6758\n",
      "\n",
      "Epoch 00770: val_loss did not improve from 13748.92413\n",
      "Epoch 771/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 13652.9282 - mean_absolute_error: 13652.9277 - val_loss: 13962.7603 - val_mean_absolute_error: 13962.7588\n",
      "\n",
      "Epoch 00771: val_loss did not improve from 13748.92413\n",
      "Epoch 772/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 13796.2336 - mean_absolute_error: 13796.2334 - val_loss: 14095.4480 - val_mean_absolute_error: 14095.4473\n",
      "\n",
      "Epoch 00772: val_loss did not improve from 13748.92413\n",
      "Epoch 773/1000\n",
      "880/880 [==============================] - 0s 83us/step - loss: 13064.3543 - mean_absolute_error: 13064.3535 - val_loss: 16146.8543 - val_mean_absolute_error: 16146.8535\n",
      "\n",
      "Epoch 00773: val_loss did not improve from 13748.92413\n",
      "Epoch 774/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 11836.7795 - mean_absolute_error: 11836.7803 - val_loss: 14106.5336 - val_mean_absolute_error: 14106.5332\n",
      "\n",
      "Epoch 00774: val_loss did not improve from 13748.92413\n",
      "Epoch 775/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 11817.1635 - mean_absolute_error: 11817.1621 - val_loss: 13919.3884 - val_mean_absolute_error: 13919.3877\n",
      "\n",
      "Epoch 00775: val_loss did not improve from 13748.92413\n",
      "Epoch 776/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 12676.1081 - mean_absolute_error: 12676.1084 - val_loss: 14289.5185 - val_mean_absolute_error: 14289.5186\n",
      "\n",
      "Epoch 00776: val_loss did not improve from 13748.92413\n",
      "Epoch 777/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 14149.7624 - mean_absolute_error: 14149.7617 - val_loss: 14410.9343 - val_mean_absolute_error: 14410.9346\n",
      "\n",
      "Epoch 00777: val_loss did not improve from 13748.92413\n",
      "Epoch 778/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 14919.8340 - mean_absolute_error: 14919.8340 - val_loss: 14163.1934 - val_mean_absolute_error: 14163.1934\n",
      "\n",
      "Epoch 00778: val_loss did not improve from 13748.92413\n",
      "Epoch 779/1000\n",
      "880/880 [==============================] - 0s 60us/step - loss: 12845.6868 - mean_absolute_error: 12845.6885 - val_loss: 14438.9627 - val_mean_absolute_error: 14438.9629\n",
      "\n",
      "Epoch 00779: val_loss did not improve from 13748.92413\n",
      "Epoch 780/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 11933.0419 - mean_absolute_error: 11933.0420 - val_loss: 13863.3728 - val_mean_absolute_error: 13863.3730\n",
      "\n",
      "Epoch 00780: val_loss did not improve from 13748.92413\n",
      "Epoch 781/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 11707.2749 - mean_absolute_error: 11707.2734 - val_loss: 14023.2697 - val_mean_absolute_error: 14023.2705\n",
      "\n",
      "Epoch 00781: val_loss did not improve from 13748.92413\n",
      "Epoch 782/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12511.8024 - mean_absolute_error: 12511.8008 - val_loss: 13801.3991 - val_mean_absolute_error: 13801.3984\n",
      "\n",
      "Epoch 00782: val_loss did not improve from 13748.92413\n",
      "Epoch 783/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 12031.7651 - mean_absolute_error: 12031.7646 - val_loss: 13882.1938 - val_mean_absolute_error: 13882.1943\n",
      "\n",
      "Epoch 00783: val_loss did not improve from 13748.92413\n",
      "Epoch 784/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12344.5305 - mean_absolute_error: 12344.5303 - val_loss: 13898.5299 - val_mean_absolute_error: 13898.5293\n",
      "\n",
      "Epoch 00784: val_loss did not improve from 13748.92413\n",
      "Epoch 785/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12371.5485 - mean_absolute_error: 12371.5479 - val_loss: 14691.0086 - val_mean_absolute_error: 14691.0088\n",
      "\n",
      "Epoch 00785: val_loss did not improve from 13748.92413\n",
      "Epoch 786/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 13016.5329 - mean_absolute_error: 13016.5332 - val_loss: 13820.0657 - val_mean_absolute_error: 13820.0654\n",
      "\n",
      "Epoch 00786: val_loss did not improve from 13748.92413\n",
      "Epoch 787/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 11823.7008 - mean_absolute_error: 11823.7012 - val_loss: 14012.2872 - val_mean_absolute_error: 14012.2871\n",
      "\n",
      "Epoch 00787: val_loss did not improve from 13748.92413\n",
      "Epoch 788/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 12095.8954 - mean_absolute_error: 12095.8945 - val_loss: 14422.8151 - val_mean_absolute_error: 14422.8154\n",
      "\n",
      "Epoch 00788: val_loss did not improve from 13748.92413\n",
      "Epoch 789/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12722.9390 - mean_absolute_error: 12722.9385 - val_loss: 13793.8212 - val_mean_absolute_error: 13793.8193\n",
      "\n",
      "Epoch 00789: val_loss did not improve from 13748.92413\n",
      "Epoch 790/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 12029.9428 - mean_absolute_error: 12029.9424 - val_loss: 14840.0655 - val_mean_absolute_error: 14840.0645\n",
      "\n",
      "Epoch 00790: val_loss did not improve from 13748.92413\n",
      "Epoch 791/1000\n",
      "880/880 [==============================] - 0s 68us/step - loss: 12652.4836 - mean_absolute_error: 12652.4844 - val_loss: 14166.9984 - val_mean_absolute_error: 14166.9980\n",
      "\n",
      "Epoch 00791: val_loss did not improve from 13748.92413\n",
      "Epoch 792/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 11974.0700 - mean_absolute_error: 11974.0703 - val_loss: 16792.0935 - val_mean_absolute_error: 16792.0938\n",
      "\n",
      "Epoch 00792: val_loss did not improve from 13748.92413\n",
      "Epoch 793/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 12915.0137 - mean_absolute_error: 12915.0137 - val_loss: 13920.7752 - val_mean_absolute_error: 13920.7754\n",
      "\n",
      "Epoch 00793: val_loss did not improve from 13748.92413\n",
      "Epoch 794/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 13070.1780 - mean_absolute_error: 13070.1777 - val_loss: 16607.2270 - val_mean_absolute_error: 16607.2266\n",
      "\n",
      "Epoch 00794: val_loss did not improve from 13748.92413\n",
      "Epoch 795/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 12981.7253 - mean_absolute_error: 12981.7227 - val_loss: 14462.0225 - val_mean_absolute_error: 14462.0225\n",
      "\n",
      "Epoch 00795: val_loss did not improve from 13748.92413\n",
      "Epoch 796/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12268.6208 - mean_absolute_error: 12268.6201 - val_loss: 13936.5595 - val_mean_absolute_error: 13936.5605\n",
      "\n",
      "Epoch 00796: val_loss did not improve from 13748.92413\n",
      "Epoch 797/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12736.8255 - mean_absolute_error: 12736.8252 - val_loss: 13876.5146 - val_mean_absolute_error: 13876.5146\n",
      "\n",
      "Epoch 00797: val_loss did not improve from 13748.92413\n",
      "Epoch 798/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12401.1082 - mean_absolute_error: 12401.1094 - val_loss: 13805.0836 - val_mean_absolute_error: 13805.0840\n",
      "\n",
      "Epoch 00798: val_loss did not improve from 13748.92413\n",
      "Epoch 799/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 12210.7275 - mean_absolute_error: 12210.7275 - val_loss: 13877.8518 - val_mean_absolute_error: 13877.8525\n",
      "\n",
      "Epoch 00799: val_loss did not improve from 13748.92413\n",
      "Epoch 800/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 12059.6518 - mean_absolute_error: 12059.6523 - val_loss: 14072.9087 - val_mean_absolute_error: 14072.9092\n",
      "\n",
      "Epoch 00800: val_loss did not improve from 13748.92413\n",
      "Epoch 801/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 11740.8940 - mean_absolute_error: 11740.8916 - val_loss: 13903.2411 - val_mean_absolute_error: 13903.2422\n",
      "\n",
      "Epoch 00801: val_loss did not improve from 13748.92413\n",
      "Epoch 802/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 12060.7041 - mean_absolute_error: 12060.7031 - val_loss: 14170.1846 - val_mean_absolute_error: 14170.1846\n",
      "\n",
      "Epoch 00802: val_loss did not improve from 13748.92413\n",
      "Epoch 803/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12990.4394 - mean_absolute_error: 12990.4385 - val_loss: 13989.1744 - val_mean_absolute_error: 13989.1748\n",
      "\n",
      "Epoch 00803: val_loss did not improve from 13748.92413\n",
      "Epoch 804/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 11685.2477 - mean_absolute_error: 11685.2471 - val_loss: 13799.7717 - val_mean_absolute_error: 13799.7725\n",
      "\n",
      "Epoch 00804: val_loss did not improve from 13748.92413\n",
      "Epoch 805/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 11766.0014 - mean_absolute_error: 11766.0010 - val_loss: 14155.9009 - val_mean_absolute_error: 14155.9014\n",
      "\n",
      "Epoch 00805: val_loss did not improve from 13748.92413\n",
      "Epoch 806/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12881.1199 - mean_absolute_error: 12881.1191 - val_loss: 14419.2830 - val_mean_absolute_error: 14419.2832\n",
      "\n",
      "Epoch 00806: val_loss did not improve from 13748.92413\n",
      "Epoch 807/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12920.7303 - mean_absolute_error: 12920.7285 - val_loss: 17039.3912 - val_mean_absolute_error: 17039.3906\n",
      "\n",
      "Epoch 00807: val_loss did not improve from 13748.92413\n",
      "Epoch 808/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 13358.7352 - mean_absolute_error: 13358.7354 - val_loss: 14131.8332 - val_mean_absolute_error: 14131.8330\n",
      "\n",
      "Epoch 00808: val_loss did not improve from 13748.92413\n",
      "Epoch 809/1000\n",
      "880/880 [==============================] - 0s 71us/step - loss: 13494.1893 - mean_absolute_error: 13494.1885 - val_loss: 15530.5600 - val_mean_absolute_error: 15530.5596\n",
      "\n",
      "Epoch 00809: val_loss did not improve from 13748.92413\n",
      "Epoch 810/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 12090.7439 - mean_absolute_error: 12090.7432 - val_loss: 13829.5071 - val_mean_absolute_error: 13829.5068\n",
      "\n",
      "Epoch 00810: val_loss did not improve from 13748.92413\n",
      "Epoch 811/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 11801.0669 - mean_absolute_error: 11801.0674 - val_loss: 13900.3335 - val_mean_absolute_error: 13900.3340\n",
      "\n",
      "Epoch 00811: val_loss did not improve from 13748.92413\n",
      "Epoch 812/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 12579.1482 - mean_absolute_error: 12579.1475 - val_loss: 14603.5430 - val_mean_absolute_error: 14603.5430\n",
      "\n",
      "Epoch 00812: val_loss did not improve from 13748.92413\n",
      "Epoch 813/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 14279.9219 - mean_absolute_error: 14279.9229 - val_loss: 14745.0368 - val_mean_absolute_error: 14745.0352\n",
      "\n",
      "Epoch 00813: val_loss did not improve from 13748.92413\n",
      "Epoch 814/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 12639.5163 - mean_absolute_error: 12639.5166 - val_loss: 13812.9206 - val_mean_absolute_error: 13812.9209\n",
      "\n",
      "Epoch 00814: val_loss did not improve from 13748.92413\n",
      "Epoch 815/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 11791.0508 - mean_absolute_error: 11791.0508 - val_loss: 13728.8095 - val_mean_absolute_error: 13728.8096\n",
      "\n",
      "Epoch 00815: val_loss improved from 13748.92413 to 13728.80950, saving model to weights.hdf5\n",
      "Epoch 816/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 11808.6365 - mean_absolute_error: 11808.6367 - val_loss: 14315.7203 - val_mean_absolute_error: 14315.7207\n",
      "\n",
      "Epoch 00816: val_loss did not improve from 13728.80950\n",
      "Epoch 817/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12356.1275 - mean_absolute_error: 12356.1260 - val_loss: 13815.6182 - val_mean_absolute_error: 13815.6182\n",
      "\n",
      "Epoch 00817: val_loss did not improve from 13728.80950\n",
      "Epoch 818/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 11694.0842 - mean_absolute_error: 11694.0830 - val_loss: 18857.5397 - val_mean_absolute_error: 18857.5391\n",
      "\n",
      "Epoch 00818: val_loss did not improve from 13728.80950\n",
      "Epoch 819/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12478.2820 - mean_absolute_error: 12478.2832 - val_loss: 14007.6524 - val_mean_absolute_error: 14007.6523\n",
      "\n",
      "Epoch 00819: val_loss did not improve from 13728.80950\n",
      "Epoch 820/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12123.3729 - mean_absolute_error: 12123.3730 - val_loss: 14303.1692 - val_mean_absolute_error: 14303.1680\n",
      "\n",
      "Epoch 00820: val_loss did not improve from 13728.80950\n",
      "Epoch 821/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 11977.6762 - mean_absolute_error: 11977.6748 - val_loss: 14351.3580 - val_mean_absolute_error: 14351.3564\n",
      "\n",
      "Epoch 00821: val_loss did not improve from 13728.80950\n",
      "Epoch 822/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 11693.5101 - mean_absolute_error: 11693.5098 - val_loss: 14182.2768 - val_mean_absolute_error: 14182.2773\n",
      "\n",
      "Epoch 00822: val_loss did not improve from 13728.80950\n",
      "Epoch 823/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 12269.3673 - mean_absolute_error: 12269.3672 - val_loss: 14767.6545 - val_mean_absolute_error: 14767.6543\n",
      "\n",
      "Epoch 00823: val_loss did not improve from 13728.80950\n",
      "Epoch 824/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 11910.7283 - mean_absolute_error: 11910.7305 - val_loss: 14278.6210 - val_mean_absolute_error: 14278.6201\n",
      "\n",
      "Epoch 00824: val_loss did not improve from 13728.80950\n",
      "Epoch 825/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12705.8597 - mean_absolute_error: 12705.8604 - val_loss: 16970.3023 - val_mean_absolute_error: 16970.3027\n",
      "\n",
      "Epoch 00825: val_loss did not improve from 13728.80950\n",
      "Epoch 826/1000\n",
      "880/880 [==============================] - 0s 67us/step - loss: 12573.2846 - mean_absolute_error: 12573.2832 - val_loss: 14210.6537 - val_mean_absolute_error: 14210.6533\n",
      "\n",
      "Epoch 00826: val_loss did not improve from 13728.80950\n",
      "Epoch 827/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 12076.7126 - mean_absolute_error: 12076.7148 - val_loss: 13652.9725 - val_mean_absolute_error: 13652.9727\n",
      "\n",
      "Epoch 00827: val_loss improved from 13728.80950 to 13652.97246, saving model to weights.hdf5\n",
      "Epoch 828/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 11500.4609 - mean_absolute_error: 11500.4609 - val_loss: 14416.5064 - val_mean_absolute_error: 14416.5068\n",
      "\n",
      "Epoch 00828: val_loss did not improve from 13652.97246\n",
      "Epoch 829/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 11782.3161 - mean_absolute_error: 11782.3154 - val_loss: 13857.2467 - val_mean_absolute_error: 13857.2471\n",
      "\n",
      "Epoch 00829: val_loss did not improve from 13652.97246\n",
      "Epoch 830/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 11302.8600 - mean_absolute_error: 11302.8604 - val_loss: 14237.8400 - val_mean_absolute_error: 14237.8398\n",
      "\n",
      "Epoch 00830: val_loss did not improve from 13652.97246\n",
      "Epoch 831/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 11983.0733 - mean_absolute_error: 11983.0713 - val_loss: 14229.5835 - val_mean_absolute_error: 14229.5840\n",
      "\n",
      "Epoch 00831: val_loss did not improve from 13652.97246\n",
      "Epoch 832/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 12452.5679 - mean_absolute_error: 12452.5684 - val_loss: 14550.8728 - val_mean_absolute_error: 14550.8730\n",
      "\n",
      "Epoch 00832: val_loss did not improve from 13652.97246\n",
      "Epoch 833/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12053.0092 - mean_absolute_error: 12053.0088 - val_loss: 17803.7565 - val_mean_absolute_error: 17803.7559\n",
      "\n",
      "Epoch 00833: val_loss did not improve from 13652.97246\n",
      "Epoch 834/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 13489.9913 - mean_absolute_error: 13489.9912 - val_loss: 14357.4515 - val_mean_absolute_error: 14357.4512\n",
      "\n",
      "Epoch 00834: val_loss did not improve from 13652.97246\n",
      "Epoch 835/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 12030.0733 - mean_absolute_error: 12030.0752 - val_loss: 14037.8567 - val_mean_absolute_error: 14037.8564\n",
      "\n",
      "Epoch 00835: val_loss did not improve from 13652.97246\n",
      "Epoch 836/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12446.3487 - mean_absolute_error: 12446.3477 - val_loss: 15158.1888 - val_mean_absolute_error: 15158.1885\n",
      "\n",
      "Epoch 00836: val_loss did not improve from 13652.97246\n",
      "Epoch 837/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 12129.5046 - mean_absolute_error: 12129.5020 - val_loss: 14597.6543 - val_mean_absolute_error: 14597.6533\n",
      "\n",
      "Epoch 00837: val_loss did not improve from 13652.97246\n",
      "Epoch 838/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 11573.8180 - mean_absolute_error: 11573.8184 - val_loss: 14112.2956 - val_mean_absolute_error: 14112.2959\n",
      "\n",
      "Epoch 00838: val_loss did not improve from 13652.97246\n",
      "Epoch 839/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 12364.4659 - mean_absolute_error: 12364.4648 - val_loss: 16252.7571 - val_mean_absolute_error: 16252.7568\n",
      "\n",
      "Epoch 00839: val_loss did not improve from 13652.97246\n",
      "Epoch 840/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 12349.8817 - mean_absolute_error: 12349.8809 - val_loss: 13689.0274 - val_mean_absolute_error: 13689.0273\n",
      "\n",
      "Epoch 00840: val_loss did not improve from 13652.97246\n",
      "Epoch 841/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 12282.5664 - mean_absolute_error: 12282.5654 - val_loss: 14763.2912 - val_mean_absolute_error: 14763.2910\n",
      "\n",
      "Epoch 00841: val_loss did not improve from 13652.97246\n",
      "Epoch 842/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 13224.2322 - mean_absolute_error: 13224.2314 - val_loss: 14281.4873 - val_mean_absolute_error: 14281.4873\n",
      "\n",
      "Epoch 00842: val_loss did not improve from 13652.97246\n",
      "Epoch 843/1000\n",
      "880/880 [==============================] - 0s 70us/step - loss: 11606.6872 - mean_absolute_error: 11606.6865 - val_loss: 16206.4928 - val_mean_absolute_error: 16206.4932\n",
      "\n",
      "Epoch 00843: val_loss did not improve from 13652.97246\n",
      "Epoch 844/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 13535.6030 - mean_absolute_error: 13535.6035 - val_loss: 14001.0723 - val_mean_absolute_error: 14001.0723\n",
      "\n",
      "Epoch 00844: val_loss did not improve from 13652.97246\n",
      "Epoch 845/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 12285.2330 - mean_absolute_error: 12285.2334 - val_loss: 13999.8232 - val_mean_absolute_error: 13999.8223\n",
      "\n",
      "Epoch 00845: val_loss did not improve from 13652.97246\n",
      "Epoch 846/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 11970.2716 - mean_absolute_error: 11970.2705 - val_loss: 13635.1856 - val_mean_absolute_error: 13635.1855\n",
      "\n",
      "Epoch 00846: val_loss improved from 13652.97246 to 13635.18556, saving model to weights.hdf5\n",
      "Epoch 847/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 12321.9809 - mean_absolute_error: 12321.9795 - val_loss: 13932.5377 - val_mean_absolute_error: 13932.5371\n",
      "\n",
      "Epoch 00847: val_loss did not improve from 13635.18556\n",
      "Epoch 848/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 11282.3119 - mean_absolute_error: 11282.3125 - val_loss: 14021.6350 - val_mean_absolute_error: 14021.6348\n",
      "\n",
      "Epoch 00848: val_loss did not improve from 13635.18556\n",
      "Epoch 849/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 11718.9957 - mean_absolute_error: 11718.9941 - val_loss: 16210.4838 - val_mean_absolute_error: 16210.4844\n",
      "\n",
      "Epoch 00849: val_loss did not improve from 13635.18556\n",
      "Epoch 850/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 12631.2635 - mean_absolute_error: 12631.2627 - val_loss: 14755.4619 - val_mean_absolute_error: 14755.4629\n",
      "\n",
      "Epoch 00850: val_loss did not improve from 13635.18556\n",
      "Epoch 851/1000\n",
      "880/880 [==============================] - 0s 67us/step - loss: 11482.9751 - mean_absolute_error: 11482.9746 - val_loss: 14058.7297 - val_mean_absolute_error: 14058.7305\n",
      "\n",
      "Epoch 00851: val_loss did not improve from 13635.18556\n",
      "Epoch 852/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12624.9346 - mean_absolute_error: 12624.9346 - val_loss: 13721.9305 - val_mean_absolute_error: 13721.9307\n",
      "\n",
      "Epoch 00852: val_loss did not improve from 13635.18556\n",
      "Epoch 853/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 11409.7985 - mean_absolute_error: 11409.7979 - val_loss: 13685.5706 - val_mean_absolute_error: 13685.5703\n",
      "\n",
      "Epoch 00853: val_loss did not improve from 13635.18556\n",
      "Epoch 854/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 11566.1119 - mean_absolute_error: 11566.1113 - val_loss: 13605.3067 - val_mean_absolute_error: 13605.3066\n",
      "\n",
      "Epoch 00854: val_loss improved from 13635.18556 to 13605.30673, saving model to weights.hdf5\n",
      "Epoch 855/1000\n",
      "880/880 [==============================] - 0s 67us/step - loss: 11161.2663 - mean_absolute_error: 11161.2646 - val_loss: 15323.8173 - val_mean_absolute_error: 15323.8154\n",
      "\n",
      "Epoch 00855: val_loss did not improve from 13605.30673\n",
      "Epoch 856/1000\n",
      "880/880 [==============================] - 0s 67us/step - loss: 12754.8815 - mean_absolute_error: 12754.8809 - val_loss: 13999.0841 - val_mean_absolute_error: 13999.0850\n",
      "\n",
      "Epoch 00856: val_loss did not improve from 13605.30673\n",
      "Epoch 857/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 11936.3772 - mean_absolute_error: 11936.3770 - val_loss: 13557.7113 - val_mean_absolute_error: 13557.7109\n",
      "\n",
      "Epoch 00857: val_loss improved from 13605.30673 to 13557.71133, saving model to weights.hdf5\n",
      "Epoch 858/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 11699.7223 - mean_absolute_error: 11699.7227 - val_loss: 14018.8604 - val_mean_absolute_error: 14018.8604\n",
      "\n",
      "Epoch 00858: val_loss did not improve from 13557.71133\n",
      "Epoch 859/1000\n",
      "880/880 [==============================] - 0s 71us/step - loss: 12056.5011 - mean_absolute_error: 12056.5010 - val_loss: 15121.4426 - val_mean_absolute_error: 15121.4434\n",
      "\n",
      "Epoch 00859: val_loss did not improve from 13557.71133\n",
      "Epoch 860/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 11848.5590 - mean_absolute_error: 11848.5596 - val_loss: 13613.1346 - val_mean_absolute_error: 13613.1338\n",
      "\n",
      "Epoch 00860: val_loss did not improve from 13557.71133\n",
      "Epoch 861/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 11286.9466 - mean_absolute_error: 11286.9463 - val_loss: 13796.7207 - val_mean_absolute_error: 13796.7207\n",
      "\n",
      "Epoch 00861: val_loss did not improve from 13557.71133\n",
      "Epoch 862/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 12005.9756 - mean_absolute_error: 12005.9766 - val_loss: 16614.6074 - val_mean_absolute_error: 16614.6074\n",
      "\n",
      "Epoch 00862: val_loss did not improve from 13557.71133\n",
      "Epoch 863/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12220.1490 - mean_absolute_error: 12220.1484 - val_loss: 14720.2848 - val_mean_absolute_error: 14720.2852\n",
      "\n",
      "Epoch 00863: val_loss did not improve from 13557.71133\n",
      "Epoch 864/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 11890.7863 - mean_absolute_error: 11890.7852 - val_loss: 14115.9725 - val_mean_absolute_error: 14115.9727\n",
      "\n",
      "Epoch 00864: val_loss did not improve from 13557.71133\n",
      "Epoch 865/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12073.4210 - mean_absolute_error: 12073.4209 - val_loss: 15352.3343 - val_mean_absolute_error: 15352.3340\n",
      "\n",
      "Epoch 00865: val_loss did not improve from 13557.71133\n",
      "Epoch 866/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12745.3158 - mean_absolute_error: 12745.3154 - val_loss: 13667.0396 - val_mean_absolute_error: 13667.0400\n",
      "\n",
      "Epoch 00866: val_loss did not improve from 13557.71133\n",
      "Epoch 867/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 11770.9663 - mean_absolute_error: 11770.9648 - val_loss: 14021.7669 - val_mean_absolute_error: 14021.7666\n",
      "\n",
      "Epoch 00867: val_loss did not improve from 13557.71133\n",
      "Epoch 868/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 12163.0906 - mean_absolute_error: 12163.0908 - val_loss: 15814.5602 - val_mean_absolute_error: 15814.5596\n",
      "\n",
      "Epoch 00868: val_loss did not improve from 13557.71133\n",
      "Epoch 869/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 13017.0126 - mean_absolute_error: 13017.0127 - val_loss: 15934.8216 - val_mean_absolute_error: 15934.8213\n",
      "\n",
      "Epoch 00869: val_loss did not improve from 13557.71133\n",
      "Epoch 870/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 11963.8987 - mean_absolute_error: 11963.8984 - val_loss: 15741.7872 - val_mean_absolute_error: 15741.7861\n",
      "\n",
      "Epoch 00870: val_loss did not improve from 13557.71133\n",
      "Epoch 871/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 11830.2671 - mean_absolute_error: 11830.2656 - val_loss: 13749.4548 - val_mean_absolute_error: 13749.4531\n",
      "\n",
      "Epoch 00871: val_loss did not improve from 13557.71133\n",
      "Epoch 872/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 11620.5077 - mean_absolute_error: 11620.5068 - val_loss: 14504.5706 - val_mean_absolute_error: 14504.5703\n",
      "\n",
      "Epoch 00872: val_loss did not improve from 13557.71133\n",
      "Epoch 873/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12814.1309 - mean_absolute_error: 12814.1299 - val_loss: 14343.3764 - val_mean_absolute_error: 14343.3770\n",
      "\n",
      "Epoch 00873: val_loss did not improve from 13557.71133\n",
      "Epoch 874/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12382.7389 - mean_absolute_error: 12382.7402 - val_loss: 14258.1459 - val_mean_absolute_error: 14258.1455\n",
      "\n",
      "Epoch 00874: val_loss did not improve from 13557.71133\n",
      "Epoch 875/1000\n",
      "880/880 [==============================] - 0s 66us/step - loss: 11612.9546 - mean_absolute_error: 11612.9531 - val_loss: 15591.5608 - val_mean_absolute_error: 15591.5605\n",
      "\n",
      "Epoch 00875: val_loss did not improve from 13557.71133\n",
      "Epoch 876/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 12887.2132 - mean_absolute_error: 12887.2148 - val_loss: 13712.3060 - val_mean_absolute_error: 13712.3057\n",
      "\n",
      "Epoch 00876: val_loss did not improve from 13557.71133\n",
      "Epoch 877/1000\n",
      "880/880 [==============================] - 0s 68us/step - loss: 11882.7817 - mean_absolute_error: 11882.7803 - val_loss: 14033.8577 - val_mean_absolute_error: 14033.8564\n",
      "\n",
      "Epoch 00877: val_loss did not improve from 13557.71133\n",
      "Epoch 878/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 11766.9357 - mean_absolute_error: 11766.9355 - val_loss: 13765.2267 - val_mean_absolute_error: 13765.2275\n",
      "\n",
      "Epoch 00878: val_loss did not improve from 13557.71133\n",
      "Epoch 879/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 12036.9265 - mean_absolute_error: 12036.9277 - val_loss: 14878.5804 - val_mean_absolute_error: 14878.5791\n",
      "\n",
      "Epoch 00879: val_loss did not improve from 13557.71133\n",
      "Epoch 880/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12649.6659 - mean_absolute_error: 12649.6660 - val_loss: 14054.3593 - val_mean_absolute_error: 14054.3594\n",
      "\n",
      "Epoch 00880: val_loss did not improve from 13557.71133\n",
      "Epoch 881/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 11353.3759 - mean_absolute_error: 11353.3760 - val_loss: 13779.4468 - val_mean_absolute_error: 13779.4463\n",
      "\n",
      "Epoch 00881: val_loss did not improve from 13557.71133\n",
      "Epoch 882/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12126.4538 - mean_absolute_error: 12126.4541 - val_loss: 14008.7552 - val_mean_absolute_error: 14008.7559\n",
      "\n",
      "Epoch 00882: val_loss did not improve from 13557.71133\n",
      "Epoch 883/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 11970.2218 - mean_absolute_error: 11970.2227 - val_loss: 13542.2092 - val_mean_absolute_error: 13542.2090\n",
      "\n",
      "Epoch 00883: val_loss improved from 13557.71133 to 13542.20922, saving model to weights.hdf5\n",
      "Epoch 884/1000\n",
      "880/880 [==============================] - 0s 66us/step - loss: 11800.7346 - mean_absolute_error: 11800.7344 - val_loss: 13642.5949 - val_mean_absolute_error: 13642.5947\n",
      "\n",
      "Epoch 00884: val_loss did not improve from 13542.20922\n",
      "Epoch 885/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12190.4948 - mean_absolute_error: 12190.4951 - val_loss: 15876.1080 - val_mean_absolute_error: 15876.1094\n",
      "\n",
      "Epoch 00885: val_loss did not improve from 13542.20922\n",
      "Epoch 886/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 12315.3965 - mean_absolute_error: 12315.3965 - val_loss: 16412.1791 - val_mean_absolute_error: 16412.1797\n",
      "\n",
      "Epoch 00886: val_loss did not improve from 13542.20922\n",
      "Epoch 887/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 12259.7344 - mean_absolute_error: 12259.7344 - val_loss: 13856.2854 - val_mean_absolute_error: 13856.2852\n",
      "\n",
      "Epoch 00887: val_loss did not improve from 13542.20922\n",
      "Epoch 888/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 11644.3796 - mean_absolute_error: 11644.3809 - val_loss: 13708.0332 - val_mean_absolute_error: 13708.0332\n",
      "\n",
      "Epoch 00888: val_loss did not improve from 13542.20922\n",
      "Epoch 889/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 11417.4265 - mean_absolute_error: 11417.4248 - val_loss: 14086.7061 - val_mean_absolute_error: 14086.7070\n",
      "\n",
      "Epoch 00889: val_loss did not improve from 13542.20922\n",
      "Epoch 890/1000\n",
      "880/880 [==============================] - 0s 60us/step - loss: 11442.4803 - mean_absolute_error: 11442.4805 - val_loss: 13853.6750 - val_mean_absolute_error: 13853.6748\n",
      "\n",
      "Epoch 00890: val_loss did not improve from 13542.20922\n",
      "Epoch 891/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 12461.3940 - mean_absolute_error: 12461.3955 - val_loss: 14075.1481 - val_mean_absolute_error: 14075.1475\n",
      "\n",
      "Epoch 00891: val_loss did not improve from 13542.20922\n",
      "Epoch 892/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 12220.2138 - mean_absolute_error: 12220.2158 - val_loss: 16930.5989 - val_mean_absolute_error: 16930.5996\n",
      "\n",
      "Epoch 00892: val_loss did not improve from 13542.20922\n",
      "Epoch 893/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12915.3263 - mean_absolute_error: 12915.3252 - val_loss: 15836.9328 - val_mean_absolute_error: 15836.9326\n",
      "\n",
      "Epoch 00893: val_loss did not improve from 13542.20922\n",
      "Epoch 894/1000\n",
      "880/880 [==============================] - 0s 68us/step - loss: 12961.0496 - mean_absolute_error: 12961.0488 - val_loss: 13897.7770 - val_mean_absolute_error: 13897.7773\n",
      "\n",
      "Epoch 00894: val_loss did not improve from 13542.20922\n",
      "Epoch 895/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 11655.9953 - mean_absolute_error: 11655.9951 - val_loss: 14056.9074 - val_mean_absolute_error: 14056.9072\n",
      "\n",
      "Epoch 00895: val_loss did not improve from 13542.20922\n",
      "Epoch 896/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 11792.6281 - mean_absolute_error: 11792.6279 - val_loss: 15841.5544 - val_mean_absolute_error: 15841.5547\n",
      "\n",
      "Epoch 00896: val_loss did not improve from 13542.20922\n",
      "Epoch 897/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 11808.6448 - mean_absolute_error: 11808.6445 - val_loss: 16692.6953 - val_mean_absolute_error: 16692.6953\n",
      "\n",
      "Epoch 00897: val_loss did not improve from 13542.20922\n",
      "Epoch 898/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12786.0858 - mean_absolute_error: 12786.0879 - val_loss: 14389.6206 - val_mean_absolute_error: 14389.6201\n",
      "\n",
      "Epoch 00898: val_loss did not improve from 13542.20922\n",
      "Epoch 899/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 11329.2944 - mean_absolute_error: 11329.2939 - val_loss: 13927.1919 - val_mean_absolute_error: 13927.1924\n",
      "\n",
      "Epoch 00899: val_loss did not improve from 13542.20922\n",
      "Epoch 900/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 11460.2918 - mean_absolute_error: 11460.2920 - val_loss: 16047.1605 - val_mean_absolute_error: 16047.1592\n",
      "\n",
      "Epoch 00900: val_loss did not improve from 13542.20922\n",
      "Epoch 901/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12602.3659 - mean_absolute_error: 12602.3662 - val_loss: 14070.3634 - val_mean_absolute_error: 14070.3623\n",
      "\n",
      "Epoch 00901: val_loss did not improve from 13542.20922\n",
      "Epoch 902/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 12303.1294 - mean_absolute_error: 12303.1299 - val_loss: 13852.7187 - val_mean_absolute_error: 13852.7178\n",
      "\n",
      "Epoch 00902: val_loss did not improve from 13542.20922\n",
      "Epoch 903/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 12009.1991 - mean_absolute_error: 12009.2002 - val_loss: 14043.2318 - val_mean_absolute_error: 14043.2314\n",
      "\n",
      "Epoch 00903: val_loss did not improve from 13542.20922\n",
      "Epoch 904/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 11181.7465 - mean_absolute_error: 11181.7480 - val_loss: 13662.2481 - val_mean_absolute_error: 13662.2480\n",
      "\n",
      "Epoch 00904: val_loss did not improve from 13542.20922\n",
      "Epoch 905/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 11279.9857 - mean_absolute_error: 11279.9854 - val_loss: 13482.7822 - val_mean_absolute_error: 13482.7822\n",
      "\n",
      "Epoch 00905: val_loss improved from 13542.20922 to 13482.78224, saving model to weights.hdf5\n",
      "Epoch 906/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 11865.1527 - mean_absolute_error: 11865.1514 - val_loss: 14760.3212 - val_mean_absolute_error: 14760.3203\n",
      "\n",
      "Epoch 00906: val_loss did not improve from 13482.78224\n",
      "Epoch 907/1000\n",
      "880/880 [==============================] - 0s 71us/step - loss: 11923.0378 - mean_absolute_error: 11923.0391 - val_loss: 13712.1934 - val_mean_absolute_error: 13712.1934\n",
      "\n",
      "Epoch 00907: val_loss did not improve from 13482.78224\n",
      "Epoch 908/1000\n",
      "880/880 [==============================] - 0s 67us/step - loss: 12129.7230 - mean_absolute_error: 12129.7227 - val_loss: 20602.4348 - val_mean_absolute_error: 20602.4336\n",
      "\n",
      "Epoch 00908: val_loss did not improve from 13482.78224\n",
      "Epoch 909/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12015.4806 - mean_absolute_error: 12015.4805 - val_loss: 13663.4771 - val_mean_absolute_error: 13663.4775\n",
      "\n",
      "Epoch 00909: val_loss did not improve from 13482.78224\n",
      "Epoch 910/1000\n",
      "880/880 [==============================] - 0s 66us/step - loss: 11624.8460 - mean_absolute_error: 11624.8467 - val_loss: 14047.8601 - val_mean_absolute_error: 14047.8594\n",
      "\n",
      "Epoch 00910: val_loss did not improve from 13482.78224\n",
      "Epoch 911/1000\n",
      "880/880 [==============================] - 0s 70us/step - loss: 11947.9206 - mean_absolute_error: 11947.9219 - val_loss: 13619.8574 - val_mean_absolute_error: 13619.8564\n",
      "\n",
      "Epoch 00911: val_loss did not improve from 13482.78224\n",
      "Epoch 912/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 11586.4311 - mean_absolute_error: 11586.4297 - val_loss: 14421.5006 - val_mean_absolute_error: 14421.5000\n",
      "\n",
      "Epoch 00912: val_loss did not improve from 13482.78224\n",
      "Epoch 913/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 13096.9315 - mean_absolute_error: 13096.9307 - val_loss: 16174.8374 - val_mean_absolute_error: 16174.8379\n",
      "\n",
      "Epoch 00913: val_loss did not improve from 13482.78224\n",
      "Epoch 914/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 13150.9952 - mean_absolute_error: 13150.9922 - val_loss: 13678.2371 - val_mean_absolute_error: 13678.2373\n",
      "\n",
      "Epoch 00914: val_loss did not improve from 13482.78224\n",
      "Epoch 915/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 11622.1077 - mean_absolute_error: 11622.1094 - val_loss: 16032.6149 - val_mean_absolute_error: 16032.6133\n",
      "\n",
      "Epoch 00915: val_loss did not improve from 13482.78224\n",
      "Epoch 916/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 13758.9762 - mean_absolute_error: 13758.9775 - val_loss: 14426.0353 - val_mean_absolute_error: 14426.0361\n",
      "\n",
      "Epoch 00916: val_loss did not improve from 13482.78224\n",
      "Epoch 917/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12883.5433 - mean_absolute_error: 12883.5439 - val_loss: 13973.6455 - val_mean_absolute_error: 13973.6455\n",
      "\n",
      "Epoch 00917: val_loss did not improve from 13482.78224\n",
      "Epoch 918/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 11640.6115 - mean_absolute_error: 11640.6104 - val_loss: 14844.8301 - val_mean_absolute_error: 14844.8291\n",
      "\n",
      "Epoch 00918: val_loss did not improve from 13482.78224\n",
      "Epoch 919/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 11552.4287 - mean_absolute_error: 11552.4307 - val_loss: 14153.0712 - val_mean_absolute_error: 14153.0703\n",
      "\n",
      "Epoch 00919: val_loss did not improve from 13482.78224\n",
      "Epoch 920/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 11197.8657 - mean_absolute_error: 11197.8672 - val_loss: 15045.5422 - val_mean_absolute_error: 15045.5420\n",
      "\n",
      "Epoch 00920: val_loss did not improve from 13482.78224\n",
      "Epoch 921/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 11639.2399 - mean_absolute_error: 11639.2412 - val_loss: 14132.6911 - val_mean_absolute_error: 14132.6904\n",
      "\n",
      "Epoch 00921: val_loss did not improve from 13482.78224\n",
      "Epoch 922/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 11414.4123 - mean_absolute_error: 11414.4150 - val_loss: 15146.5781 - val_mean_absolute_error: 15146.5771\n",
      "\n",
      "Epoch 00922: val_loss did not improve from 13482.78224\n",
      "Epoch 923/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 11382.0012 - mean_absolute_error: 11382.0010 - val_loss: 14361.6867 - val_mean_absolute_error: 14361.6865\n",
      "\n",
      "Epoch 00923: val_loss did not improve from 13482.78224\n",
      "Epoch 924/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 11327.7457 - mean_absolute_error: 11327.7480 - val_loss: 13814.1574 - val_mean_absolute_error: 13814.1572\n",
      "\n",
      "Epoch 00924: val_loss did not improve from 13482.78224\n",
      "Epoch 925/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 11775.5906 - mean_absolute_error: 11775.5908 - val_loss: 13725.2027 - val_mean_absolute_error: 13725.2021\n",
      "\n",
      "Epoch 00925: val_loss did not improve from 13482.78224\n",
      "Epoch 926/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 11905.2005 - mean_absolute_error: 11905.2002 - val_loss: 14241.1776 - val_mean_absolute_error: 14241.1777\n",
      "\n",
      "Epoch 00926: val_loss did not improve from 13482.78224\n",
      "Epoch 927/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12011.1242 - mean_absolute_error: 12011.1250 - val_loss: 15059.6575 - val_mean_absolute_error: 15059.6582\n",
      "\n",
      "Epoch 00927: val_loss did not improve from 13482.78224\n",
      "Epoch 928/1000\n",
      "880/880 [==============================] - 0s 67us/step - loss: 12418.7518 - mean_absolute_error: 12418.7500 - val_loss: 13831.3793 - val_mean_absolute_error: 13831.3799\n",
      "\n",
      "Epoch 00928: val_loss did not improve from 13482.78224\n",
      "Epoch 929/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 11525.8132 - mean_absolute_error: 11525.8125 - val_loss: 14484.0966 - val_mean_absolute_error: 14484.0957\n",
      "\n",
      "Epoch 00929: val_loss did not improve from 13482.78224\n",
      "Epoch 930/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 11698.7915 - mean_absolute_error: 11698.7920 - val_loss: 14843.9784 - val_mean_absolute_error: 14843.9775\n",
      "\n",
      "Epoch 00930: val_loss did not improve from 13482.78224\n",
      "Epoch 931/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 11765.0950 - mean_absolute_error: 11765.0957 - val_loss: 13708.6582 - val_mean_absolute_error: 13708.6582\n",
      "\n",
      "Epoch 00931: val_loss did not improve from 13482.78224\n",
      "Epoch 932/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 11533.3763 - mean_absolute_error: 11533.3770 - val_loss: 13723.4678 - val_mean_absolute_error: 13723.4668\n",
      "\n",
      "Epoch 00932: val_loss did not improve from 13482.78224\n",
      "Epoch 933/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12021.8993 - mean_absolute_error: 12021.8975 - val_loss: 13873.1252 - val_mean_absolute_error: 13873.1250\n",
      "\n",
      "Epoch 00933: val_loss did not improve from 13482.78224\n",
      "Epoch 934/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 12606.7435 - mean_absolute_error: 12606.7432 - val_loss: 13841.5079 - val_mean_absolute_error: 13841.5068\n",
      "\n",
      "Epoch 00934: val_loss did not improve from 13482.78224\n",
      "Epoch 935/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 12157.2066 - mean_absolute_error: 12157.2070 - val_loss: 14113.7373 - val_mean_absolute_error: 14113.7363\n",
      "\n",
      "Epoch 00935: val_loss did not improve from 13482.78224\n",
      "Epoch 936/1000\n",
      "880/880 [==============================] - 0s 88us/step - loss: 11204.7711 - mean_absolute_error: 11204.7705 - val_loss: 14877.2330 - val_mean_absolute_error: 14877.2334\n",
      "\n",
      "Epoch 00936: val_loss did not improve from 13482.78224\n",
      "Epoch 937/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 11858.5075 - mean_absolute_error: 11858.5068 - val_loss: 15999.5953 - val_mean_absolute_error: 15999.5957\n",
      "\n",
      "Epoch 00937: val_loss did not improve from 13482.78224\n",
      "Epoch 938/1000\n",
      "880/880 [==============================] - 0s 67us/step - loss: 11678.5255 - mean_absolute_error: 11678.5254 - val_loss: 13658.0745 - val_mean_absolute_error: 13658.0742\n",
      "\n",
      "Epoch 00938: val_loss did not improve from 13482.78224\n",
      "Epoch 939/1000\n",
      "880/880 [==============================] - 0s 69us/step - loss: 10878.0994 - mean_absolute_error: 10878.0996 - val_loss: 14290.3743 - val_mean_absolute_error: 14290.3750\n",
      "\n",
      "Epoch 00939: val_loss did not improve from 13482.78224\n",
      "Epoch 940/1000\n",
      "880/880 [==============================] - 0s 68us/step - loss: 11308.3466 - mean_absolute_error: 11308.3467 - val_loss: 13798.6045 - val_mean_absolute_error: 13798.6045\n",
      "\n",
      "Epoch 00940: val_loss did not improve from 13482.78224\n",
      "Epoch 941/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 11539.0350 - mean_absolute_error: 11539.0342 - val_loss: 14430.7353 - val_mean_absolute_error: 14430.7363\n",
      "\n",
      "Epoch 00941: val_loss did not improve from 13482.78224\n",
      "Epoch 942/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 11748.3835 - mean_absolute_error: 11748.3838 - val_loss: 13832.3119 - val_mean_absolute_error: 13832.3115\n",
      "\n",
      "Epoch 00942: val_loss did not improve from 13482.78224\n",
      "Epoch 943/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 11597.4618 - mean_absolute_error: 11597.4609 - val_loss: 14079.0434 - val_mean_absolute_error: 14079.0430\n",
      "\n",
      "Epoch 00943: val_loss did not improve from 13482.78224\n",
      "Epoch 944/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 12216.2469 - mean_absolute_error: 12216.2480 - val_loss: 15937.7669 - val_mean_absolute_error: 15937.7666\n",
      "\n",
      "Epoch 00944: val_loss did not improve from 13482.78224\n",
      "Epoch 945/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 12122.6354 - mean_absolute_error: 12122.6338 - val_loss: 14372.1648 - val_mean_absolute_error: 14372.1641\n",
      "\n",
      "Epoch 00945: val_loss did not improve from 13482.78224\n",
      "Epoch 946/1000\n",
      "880/880 [==============================] - 0s 70us/step - loss: 13734.5845 - mean_absolute_error: 13734.5840 - val_loss: 15738.0267 - val_mean_absolute_error: 15738.0264\n",
      "\n",
      "Epoch 00946: val_loss did not improve from 13482.78224\n",
      "Epoch 947/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 14667.3175 - mean_absolute_error: 14667.3174 - val_loss: 14178.8893 - val_mean_absolute_error: 14178.8887\n",
      "\n",
      "Epoch 00947: val_loss did not improve from 13482.78224\n",
      "Epoch 948/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 11862.1692 - mean_absolute_error: 11862.1689 - val_loss: 13726.5064 - val_mean_absolute_error: 13726.5068\n",
      "\n",
      "Epoch 00948: val_loss did not improve from 13482.78224\n",
      "Epoch 949/1000\n",
      "880/880 [==============================] - 0s 67us/step - loss: 11314.8006 - mean_absolute_error: 11314.8027 - val_loss: 13652.6639 - val_mean_absolute_error: 13652.6641\n",
      "\n",
      "Epoch 00949: val_loss did not improve from 13482.78224\n",
      "Epoch 950/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 11300.9446 - mean_absolute_error: 11300.9463 - val_loss: 14159.2856 - val_mean_absolute_error: 14159.2852\n",
      "\n",
      "Epoch 00950: val_loss did not improve from 13482.78224\n",
      "Epoch 951/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 11350.6067 - mean_absolute_error: 11350.6084 - val_loss: 13589.3018 - val_mean_absolute_error: 13589.3027\n",
      "\n",
      "Epoch 00951: val_loss did not improve from 13482.78224\n",
      "Epoch 952/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 11324.8324 - mean_absolute_error: 11324.8340 - val_loss: 13638.7084 - val_mean_absolute_error: 13638.7080\n",
      "\n",
      "Epoch 00952: val_loss did not improve from 13482.78224\n",
      "Epoch 953/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 12610.2702 - mean_absolute_error: 12610.2695 - val_loss: 14253.3888 - val_mean_absolute_error: 14253.3887\n",
      "\n",
      "Epoch 00953: val_loss did not improve from 13482.78224\n",
      "Epoch 954/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 11120.9496 - mean_absolute_error: 11120.9502 - val_loss: 16233.8436 - val_mean_absolute_error: 16233.8428\n",
      "\n",
      "Epoch 00954: val_loss did not improve from 13482.78224\n",
      "Epoch 955/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 11187.8343 - mean_absolute_error: 11187.8340 - val_loss: 14824.8520 - val_mean_absolute_error: 14824.8525\n",
      "\n",
      "Epoch 00955: val_loss did not improve from 13482.78224\n",
      "Epoch 956/1000\n",
      "880/880 [==============================] - 0s 59us/step - loss: 11978.0223 - mean_absolute_error: 11978.0225 - val_loss: 14532.2397 - val_mean_absolute_error: 14532.2402\n",
      "\n",
      "Epoch 00956: val_loss did not improve from 13482.78224\n",
      "Epoch 957/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 10914.8977 - mean_absolute_error: 10914.8984 - val_loss: 13991.9237 - val_mean_absolute_error: 13991.9238\n",
      "\n",
      "Epoch 00957: val_loss did not improve from 13482.78224\n",
      "Epoch 958/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 11570.4972 - mean_absolute_error: 11570.4990 - val_loss: 16666.7750 - val_mean_absolute_error: 16666.7754\n",
      "\n",
      "Epoch 00958: val_loss did not improve from 13482.78224\n",
      "Epoch 959/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 13352.8385 - mean_absolute_error: 13352.8398 - val_loss: 13631.2257 - val_mean_absolute_error: 13631.2266\n",
      "\n",
      "Epoch 00959: val_loss did not improve from 13482.78224\n",
      "Epoch 960/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 10885.3823 - mean_absolute_error: 10885.3828 - val_loss: 13826.5766 - val_mean_absolute_error: 13826.5771\n",
      "\n",
      "Epoch 00960: val_loss did not improve from 13482.78224\n",
      "Epoch 961/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 10688.6807 - mean_absolute_error: 10688.6807 - val_loss: 14368.6235 - val_mean_absolute_error: 14368.6230\n",
      "\n",
      "Epoch 00961: val_loss did not improve from 13482.78224\n",
      "Epoch 962/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 11318.7348 - mean_absolute_error: 11318.7334 - val_loss: 14027.6363 - val_mean_absolute_error: 14027.6348\n",
      "\n",
      "Epoch 00962: val_loss did not improve from 13482.78224\n",
      "Epoch 963/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 10900.4872 - mean_absolute_error: 10900.4902 - val_loss: 14315.4490 - val_mean_absolute_error: 14315.4502\n",
      "\n",
      "Epoch 00963: val_loss did not improve from 13482.78224\n",
      "Epoch 964/1000\n",
      "880/880 [==============================] - 0s 60us/step - loss: 11847.5843 - mean_absolute_error: 11847.5850 - val_loss: 13593.7198 - val_mean_absolute_error: 13593.7207\n",
      "\n",
      "Epoch 00964: val_loss did not improve from 13482.78224\n",
      "Epoch 965/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 11771.3623 - mean_absolute_error: 11771.3623 - val_loss: 14240.2175 - val_mean_absolute_error: 14240.2168\n",
      "\n",
      "Epoch 00965: val_loss did not improve from 13482.78224\n",
      "Epoch 966/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 11428.5060 - mean_absolute_error: 11428.5059 - val_loss: 14198.9555 - val_mean_absolute_error: 14198.9541\n",
      "\n",
      "Epoch 00966: val_loss did not improve from 13482.78224\n",
      "Epoch 967/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 11382.7180 - mean_absolute_error: 11382.7178 - val_loss: 14404.7825 - val_mean_absolute_error: 14404.7832\n",
      "\n",
      "Epoch 00967: val_loss did not improve from 13482.78224\n",
      "Epoch 968/1000\n",
      "880/880 [==============================] - 0s 59us/step - loss: 11235.9102 - mean_absolute_error: 11235.9102 - val_loss: 15868.5919 - val_mean_absolute_error: 15868.5918\n",
      "\n",
      "Epoch 00968: val_loss did not improve from 13482.78224\n",
      "Epoch 969/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 11277.4433 - mean_absolute_error: 11277.4453 - val_loss: 14041.7280 - val_mean_absolute_error: 14041.7285\n",
      "\n",
      "Epoch 00969: val_loss did not improve from 13482.78224\n",
      "Epoch 970/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 11241.9290 - mean_absolute_error: 11241.9287 - val_loss: 14211.8689 - val_mean_absolute_error: 14211.8691\n",
      "\n",
      "Epoch 00970: val_loss did not improve from 13482.78224\n",
      "Epoch 971/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 12288.2722 - mean_absolute_error: 12288.2705 - val_loss: 14348.6740 - val_mean_absolute_error: 14348.6738\n",
      "\n",
      "Epoch 00971: val_loss did not improve from 13482.78224\n",
      "Epoch 972/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 11432.3089 - mean_absolute_error: 11432.3096 - val_loss: 16049.9475 - val_mean_absolute_error: 16049.9463\n",
      "\n",
      "Epoch 00972: val_loss did not improve from 13482.78224\n",
      "Epoch 973/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 11305.7545 - mean_absolute_error: 11305.7568 - val_loss: 14891.4678 - val_mean_absolute_error: 14891.4668\n",
      "\n",
      "Epoch 00973: val_loss did not improve from 13482.78224\n",
      "Epoch 974/1000\n",
      "880/880 [==============================] - 0s 59us/step - loss: 11271.8428 - mean_absolute_error: 11271.8428 - val_loss: 13869.6601 - val_mean_absolute_error: 13869.6602\n",
      "\n",
      "Epoch 00974: val_loss did not improve from 13482.78224\n",
      "Epoch 975/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 10785.4986 - mean_absolute_error: 10785.4971 - val_loss: 14321.6881 - val_mean_absolute_error: 14321.6885\n",
      "\n",
      "Epoch 00975: val_loss did not improve from 13482.78224\n",
      "Epoch 976/1000\n",
      "880/880 [==============================] - 0s 67us/step - loss: 12585.5337 - mean_absolute_error: 12585.5322 - val_loss: 14515.7796 - val_mean_absolute_error: 14515.7793\n",
      "\n",
      "Epoch 00976: val_loss did not improve from 13482.78224\n",
      "Epoch 977/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 11032.9368 - mean_absolute_error: 11032.9375 - val_loss: 14464.6830 - val_mean_absolute_error: 14464.6826\n",
      "\n",
      "Epoch 00977: val_loss did not improve from 13482.78224\n",
      "Epoch 978/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 12456.0018 - mean_absolute_error: 12456.0010 - val_loss: 17394.8405 - val_mean_absolute_error: 17394.8398\n",
      "\n",
      "Epoch 00978: val_loss did not improve from 13482.78224\n",
      "Epoch 979/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 11536.1516 - mean_absolute_error: 11536.1523 - val_loss: 13868.8944 - val_mean_absolute_error: 13868.8955\n",
      "\n",
      "Epoch 00979: val_loss did not improve from 13482.78224\n",
      "Epoch 980/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 12233.2142 - mean_absolute_error: 12233.2129 - val_loss: 14928.7953 - val_mean_absolute_error: 14928.7939\n",
      "\n",
      "Epoch 00980: val_loss did not improve from 13482.78224\n",
      "Epoch 981/1000\n",
      "880/880 [==============================] - 0s 67us/step - loss: 12313.9362 - mean_absolute_error: 12313.9355 - val_loss: 16330.3534 - val_mean_absolute_error: 16330.3535\n",
      "\n",
      "Epoch 00981: val_loss did not improve from 13482.78224\n",
      "Epoch 982/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 14436.9568 - mean_absolute_error: 14436.9570 - val_loss: 17986.0124 - val_mean_absolute_error: 17986.0117\n",
      "\n",
      "Epoch 00982: val_loss did not improve from 13482.78224\n",
      "Epoch 983/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 13574.4519 - mean_absolute_error: 13574.4531 - val_loss: 15313.3121 - val_mean_absolute_error: 15313.3115\n",
      "\n",
      "Epoch 00983: val_loss did not improve from 13482.78224\n",
      "Epoch 984/1000\n",
      "880/880 [==============================] - 0s 60us/step - loss: 11455.1600 - mean_absolute_error: 11455.1611 - val_loss: 14177.3083 - val_mean_absolute_error: 14177.3096\n",
      "\n",
      "Epoch 00984: val_loss did not improve from 13482.78224\n",
      "Epoch 985/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 11448.0773 - mean_absolute_error: 11448.0771 - val_loss: 14133.8825 - val_mean_absolute_error: 14133.8818\n",
      "\n",
      "Epoch 00985: val_loss did not improve from 13482.78224\n",
      "Epoch 986/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 11348.7441 - mean_absolute_error: 11348.7441 - val_loss: 13747.1932 - val_mean_absolute_error: 13747.1934\n",
      "\n",
      "Epoch 00986: val_loss did not improve from 13482.78224\n",
      "Epoch 987/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 10641.3412 - mean_absolute_error: 10641.3408 - val_loss: 13605.2696 - val_mean_absolute_error: 13605.2686\n",
      "\n",
      "Epoch 00987: val_loss did not improve from 13482.78224\n",
      "Epoch 988/1000\n",
      "880/880 [==============================] - 0s 65us/step - loss: 11089.7352 - mean_absolute_error: 11089.7363 - val_loss: 14234.3364 - val_mean_absolute_error: 14234.3350\n",
      "\n",
      "Epoch 00988: val_loss did not improve from 13482.78224\n",
      "Epoch 989/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 10979.5898 - mean_absolute_error: 10979.5908 - val_loss: 14736.3752 - val_mean_absolute_error: 14736.3750\n",
      "\n",
      "Epoch 00989: val_loss did not improve from 13482.78224\n",
      "Epoch 990/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 11277.1300 - mean_absolute_error: 11277.1299 - val_loss: 13712.8857 - val_mean_absolute_error: 13712.8848\n",
      "\n",
      "Epoch 00990: val_loss did not improve from 13482.78224\n",
      "Epoch 991/1000\n",
      "880/880 [==============================] - 0s 61us/step - loss: 12720.4297 - mean_absolute_error: 12720.4297 - val_loss: 15635.1761 - val_mean_absolute_error: 15635.1748\n",
      "\n",
      "Epoch 00991: val_loss did not improve from 13482.78224\n",
      "Epoch 992/1000\n",
      "880/880 [==============================] - 0s 64us/step - loss: 13406.8827 - mean_absolute_error: 13406.8818 - val_loss: 16344.2453 - val_mean_absolute_error: 16344.2451\n",
      "\n",
      "Epoch 00992: val_loss did not improve from 13482.78224\n",
      "Epoch 993/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 11890.2163 - mean_absolute_error: 11890.2168 - val_loss: 14226.4389 - val_mean_absolute_error: 14226.4385\n",
      "\n",
      "Epoch 00993: val_loss did not improve from 13482.78224\n",
      "Epoch 994/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 11231.4591 - mean_absolute_error: 11231.4600 - val_loss: 15436.5656 - val_mean_absolute_error: 15436.5674\n",
      "\n",
      "Epoch 00994: val_loss did not improve from 13482.78224\n",
      "Epoch 995/1000\n",
      "880/880 [==============================] - 0s 63us/step - loss: 11629.2418 - mean_absolute_error: 11629.2422 - val_loss: 13754.0230 - val_mean_absolute_error: 13754.0225\n",
      "\n",
      "Epoch 00995: val_loss did not improve from 13482.78224\n",
      "Epoch 996/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 11423.2479 - mean_absolute_error: 11423.2471 - val_loss: 15482.8185 - val_mean_absolute_error: 15482.8184\n",
      "\n",
      "Epoch 00996: val_loss did not improve from 13482.78224\n",
      "Epoch 997/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 11807.3039 - mean_absolute_error: 11807.3047 - val_loss: 15027.5183 - val_mean_absolute_error: 15027.5195\n",
      "\n",
      "Epoch 00997: val_loss did not improve from 13482.78224\n",
      "Epoch 998/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 11138.7912 - mean_absolute_error: 11138.7920 - val_loss: 13707.4114 - val_mean_absolute_error: 13707.4102\n",
      "\n",
      "Epoch 00998: val_loss did not improve from 13482.78224\n",
      "Epoch 999/1000\n",
      "880/880 [==============================] - 0s 78us/step - loss: 10953.1146 - mean_absolute_error: 10953.1133 - val_loss: 13887.9188 - val_mean_absolute_error: 13887.9180\n",
      "\n",
      "Epoch 00999: val_loss did not improve from 13482.78224\n",
      "Epoch 1000/1000\n",
      "880/880 [==============================] - 0s 62us/step - loss: 10973.2680 - mean_absolute_error: 10973.2695 - val_loss: 16113.7946 - val_mean_absolute_error: 16113.7959\n",
      "\n",
      "Epoch 01000: val_loss did not improve from 13482.78224\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "hist=model.fit(XT,YT,epochs=1000,batch_size=32,validation_split=0.2,callbacks=callbacks_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "id": "4MhoZbbzTkxq",
    "outputId": "1311e931-cc08-420a-c5f1-811d9d665607"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.style.use('seaborn')\n",
    "    \n",
    "def plotLosses(trainLoss,validationLoss):\n",
    "    plt.plot(trainLoss,label='Training MAE')\n",
    "    plt.plot(validationLoss,label='Validation MAE')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plotLosses(hist.history['mean_absolute_error'],hist.history['val_mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eVg6vXzZVC5x"
   },
   "outputs": [],
   "source": [
    "model.load_weights('weights.hdf5')\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "c1uq-WEOX5Uo",
    "outputId": "1e647c96-1d1e-4abf-8c7b-6168ccc86553"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146820"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.load_model('weights.hdf5')\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "open(\"converted_model.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OY4yoSfOYDUl"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "housingprice",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
